,url,repository_url,labels_url,comments_url,events_url,html_url,id,node_id,number,title,labels,state,locked,assignee,assignees,milestone,comments,created_at,updated_at,closed_at,author_association,active_lock_reason,body,timeline_url,performed_via_github_app,repo_name,owner,user.login,user.id,user.node_id,user.avatar_url,user.gravatar_id,user.url,user.html_url,user.followers_url,user.following_url,user.gists_url,user.starred_url,user.subscriptions_url,user.organizations_url,user.repos_url,user.events_url,user.received_events_url,user.type,user.site_admin,pull_request.url,pull_request.html_url,pull_request.diff_url,pull_request.patch_url,reactions.url,reactions.total_count,reactions.+1,reactions.-1,reactions.laugh,reactions.hooray,reactions.confused,reactions.heart,reactions.rocket,reactions.eyes
0,https://api.github.com/repos/apache/spark/issues/34169,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34169/labels{/name},https://api.github.com/repos/apache/spark/issues/34169/comments,https://api.github.com/repos/apache/spark/issues/34169/events,https://github.com/apache/spark/pull/34169,1015046017,PR_kwDOAQXtWs4soi_4,34169,[SPARK-36920][SQL] Support ANSI intervals by `ABS()`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-10-04T11:14:47Z,2021-10-04T15:10:10Z,,MEMBER,,"### What changes were proposed in this pull request?
In the PR, I propose to handle ANSI interval types by the `Abs` expression, and the `abs()` function as a consequence of that:
- for positive and zero intervals, `ABS()` returns the same input value,
- for minimal supported values (`Int.MinValue` months for year-month interval and `Long.MinValue` microseconds for day-time interval), `ABS()` throws the arithmetic overflow exception.
- for other supported negative intervals, `ABS()` negate its input and returns a positive interval.

For example:
```sql
spark-sql> SELECT ABS(INTERVAL -'10-8' YEAR TO MONTH);
10-8
spark-sql> SELECT ABS(INTERVAL '-10 01:02:03.123456' DAY TO SECOND);
10 01:02:03.123456000
```

### Why are the changes needed?
To improve user experience with Spark SQL.

### Does this PR introduce _any_ user-facing change?
No, this PR just extends `ABS()` by supporting new types.

### How was this patch tested?
By running new tests:
```
$ build/sbt ""test:testOnly *ArithmeticExpressionSuite""
$ build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z interval.sql""
$ build/sbt ""sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite""
```",https://api.github.com/repos/apache/spark/issues/34169/timeline,,spark,apache,MaxGekk,1580697,MDQ6VXNlcjE1ODA2OTc=,https://avatars.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34169,https://github.com/apache/spark/pull/34169,https://github.com/apache/spark/pull/34169.diff,https://github.com/apache/spark/pull/34169.patch,https://api.github.com/repos/apache/spark/issues/34169/reactions,0,0,0,0,0,0,0,0,0
1,https://api.github.com/repos/apache/spark/issues/34168,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34168/labels{/name},https://api.github.com/repos/apache/spark/issues/34168/comments,https://api.github.com/repos/apache/spark/issues/34168/events,https://github.com/apache/spark/pull/34168,1014922325,PR_kwDOAQXtWs4soJ6k,34168,[SPARK-36302][SQL] Refactor thirteenth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-04T09:08:17Z,2021-10-04T09:37:17Z,,NONE,,"

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->Adds error classes to some of the exceptions in QueryExecutionErrors.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->Fills in missing error class and SQLSTATE fields.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->Existing tests
",https://api.github.com/repos/apache/spark/issues/34168/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34168,https://github.com/apache/spark/pull/34168,https://github.com/apache/spark/pull/34168.diff,https://github.com/apache/spark/pull/34168.patch,https://api.github.com/repos/apache/spark/issues/34168/reactions,0,0,0,0,0,0,0,0,0
2,https://api.github.com/repos/apache/spark/issues/34167,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34167/labels{/name},https://api.github.com/repos/apache/spark/issues/34167/comments,https://api.github.com/repos/apache/spark/issues/34167/events,https://github.com/apache/spark/pull/34167,1014742507,PR_kwDOAQXtWs4snmXY,34167,[SPARK-36919] Make BadRecordException fields transient,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-04T05:14:55Z,2021-10-04T05:36:31Z,,NONE,,"### What changes were proposed in this pull request?
Migrating a Spark application from 2.4.x to 3.1.x and finding a difference in the exception chaining behavior. In a case of parsing a malformed CSV, where the root cause exception should be Caused by: java.lang.RuntimeException: Malformed CSV record, only the top level exception is kept, and all lower level exceptions and root cause are lost. Thus, when we call ExceptionUtils.getRootCause on the exception, we still get itself.
The reason for the difference is that RuntimeException is wrapped in BadRecordException, which has unserializable fields. When we try to serialize the exception from tasks and deserialize from scheduler, the exception is lost.
This PR makes unserializable fields of BadRecordException transient, so the rest of the exception could be serialized and deserialized properly.

### Why are the changes needed?
Make BadRecordException serializable


### Does this PR introduce _any_ user-facing change?
User could get root cause of BadRecordException



### How was this patch tested?
Unit testing",https://api.github.com/repos/apache/spark/issues/34167/timeline,,spark,apache,tianhanhu,30429546,MDQ6VXNlcjMwNDI5NTQ2,https://avatars.githubusercontent.com/u/30429546?v=4,,https://api.github.com/users/tianhanhu,https://github.com/tianhanhu,https://api.github.com/users/tianhanhu/followers,https://api.github.com/users/tianhanhu/following{/other_user},https://api.github.com/users/tianhanhu/gists{/gist_id},https://api.github.com/users/tianhanhu/starred{/owner}{/repo},https://api.github.com/users/tianhanhu/subscriptions,https://api.github.com/users/tianhanhu/orgs,https://api.github.com/users/tianhanhu/repos,https://api.github.com/users/tianhanhu/events{/privacy},https://api.github.com/users/tianhanhu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34167,https://github.com/apache/spark/pull/34167,https://github.com/apache/spark/pull/34167.diff,https://github.com/apache/spark/pull/34167.patch,https://api.github.com/repos/apache/spark/issues/34167/reactions,0,0,0,0,0,0,0,0,0
3,https://api.github.com/repos/apache/spark/issues/34166,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34166/labels{/name},https://api.github.com/repos/apache/spark/issues/34166/comments,https://api.github.com/repos/apache/spark/issues/34166/events,https://github.com/apache/spark/pull/34166,1014632594,PR_kwDOAQXtWs4snQPT,34166,[SPARK-36918] Ignore types when comparing structs for unionByName,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-04T01:24:43Z,2021-10-04T01:36:14Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Rather than using `DataType.sameType` for comparing structs when unioning by name, only compare the names and orders of the fields, since all we are doing is determining if we need to recreate the struct in a different order. After ResolveUnion is done, the normal union will handle if the types are incompatible or not.

Additionally, adds a check to the recursive struct handling as well, so we don't have to recreate a nested struct if only one of its parents or siblings are different.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Performance improvement, especially with nested nullable structs, which get wrapped in an If(IsNull()). Unioning three or more structs can explode the plan due to the multiple structs created extracting values from If(IsNull()) values when the projections are collapsed.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, just a performance improvement.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New unit test for the helper method, and existing tests still pass.
",https://api.github.com/repos/apache/spark/issues/34166/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34166,https://github.com/apache/spark/pull/34166,https://github.com/apache/spark/pull/34166.diff,https://github.com/apache/spark/pull/34166.patch,https://api.github.com/repos/apache/spark/issues/34166/reactions,0,0,0,0,0,0,0,0,0
4,https://api.github.com/repos/apache/spark/issues/34165,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34165/labels{/name},https://api.github.com/repos/apache/spark/issues/34165/comments,https://api.github.com/repos/apache/spark/issues/34165/events,https://github.com/apache/spark/pull/34165,1014468489,PR_kwDOAQXtWs4smy8p,34165,[SPARK-36916] Enable Dependabot for improving security posture of the dependencies,"[{'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-03T17:29:19Z,2021-10-03T19:27:50Z,,NONE,,"
### What changes were proposed in this pull request?
Enable dependabot to get security updates and if needed version updates on dependencies 

### Why are the changes needed?

https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically

Having knowledge about vulnerabilities of the dependencies helps the project owners decide on their dependencies security posture to make decisions.

If the project decides to get updates only on security updates and not on any version updates then setting these options would not open any PR 's `open-pull-requests-limit: 0`


### Does this PR introduce _any_ user-facing change?
NO
This option has to be enabled in the security section of the project.
https://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/configuring-dependabot-security-updates#managing-dependabot-security-updates-for-your-repositories
### How was this patch tested?
N/A
",https://api.github.com/repos/apache/spark/issues/34165/timeline,,spark,apache,naveensrinivasan,172697,MDQ6VXNlcjE3MjY5Nw==,https://avatars.githubusercontent.com/u/172697?v=4,,https://api.github.com/users/naveensrinivasan,https://github.com/naveensrinivasan,https://api.github.com/users/naveensrinivasan/followers,https://api.github.com/users/naveensrinivasan/following{/other_user},https://api.github.com/users/naveensrinivasan/gists{/gist_id},https://api.github.com/users/naveensrinivasan/starred{/owner}{/repo},https://api.github.com/users/naveensrinivasan/subscriptions,https://api.github.com/users/naveensrinivasan/orgs,https://api.github.com/users/naveensrinivasan/repos,https://api.github.com/users/naveensrinivasan/events{/privacy},https://api.github.com/users/naveensrinivasan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34165,https://github.com/apache/spark/pull/34165,https://github.com/apache/spark/pull/34165.diff,https://github.com/apache/spark/pull/34165.patch,https://api.github.com/repos/apache/spark/issues/34165/reactions,0,0,0,0,0,0,0,0,0
5,https://api.github.com/repos/apache/spark/issues/34164,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34164/labels{/name},https://api.github.com/repos/apache/spark/issues/34164/comments,https://api.github.com/repos/apache/spark/issues/34164/events,https://github.com/apache/spark/pull/34164,1014138615,PR_kwDOAQXtWs4smE3V,34164,[SPARK-36913][SQL] Implement createIndex and IndexExists in DS V2 JDBC (MySQL dialect),"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-10-02T18:50:08Z,2021-10-04T10:37:10Z,,CONTRIBUTOR,,"

### What changes were proposed in this pull request?
Implementing `createIndex`/`IndexExists` in DS V2 JDBC


### Why are the changes needed?
This is a subtask of the V2 Index support. I am implementing index support for DS V2 JDBC so we can have a POC and an end to end testing. This PR implements `createIndex` and `IndexExists`. Next PR will implement `listIndexes` and `dropIndex`. I intentionally make the PR small so it's easier to review.

Index is not supported by h2 database and create/drop index are not standard SQL syntax. This PR only implements `createIndex` and `IndexExists` in `MySQL` dialect.


### Does this PR introduce _any_ user-facing change?
Yes, `createIndex`/`IndexExist` in DS V2 JDBC


### How was this patch tested?
new test
",https://api.github.com/repos/apache/spark/issues/34164/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34164,https://github.com/apache/spark/pull/34164,https://github.com/apache/spark/pull/34164.diff,https://github.com/apache/spark/pull/34164.patch,https://api.github.com/repos/apache/spark/issues/34164/reactions,0,0,0,0,0,0,0,0,0
6,https://api.github.com/repos/apache/spark/issues/34163,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34163/labels{/name},https://api.github.com/repos/apache/spark/issues/34163/comments,https://api.github.com/repos/apache/spark/issues/34163/events,https://github.com/apache/spark/pull/34163,1014105554,PR_kwDOAQXtWs4sl_8S,34163,[SPARK-36915] Pin actions to a full length commit SHA,"[{'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-10-02T16:48:41Z,2021-10-03T19:37:56Z,,NONE,,"### What changes were proposed in this pull request?
Pinning github actions to a SHA


### Why are the changes needed?
Pinning an action to a full length commit SHA is currently the only way to use an action as
an immutable release. Pinning to a particular SHA helps mitigate the risk of a bad actor adding a
backdoor to the action's repository, as they would need to generate a SHA-1 collision for
a valid Git object payload.

https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions#using-third-party-actions

https://github.com/ossf/scorecard/blob/main/docs/checks.md#pinned-dependencies


### Does this PR introduce _any_ user-facing change?
Running github action and checking the SHA with the existing repository


### How was this patch tested?
Running the GitHub action
",https://api.github.com/repos/apache/spark/issues/34163/timeline,,spark,apache,naveensrinivasan,172697,MDQ6VXNlcjE3MjY5Nw==,https://avatars.githubusercontent.com/u/172697?v=4,,https://api.github.com/users/naveensrinivasan,https://github.com/naveensrinivasan,https://api.github.com/users/naveensrinivasan/followers,https://api.github.com/users/naveensrinivasan/following{/other_user},https://api.github.com/users/naveensrinivasan/gists{/gist_id},https://api.github.com/users/naveensrinivasan/starred{/owner}{/repo},https://api.github.com/users/naveensrinivasan/subscriptions,https://api.github.com/users/naveensrinivasan/orgs,https://api.github.com/users/naveensrinivasan/repos,https://api.github.com/users/naveensrinivasan/events{/privacy},https://api.github.com/users/naveensrinivasan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34163,https://github.com/apache/spark/pull/34163,https://github.com/apache/spark/pull/34163.diff,https://github.com/apache/spark/pull/34163.patch,https://api.github.com/repos/apache/spark/issues/34163/reactions,0,0,0,0,0,0,0,0,0
7,https://api.github.com/repos/apache/spark/issues/34159,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34159/labels{/name},https://api.github.com/repos/apache/spark/issues/34159/comments,https://api.github.com/repos/apache/spark/issues/34159/events,https://github.com/apache/spark/pull/34159,1012660732,PR_kwDOAQXtWs4sh4Yd,34159,[SPARK-36906][PYTHON] Inline type hints for conf.py and observation.py in python/pyspark/sql,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-30T21:58:09Z,2021-10-03T03:27:46Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Inline type hints for conf.py and observation.py in python/pyspark/sql.

### Why are the changes needed?
Currently, there is type hint stub files (*.pyi) to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No.

It has a DOC typo fix:
`Metrics are aggregation expressions, which are applied to the DataFrame while **is** is being`
is changed to
`Metrics are aggregation expressions, which are applied to the DataFrame while **it** is being`

### How was this patch tested?
Existing test.",https://api.github.com/repos/apache/spark/issues/34159/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34159,https://github.com/apache/spark/pull/34159,https://github.com/apache/spark/pull/34159.diff,https://github.com/apache/spark/pull/34159.patch,https://api.github.com/repos/apache/spark/issues/34159/reactions,0,0,0,0,0,0,0,0,0
8,https://api.github.com/repos/apache/spark/issues/34158,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34158/labels{/name},https://api.github.com/repos/apache/spark/issues/34158/comments,https://api.github.com/repos/apache/spark/issues/34158/events,https://github.com/apache/spark/pull/34158,1012473336,PR_kwDOAQXtWs4shUY6,34158,[SPARK-36705][FOLLOW-UP] Support the case when user's classes need to register for Kryo serialization ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2021-09-30T18:08:27Z,2021-10-04T15:05:56Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

- Make the val lazy wherever `isPushBasedShuffleEnabled` is invoked when it is a class instance variable, so it can happen after user-defined jars/classes in `spark.kryo.classesToRegister` are downloaded and available on executor-side, as part of the fix for the exception mentioned below.

- Add a flag `checkSerializer` to control whether we need to check a serializer is `supportsRelocationOfSerializedObjects` or not within `isPushBasedShuffleEnabled` as part of the fix for the exception mentioned below. Specifically, we don't check this in `registerWithExternalShuffleServer()` in `BlockManager` and `createLocalDirsForMergedShuffleBlocks()` in `DiskBlockManager.scala` as the same issue would raise otherwise.

- Move `instantiateClassFromConf` and `instantiateClass` from `SparkEnv` into `Utils`, in order to let `isPushBasedShuffleEnabled` to leverage them for instantiating serializer instances.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
When user tries to set classes for Kryo Serialization by `spark.kryo.classesToRegister`, below exception(or similar) would be encountered in `isPushBasedShuffleEnabled` as indicated below.
Reproduced the issue in our internal branch by launching spark-shell as:
```
spark-shell --spark-version 3.1.1 --packages ml.dmlc:xgboost4j_2.12:1.3.1 --conf spark.kryo.classesToRegister=ml.dmlc.xgboost4j.scala.Booster
```

```
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:393)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:83)
	at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)
Caused by: org.apache.spark.SparkException: Failed to register classes with Kryo
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:183)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:230)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:171)
	at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
	at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:346)
	at org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:446)
	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:253)
	at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:249)
	at org.apache.spark.util.Utils$.isPushBasedShuffleEnabled(Utils.scala:2584)
	at org.apache.spark.MapOutputTrackerWorker.<init>(MapOutputTracker.scala:1109)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:322)
	at org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:205)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.$anonfun$run$7(CoarseGrainedExecutorBackend.scala:442)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:62)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
	... 4 more
Caused by: java.lang.ClassNotFoundException: ml.dmlc.xgboost4j.scala.Booster
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:217)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$6(KryoSerializer.scala:174)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:173)
	... 24 more
```
Registering user class for kryo serialization is happening after serializer creation in SparkEnv. Serializer creation can happen in `isPushBasedShuffleEnabled`, which can be called in some places prior to SparkEnv is created. Also, as per analysis by @JoshRosen, this is probably due to Kryo instantiation was failing because added packages hadn't been downloaded to the executor yet (because this code is running during executor startup, not task startup). The proposed change helps fix this issue.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Passed existing tests.
Tested this patch in our internal branch where user reported the issue. Issue is now not reproducible with this patch.",https://api.github.com/repos/apache/spark/issues/34158/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34158,https://github.com/apache/spark/pull/34158,https://github.com/apache/spark/pull/34158.diff,https://github.com/apache/spark/pull/34158.patch,https://api.github.com/repos/apache/spark/issues/34158/reactions,0,0,0,0,0,0,0,0,0
9,https://api.github.com/repos/apache/spark/issues/34156,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34156/labels{/name},https://api.github.com/repos/apache/spark/issues/34156/comments,https://api.github.com/repos/apache/spark/issues/34156/events,https://github.com/apache/spark/pull/34156,1011797521,PR_kwDOAQXtWs4sfSTC,34156,[WIP] [SPARK-36892] [Core] Disable batch fetch for a shuffle when push based shuffle is enabled,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-09-30T07:05:24Z,2021-10-03T18:08:43Z,,CONTRIBUTOR,,"We found an issue where user configured both AQE and push based shuffle, but the job started to hang after running some  stages. We took the thread dump from the Executors, which showed the task is still waiting to fetch shuffle blocks.
Proposed changes in the PR to fix the issue.

### What changes were proposed in this pull request?
Disabled Batch fetch when push based shuffle is enabled. 

### Why are the changes needed?
Without this patch, enabling AQE and Push based shuffle will have a chance to hang the tasks.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Tested the PR within our PR, with Spark shell and the queries are:

sql(""""""SELECT CASE WHEN rand() < 0.8 THEN 100 ELSE CAST(rand() * 30000000 AS INT) END AS s_item_id, CAST(rand() * 100 AS INT) AS s_quantity, DATE_ADD(current_date(), - CAST(rand() * 360 AS INT)) AS s_date FROM RANGE(1000000000)"""""").createOrReplaceTempView(""sales"")
// Dynamically coalesce partitions
sql(""""""SELECT s_date, sum(s_quantity) AS q FROM sales GROUP BY s_date ORDER BY q DESC"""""").collect

Unit tests to be added.
",https://api.github.com/repos/apache/spark/issues/34156/timeline,,spark,apache,zhouyejoe,8699921,MDQ6VXNlcjg2OTk5MjE=,https://avatars.githubusercontent.com/u/8699921?v=4,,https://api.github.com/users/zhouyejoe,https://github.com/zhouyejoe,https://api.github.com/users/zhouyejoe/followers,https://api.github.com/users/zhouyejoe/following{/other_user},https://api.github.com/users/zhouyejoe/gists{/gist_id},https://api.github.com/users/zhouyejoe/starred{/owner}{/repo},https://api.github.com/users/zhouyejoe/subscriptions,https://api.github.com/users/zhouyejoe/orgs,https://api.github.com/users/zhouyejoe/repos,https://api.github.com/users/zhouyejoe/events{/privacy},https://api.github.com/users/zhouyejoe/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34156,https://github.com/apache/spark/pull/34156,https://github.com/apache/spark/pull/34156.diff,https://github.com/apache/spark/pull/34156.patch,https://api.github.com/repos/apache/spark/issues/34156/reactions,0,0,0,0,0,0,0,0,0
10,https://api.github.com/repos/apache/spark/issues/34154,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34154/labels{/name},https://api.github.com/repos/apache/spark/issues/34154/comments,https://api.github.com/repos/apache/spark/issues/34154/events,https://github.com/apache/spark/pull/34154,1011768931,PR_kwDOAQXtWs4sfNGI,34154,[WIP][SQL] Add lpad and rpad functions for binary strings,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-30T06:28:09Z,2021-09-30T09:35:05Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/34154/timeline,,spark,apache,mkaravel,6397014,MDQ6VXNlcjYzOTcwMTQ=,https://avatars.githubusercontent.com/u/6397014?v=4,,https://api.github.com/users/mkaravel,https://github.com/mkaravel,https://api.github.com/users/mkaravel/followers,https://api.github.com/users/mkaravel/following{/other_user},https://api.github.com/users/mkaravel/gists{/gist_id},https://api.github.com/users/mkaravel/starred{/owner}{/repo},https://api.github.com/users/mkaravel/subscriptions,https://api.github.com/users/mkaravel/orgs,https://api.github.com/users/mkaravel/repos,https://api.github.com/users/mkaravel/events{/privacy},https://api.github.com/users/mkaravel/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34154,https://github.com/apache/spark/pull/34154,https://github.com/apache/spark/pull/34154.diff,https://github.com/apache/spark/pull/34154.patch,https://api.github.com/repos/apache/spark/issues/34154/reactions,0,0,0,0,0,0,0,0,0
11,https://api.github.com/repos/apache/spark/issues/34153,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34153/labels{/name},https://api.github.com/repos/apache/spark/issues/34153/comments,https://api.github.com/repos/apache/spark/issues/34153/events,https://github.com/apache/spark/pull/34153,1011681140,PR_kwDOAQXtWs4se9vH,34153,[SPARK-36796][BUILD][CORE][SQL] Pass all `sql/core` and dependent modules UTs use Maven with JDK 17,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2021-09-30T04:09:38Z,2021-10-04T12:25:18Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
In order to pass the UTs related to sql/core and dependent modules, this pr mainly does the following change:

- Add a new property named `extraJavaTestArgs` to `pom.xml`, It includes all `--add-opens` configurations required for UTs with Java 17 and It also include `-XX:+IgnoreUnrecognizedVMOptions` to compatible with Java 8.
- Add a new Util named `JavaModuleUtils`, It is used to supplement `--add-opens` configurations to `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` for test cases using `local` and `local=cluster` modes
- Increase `-Xmx4g` to `-Xmx5g` for `scalatest-maven-plugin` because `SPARK-36464: size returns correct positive number even with over 2GB data` will oom with JDK 17
- `DateTimeFormatterHelper`: the formatting keyword 'B' is disabled for compatibility with Java 8 behavior, where 'B' represents `Pattern letters to output a day period` in Java 17
- Independent verification the result of `postgreSQL/text.sql` because `select format_string('%0$s', 'Hello')` has different behavior between Java 8 and Java 17,  but it seems that using java 17 has expected behavior of PostgreSQL(`PostgreSQL throw ERROR: format specifies argument 0, but arguments are numbered from 1`)
- Replace `UseCompressedOops` with `UseCompressedClassPointers` in `WholeStageCodegenSparkSubmitSuite` because it seems that 'UseCompressedOops' no longer affects the behavior of Array types,  but the behavior of `UseCompressedClassPointers` is expected.

### Why are the changes needed?
Pass Spark UTs with JDK 17

### Does this PR introduce _any_ user-facing change?
`select format_string('%0$s', 'Hello')` will throw exception because Java 17 corrects a wrong behavior about the starting index of format specifications.


### How was this patch tested?
- Pass the Jenkins or GitHub Action
- Manual test `mvn clean install -pl sql/core -am` with Java 17, all tests passed 
",https://api.github.com/repos/apache/spark/issues/34153/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34153,https://github.com/apache/spark/pull/34153,https://github.com/apache/spark/pull/34153.diff,https://github.com/apache/spark/pull/34153.patch,https://api.github.com/repos/apache/spark/issues/34153/reactions,2,2,0,0,0,0,0,0,0
12,https://api.github.com/repos/apache/spark/issues/34150,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34150/labels{/name},https://api.github.com/repos/apache/spark/issues/34150/comments,https://api.github.com/repos/apache/spark/issues/34150/events,https://github.com/apache/spark/pull/34150,1011618222,PR_kwDOAQXtWs4seyxR,34150,[SPARK-36898][SQL] Make the shuffle hash join factor configurable,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-30T02:12:11Z,2021-09-30T07:35:11Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Currently the shuffle hash join factor is 3x, which cannot be changed. This PR make the factor can be configurable by adding a new config ""spark.sql.shuffleHashJoinFactor"".
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Make the choice of shuffle hash join more flexible
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
Add new config
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Existing tests.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/34150/timeline,,spark,apache,JkSelf,11972570,MDQ6VXNlcjExOTcyNTcw,https://avatars.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34150,https://github.com/apache/spark/pull/34150,https://github.com/apache/spark/pull/34150.diff,https://github.com/apache/spark/pull/34150.patch,https://api.github.com/repos/apache/spark/issues/34150/reactions,0,0,0,0,0,0,0,0,0
13,https://api.github.com/repos/apache/spark/issues/34148,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34148/labels{/name},https://api.github.com/repos/apache/spark/issues/34148/comments,https://api.github.com/repos/apache/spark/issues/34148/events,https://github.com/apache/spark/pull/34148,1011560484,PR_kwDOAQXtWs4seolX,34148,[SPARK-36895][SQL] Add Create Index syntax support,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-09-30T00:22:26Z,2021-10-03T18:34:08Z,,CONTRIBUTOR,,"
### What changes were proposed in this pull request?
This is the 2nd PR for DSv2 index support.

This PR adds the following:

- create index syntax support in parser and analyzer
- `CreateIndex` logic node
- `CreateIndexExec` physical node

`CreateIndex` is not implemented yet in this PR. Calling `CreateIndex` will throw `SQLFeatureNotSupportedException`, and the parsed index information such as `IndexName` `indexType` `columns` and index properties will be included in the error message for now for testing purpose.

### Why are the changes needed?
To support index in DSv2


### Does this PR introduce _any_ user-facing change?
Yes, the create table syntax as the following:

```
CREATE [index_type] INDEX [index_name] ON [TABLE] table_name (column_index_property_list)[OPTIONS indexPropertyList]

    column_index_property_list: column_name [OPTIONS(indexPropertyList)]  [ ,  . . . ]
    indexPropertyList: index_property_name [= index_property_value] [ ,  . . . ]
```

### How was this patch tested?
add a UT",https://api.github.com/repos/apache/spark/issues/34148/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34148,https://github.com/apache/spark/pull/34148,https://github.com/apache/spark/pull/34148.diff,https://github.com/apache/spark/pull/34148.patch,https://api.github.com/repos/apache/spark/issues/34148/reactions,0,0,0,0,0,0,0,0,0
14,https://api.github.com/repos/apache/spark/issues/34146,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34146/labels{/name},https://api.github.com/repos/apache/spark/issues/34146/comments,https://api.github.com/repos/apache/spark/issues/34146/events,https://github.com/apache/spark/pull/34146,1011503855,PR_kwDOAQXtWs4seeMO,34146,[SPARK-36894][PYTHON] Synchronize RDD.toDF annotations with SparkSession.createDataFrame variants.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-09-29T22:31:33Z,2021-10-02T11:29:08Z,,MEMBER,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pull request synchronizes `RDD.toDF` annotations with `SparkSession.createDataFrame` variants.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

- Adds support for providing `str` schema.
- Add supports for converting `RDDs` of ""atomic"" values, if schema is provided.

Additionally it introduces a `TypeVar` representing supported ""atomic"" values. This was done to avoid issue with manual data tests, where the following

```python
sc.parallelize([1]).toDF(schema=IntegerType())
```

results in 

```
error: No overload variant of ""toDF"" of ""RDD"" matches argument type ""IntegerType""  [call-overload]
note: Possible overload variants:
note:     def toDF(self, schema: Union[List[str], Tuple[str, ...], None] = ..., sampleRatio: Optional[float] = ...) -> DataFrame
note:     def toDF(self, schema: Union[StructType, str, None] = ...) -> DataFrame
```

when `Union` type is used (this problem doesn't surface when non-self bound is used).

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Type checker only.

Please note, that these annotations serve primarily to support documentation, as checks on `self` types are still very limited.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Existing tests and manual data tests.",https://api.github.com/repos/apache/spark/issues/34146/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34146,https://github.com/apache/spark/pull/34146,https://github.com/apache/spark/pull/34146.diff,https://github.com/apache/spark/pull/34146.patch,https://api.github.com/repos/apache/spark/issues/34146/reactions,1,0,0,0,1,0,0,0,0
15,https://api.github.com/repos/apache/spark/issues/34143,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34143/labels{/name},https://api.github.com/repos/apache/spark/issues/34143/comments,https://api.github.com/repos/apache/spark/issues/34143/events,https://github.com/apache/spark/pull/34143,1011195853,PR_kwDOAQXtWs4sdg0q,34143,[SPARK-36890][K8S] Use default WebsocketPingInterval for Kubernetes watches,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-09-29T16:55:43Z,2021-10-04T08:29:41Z,,NONE,,"
### What changes were proposed in this pull request?
This pull request removes the configuration that disables the WebsocketPingInterval. The default value of the [fabric8io/kubernetes-client](https://github.com/fabric8io/kubernetes-client/blob/master/README.md) library is 30 seconds.


### Why are the changes needed?
With a periodical websocket ping, the tunnel timeout in loadbalancers should work as expected.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
It is not possible for me to write a test for this particular infrastructure. I have checked the setting locally with my infrastructure with a small JUnit test.
",https://api.github.com/repos/apache/spark/issues/34143/timeline,,spark,apache,Reamer,454320,MDQ6VXNlcjQ1NDMyMA==,https://avatars.githubusercontent.com/u/454320?v=4,,https://api.github.com/users/Reamer,https://github.com/Reamer,https://api.github.com/users/Reamer/followers,https://api.github.com/users/Reamer/following{/other_user},https://api.github.com/users/Reamer/gists{/gist_id},https://api.github.com/users/Reamer/starred{/owner}{/repo},https://api.github.com/users/Reamer/subscriptions,https://api.github.com/users/Reamer/orgs,https://api.github.com/users/Reamer/repos,https://api.github.com/users/Reamer/events{/privacy},https://api.github.com/users/Reamer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34143,https://github.com/apache/spark/pull/34143,https://github.com/apache/spark/pull/34143.diff,https://github.com/apache/spark/pull/34143.patch,https://api.github.com/repos/apache/spark/issues/34143/reactions,0,0,0,0,0,0,0,0,0
16,https://api.github.com/repos/apache/spark/issues/34141,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34141/labels{/name},https://api.github.com/repos/apache/spark/issues/34141/comments,https://api.github.com/repos/apache/spark/issues/34141/events,https://github.com/apache/spark/pull/34141,1010604025,PR_kwDOAQXtWs4sbt0p,34141,[SPARK-33887][SQL] Allow insert overwrite same table with static partition if using dynamic partition overwrite mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-09-29T07:38:14Z,2021-09-29T08:21:48Z,,NONE,,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
For dynamic partition overwrite, we do not delete partition directories ahead, We write to staging directories and move to final partition directories after writing job is done. So it is ok to have outputPath try to overwrite inputpath. In view of this, i think inserting static partition in dynamic partition overwrite mode is also supported, i adjust the judging conditions of `dynamicPartitionOverwrite` to allow this.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
In some cases, we need overwrite static partition using itself.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
UT
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/34141/timeline,,spark,apache,PengleiShi,23723660,MDQ6VXNlcjIzNzIzNjYw,https://avatars.githubusercontent.com/u/23723660?v=4,,https://api.github.com/users/PengleiShi,https://github.com/PengleiShi,https://api.github.com/users/PengleiShi/followers,https://api.github.com/users/PengleiShi/following{/other_user},https://api.github.com/users/PengleiShi/gists{/gist_id},https://api.github.com/users/PengleiShi/starred{/owner}{/repo},https://api.github.com/users/PengleiShi/subscriptions,https://api.github.com/users/PengleiShi/orgs,https://api.github.com/users/PengleiShi/repos,https://api.github.com/users/PengleiShi/events{/privacy},https://api.github.com/users/PengleiShi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34141,https://github.com/apache/spark/pull/34141,https://github.com/apache/spark/pull/34141.diff,https://github.com/apache/spark/pull/34141.patch,https://api.github.com/repos/apache/spark/issues/34141/reactions,0,0,0,0,0,0,0,0,0
17,https://api.github.com/repos/apache/spark/issues/34139,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34139/labels{/name},https://api.github.com/repos/apache/spark/issues/34139/comments,https://api.github.com/repos/apache/spark/issues/34139/events,https://github.com/apache/spark/pull/34139,1010512087,PR_kwDOAQXtWs4sbdhe,34139,[SPARK-36876][SQL] Support Dynamic Partition pruning for HiveTableScanExec,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-09-29T05:42:11Z,2021-10-02T15:30:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Current code just support dynamic partition pruning for DSV1 and DSV2, here we support HiveTableScan


### Why are the changes needed?
Optimize Hive Table Scan dynamic partition pruning 


### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/34139/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34139,https://github.com/apache/spark/pull/34139,https://github.com/apache/spark/pull/34139.diff,https://github.com/apache/spark/pull/34139.patch,https://api.github.com/repos/apache/spark/issues/34139/reactions,0,0,0,0,0,0,0,0,0
18,https://api.github.com/repos/apache/spark/issues/34137,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34137/labels{/name},https://api.github.com/repos/apache/spark/issues/34137/comments,https://api.github.com/repos/apache/spark/issues/34137/events,https://github.com/apache/spark/pull/34137,1010488501,PR_kwDOAQXtWs4sbZK1,34137,[SPARK-36588] Migrate SHOW TABLES to use V2 command by default,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-29T04:58:42Z,2021-09-29T16:50:04Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR proposes to use V2 commands as default as outlined in [SPARK-36588](https://issues.apache.org/jira/browse/SPARK-36588), and this PR migrates `SHOW TABLES` to use v2 command by default.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
It's been a while since we introduced the v2 commands,  and it seems reasonable to use v2 commands by default even for the session catalog, with a legacy config to fall back to the v1 commands.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, the user can use v1 command by setting `spark.sql.legacy.useV1Command` to `true`.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests.",https://api.github.com/repos/apache/spark/issues/34137/timeline,,spark,apache,imback82,12103644,MDQ6VXNlcjEyMTAzNjQ0,https://avatars.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34137,https://github.com/apache/spark/pull/34137,https://github.com/apache/spark/pull/34137.diff,https://github.com/apache/spark/pull/34137.patch,https://api.github.com/repos/apache/spark/issues/34137/reactions,0,0,0,0,0,0,0,0,0
19,https://api.github.com/repos/apache/spark/issues/34136,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34136/labels{/name},https://api.github.com/repos/apache/spark/issues/34136/comments,https://api.github.com/repos/apache/spark/issues/34136/events,https://github.com/apache/spark/pull/34136,1010419700,PR_kwDOAQXtWs4sbNFv,34136,[SPARK-36884][PYTHON] Inline type hints for pyspark.sql.session,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-09-29T02:47:21Z,2021-10-02T09:16:31Z,,MEMBER,,"### What changes were proposed in this pull request?

Inline type hints from `python/pyspark/sql/session.pyi` to `python/pyspark/sql/session.py`.

### Why are the changes needed?

Currently, there is type hint stub files `python/pyspark/sql/session.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing test.",https://api.github.com/repos/apache/spark/issues/34136/timeline,,spark,apache,ueshin,506656,MDQ6VXNlcjUwNjY1Ng==,https://avatars.githubusercontent.com/u/506656?v=4,,https://api.github.com/users/ueshin,https://github.com/ueshin,https://api.github.com/users/ueshin/followers,https://api.github.com/users/ueshin/following{/other_user},https://api.github.com/users/ueshin/gists{/gist_id},https://api.github.com/users/ueshin/starred{/owner}{/repo},https://api.github.com/users/ueshin/subscriptions,https://api.github.com/users/ueshin/orgs,https://api.github.com/users/ueshin/repos,https://api.github.com/users/ueshin/events{/privacy},https://api.github.com/users/ueshin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34136,https://github.com/apache/spark/pull/34136,https://github.com/apache/spark/pull/34136.diff,https://github.com/apache/spark/pull/34136.patch,https://api.github.com/repos/apache/spark/issues/34136/reactions,0,0,0,0,0,0,0,0,0
20,https://api.github.com/repos/apache/spark/issues/34133,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34133/labels{/name},https://api.github.com/repos/apache/spark/issues/34133/comments,https://api.github.com/repos/apache/spark/issues/34133/events,https://github.com/apache/spark/pull/34133,1010269722,PR_kwDOAQXtWs4sayG9,34133,[SPARK-36881][PYTHON] Inline type hints for python/pyspark/sql/catalog.py,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-09-28T22:28:08Z,2021-10-01T21:59:59Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/sql/catalog.py.


### Why are the changes needed?
Currently, a type hint stub file hints for python/pyspark/sql/catalog.pyi is used. We may leverage static type check by inlining type hints.


### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Existing test.",https://api.github.com/repos/apache/spark/issues/34133/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34133,https://github.com/apache/spark/pull/34133,https://github.com/apache/spark/pull/34133.diff,https://github.com/apache/spark/pull/34133.patch,https://api.github.com/repos/apache/spark/issues/34133/reactions,0,0,0,0,0,0,0,0,0
21,https://api.github.com/repos/apache/spark/issues/34131,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34131/labels{/name},https://api.github.com/repos/apache/spark/issues/34131/comments,https://api.github.com/repos/apache/spark/issues/34131/events,https://github.com/apache/spark/pull/34131,1010188332,PR_kwDOAQXtWs4saimV,34131,[SPARK-36871][SQL][WIP] Migrate CreateViewStatement to v2 command,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-28T20:41:15Z,2021-09-29T01:44:08Z,,CONTRIBUTOR,,"

### What changes were proposed in this pull request?
Migrate `CreateViewStatement`  to v2 command framework

### Why are the changes needed?
Migrate to the standard V2 framework




### Does this PR introduce _any_ user-facing change?
no


### How was this patch tested?
existing tests",https://api.github.com/repos/apache/spark/issues/34131/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34131,https://github.com/apache/spark/pull/34131,https://github.com/apache/spark/pull/34131.diff,https://github.com/apache/spark/pull/34131.patch,https://api.github.com/repos/apache/spark/issues/34131/reactions,0,0,0,0,0,0,0,0,0
22,https://api.github.com/repos/apache/spark/issues/34130,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34130/labels{/name},https://api.github.com/repos/apache/spark/issues/34130/comments,https://api.github.com/repos/apache/spark/issues/34130/events,https://github.com/apache/spark/pull/34130,1010073798,PR_kwDOAQXtWs4saMlV,34130,[SPARK-36880][PYTHON] Inline type hints for python/pyspark/sql/functions.py	,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-09-28T18:27:07Z,2021-10-03T03:29:35Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Inline type hints from `python/pyspark/sql/functions.pyi` to `python/pyspark/sql/functions.py`.


### Why are the changes needed?
Currently, there is type hint stub files `python/pyspark/sql/functions.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Existing test.",https://api.github.com/repos/apache/spark/issues/34130/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34130,https://github.com/apache/spark/pull/34130,https://github.com/apache/spark/pull/34130.diff,https://github.com/apache/spark/pull/34130.patch,https://api.github.com/repos/apache/spark/issues/34130/reactions,0,0,0,0,0,0,0,0,0
23,https://api.github.com/repos/apache/spark/issues/34127,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34127/labels{/name},https://api.github.com/repos/apache/spark/issues/34127/comments,https://api.github.com/repos/apache/spark/issues/34127/events,https://github.com/apache/spark/pull/34127,1009674891,PR_kwDOAQXtWs4sY-18,34127,[SPARK-36849][SQL] Migrate UseStatement to v2 command framework,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-09-28T12:14:46Z,2021-10-01T11:38:08Z,,NONE,,"What changes were proposed in this pull request?
Migrate UseStatement to v2 command framework

Why are the changes needed?
Migrate to the standard V2 framework

Does this PR introduce any user-facing change?
no

How was this patch tested?
existing tests",https://api.github.com/repos/apache/spark/issues/34127/timeline,,spark,apache,dohongdayi,1321316,MDQ6VXNlcjEzMjEzMTY=,https://avatars.githubusercontent.com/u/1321316?v=4,,https://api.github.com/users/dohongdayi,https://github.com/dohongdayi,https://api.github.com/users/dohongdayi/followers,https://api.github.com/users/dohongdayi/following{/other_user},https://api.github.com/users/dohongdayi/gists{/gist_id},https://api.github.com/users/dohongdayi/starred{/owner}{/repo},https://api.github.com/users/dohongdayi/subscriptions,https://api.github.com/users/dohongdayi/orgs,https://api.github.com/users/dohongdayi/repos,https://api.github.com/users/dohongdayi/events{/privacy},https://api.github.com/users/dohongdayi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34127,https://github.com/apache/spark/pull/34127,https://github.com/apache/spark/pull/34127.diff,https://github.com/apache/spark/pull/34127.patch,https://api.github.com/repos/apache/spark/issues/34127/reactions,0,0,0,0,0,0,0,0,0
24,https://api.github.com/repos/apache/spark/issues/34122,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34122/labels{/name},https://api.github.com/repos/apache/spark/issues/34122/comments,https://api.github.com/repos/apache/spark/issues/34122/events,https://github.com/apache/spark/pull/34122,1008746689,PR_kwDOAQXtWs4sV-pu,34122,[WIP][SPARK-34826][SHUFFLE] Adaptively fetch shuffle mergers for push based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-27T22:46:32Z,2021-09-28T01:34:08Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Currently shuffle mergers are fetched before the start of the ShuffleMapStage. But for initial stages this can be problematic as shuffle mergers are nothing but unique hosts with shuffle services running which could be very few based on executors and this can cause merge ratio to be low. 

With this approach, `ShuffleMapTask` query for merger locations if not available and if available and start using this for pushing the blocks. Since partitions are mapped uniquely to a merger location, it should be fine to not push for the earlier set of tasks. This should improve the merge ratio for even initial stages.

Note: Currently this is in WIP because the changes are on top of SPARK-33701, once that gets merged will remove those changes updated it and remove WIP tag.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Performance improvement. No new APIs change.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests and also has been working in our internal production environment for a while now.",https://api.github.com/repos/apache/spark/issues/34122/timeline,,spark,apache,venkata91,8871522,MDQ6VXNlcjg4NzE1MjI=,https://avatars.githubusercontent.com/u/8871522?v=4,,https://api.github.com/users/venkata91,https://github.com/venkata91,https://api.github.com/users/venkata91/followers,https://api.github.com/users/venkata91/following{/other_user},https://api.github.com/users/venkata91/gists{/gist_id},https://api.github.com/users/venkata91/starred{/owner}{/repo},https://api.github.com/users/venkata91/subscriptions,https://api.github.com/users/venkata91/orgs,https://api.github.com/users/venkata91/repos,https://api.github.com/users/venkata91/events{/privacy},https://api.github.com/users/venkata91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34122,https://github.com/apache/spark/pull/34122,https://github.com/apache/spark/pull/34122.diff,https://github.com/apache/spark/pull/34122.patch,https://api.github.com/repos/apache/spark/issues/34122/reactions,0,0,0,0,0,0,0,0,0
25,https://api.github.com/repos/apache/spark/issues/34121,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34121/labels{/name},https://api.github.com/repos/apache/spark/issues/34121/comments,https://api.github.com/repos/apache/spark/issues/34121/events,https://github.com/apache/spark/pull/34121,1008681229,PR_kwDOAQXtWs4sVwx1,34121,[SPARK-36868][SQL] Migrate CreateFunctionStatement to v2 command framework,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-09-27T22:01:00Z,2021-09-30T02:43:53Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Migrate `CreateFunctionStatement` to v2 command framework

### Why are the changes needed?
Migrate to the standard V2 framework


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing tests
",https://api.github.com/repos/apache/spark/issues/34121/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34121,https://github.com/apache/spark/pull/34121,https://github.com/apache/spark/pull/34121.diff,https://github.com/apache/spark/pull/34121.patch,https://api.github.com/repos/apache/spark/issues/34121/reactions,0,0,0,0,0,0,0,0,0
26,https://api.github.com/repos/apache/spark/issues/34120,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34120/labels{/name},https://api.github.com/repos/apache/spark/issues/34120/comments,https://api.github.com/repos/apache/spark/issues/34120/events,https://github.com/apache/spark/pull/34120,1008581870,PR_kwDOAQXtWs4sVb5k,34120,[SPARK-35672][CORE][YARN] Pass user classpath entries to executors using config instead of command line.,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-09-27T20:38:40Z,2021-09-30T20:36:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Refactor the logic for constructing the user classpath from `yarn.ApplicationMaster` into `yarn.Client` so that it can be leveraged on the executor side as well, instead of having the driver construct it and pass it to the executor via command-line arguments. A new method, `getUserClassPath`, is added to `CoarseGrainedExecutorBackend` which defaults to `Nil` (consistent with the existing behavior where non-YARN resource managers do not configure the user classpath). `YarnCoarseGrainedExecutorBackend` overrides this to construct the user classpath from the existing `APP_JAR` and `SECONDARY_JARS` configs. Within `yarn.Client`, environment variables in the configured paths are resolved before constructing the classpath.

Please note that this is a re-submission of #32810, which was reverted in #34082 due to the issues described in [this comment](https://issues.apache.org/jira/browse/SPARK-35672?focusedCommentId=17419285&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17419285). This PR additionally includes the changes described in #34084 to resolve the issue, though this PR has been enhanced to properly handle escape strings, unlike #34084.

### Why are the changes needed?
User-provided JARs are made available to executors using a custom classloader, so they do not appear on the standard Java classpath. Instead, they are passed as a list to the executor which then creates a classloader out of the URLs. Currently in the case of YARN, this list of JARs is crafted by the Driver (in `ExecutorRunnable`), which then passes the information to the executors (`CoarseGrainedExecutorBackend`) by specifying each JAR on the executor command line as `--user-class-path /path/to/myjar.jar`. This can cause extremely long argument lists when there are many JARs, which can cause the OS argument length to be exceeded, typically manifesting as the error message:

> /bin/bash: Argument list too long

A [Google search](https://www.google.com/search?q=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22&oq=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22) indicates that this is not a theoretical problem and afflicts real users, including ours. Passing this list using the configurations instead resolves this issue.

### Does this PR introduce _any_ user-facing change?
No, except for fixing the bug, allowing for larger JAR lists to be passed successfully. Configuration of JARs is identical to before. Substitution of environment variables in `spark.jars` or `spark.yarn.config.replacementPath` works as expected.

### How was this patch tested?
New unit tests were added in `YarnClusterSuite`. Also, we have been running a similar fix internally for 4 months with great success.",https://api.github.com/repos/apache/spark/issues/34120/timeline,,spark,apache,xkrogen,6570401,MDQ6VXNlcjY1NzA0MDE=,https://avatars.githubusercontent.com/u/6570401?v=4,,https://api.github.com/users/xkrogen,https://github.com/xkrogen,https://api.github.com/users/xkrogen/followers,https://api.github.com/users/xkrogen/following{/other_user},https://api.github.com/users/xkrogen/gists{/gist_id},https://api.github.com/users/xkrogen/starred{/owner}{/repo},https://api.github.com/users/xkrogen/subscriptions,https://api.github.com/users/xkrogen/orgs,https://api.github.com/users/xkrogen/repos,https://api.github.com/users/xkrogen/events{/privacy},https://api.github.com/users/xkrogen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34120,https://github.com/apache/spark/pull/34120,https://github.com/apache/spark/pull/34120.diff,https://github.com/apache/spark/pull/34120.patch,https://api.github.com/repos/apache/spark/issues/34120/reactions,0,0,0,0,0,0,0,0,0
27,https://api.github.com/repos/apache/spark/issues/34114,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34114/labels{/name},https://api.github.com/repos/apache/spark/issues/34114/comments,https://api.github.com/repos/apache/spark/issues/34114/events,https://github.com/apache/spark/pull/34114,1007985709,PR_kwDOAQXtWs4sTkDM,34114,[SPARK-36438][PYTHON] Support list-like Python objects for Series comparison,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-09-27T10:39:35Z,2021-10-01T08:38:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

This PR proposes to implement `Series` comparison with list-like Python objects.

Currently `Series` doesn't support the comparison to list-like Python objects such as `list`, `tuple`, `dict`, `set`.

**Before**

```python
>>> psser
0    1
1    2
2    3
dtype: int64

>>> psser == [3, 2, 1]
Traceback (most recent call last):
...
TypeError: The operation can not be applied to list.
...
```

**After**

```python
>>> psser
0    1
1    2
2    3
dtype: int64

>>> psser == [3, 2, 1]
0    False
1     True
2    False
dtype: bool
```

This was originally proposed in https://github.com/databricks/koalas/pull/2022, and all reviews in origin PR has been resolved.

### Why are the changes needed?

To follow pandas' behavior.


### Does this PR introduce _any_ user-facing change?

Yes, the `Series` comparison with list-like Python objects now possible.


### How was this patch tested?

Unittests
",https://api.github.com/repos/apache/spark/issues/34114/timeline,,spark,apache,itholic,44108233,MDQ6VXNlcjQ0MTA4MjMz,https://avatars.githubusercontent.com/u/44108233?v=4,,https://api.github.com/users/itholic,https://github.com/itholic,https://api.github.com/users/itholic/followers,https://api.github.com/users/itholic/following{/other_user},https://api.github.com/users/itholic/gists{/gist_id},https://api.github.com/users/itholic/starred{/owner}{/repo},https://api.github.com/users/itholic/subscriptions,https://api.github.com/users/itholic/orgs,https://api.github.com/users/itholic/repos,https://api.github.com/users/itholic/events{/privacy},https://api.github.com/users/itholic/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34114,https://github.com/apache/spark/pull/34114,https://github.com/apache/spark/pull/34114.diff,https://github.com/apache/spark/pull/34114.patch,https://api.github.com/repos/apache/spark/issues/34114/reactions,0,0,0,0,0,0,0,0,0
28,https://api.github.com/repos/apache/spark/issues/34108,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34108/labels{/name},https://api.github.com/repos/apache/spark/issues/34108/comments,https://api.github.com/repos/apache/spark/issues/34108/events,https://github.com/apache/spark/pull/34108,1007293416,PR_kwDOAQXtWs4sRnM9,34108,[SPARK-36638][SQL][TEST] Generalize OptimizeSkewedJoin - correctness,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-26T06:25:07Z,2021-09-28T01:37:11Z,,CONTRIBUTOR,,this draft is only used to check correctness of the algorithm in https://github.com/apache/spark/pull/33893,https://api.github.com/repos/apache/spark/issues/34108/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34108,https://github.com/apache/spark/pull/34108,https://github.com/apache/spark/pull/34108.diff,https://github.com/apache/spark/pull/34108.patch,https://api.github.com/repos/apache/spark/issues/34108/reactions,0,0,0,0,0,0,0,0,0
29,https://api.github.com/repos/apache/spark/issues/34098,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34098/labels{/name},https://api.github.com/repos/apache/spark/issues/34098/comments,https://api.github.com/repos/apache/spark/issues/34098/events,https://github.com/apache/spark/pull/34098,1006345272,PR_kwDOAQXtWs4sO97D,34098,[SPARK-36842][Core] TaskSchedulerImpl - stop TaskResultGetter properly,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-09-24T11:02:51Z,2021-09-29T11:14:15Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Catch exception during TaskSchedulerImpl.stop() so that all components can be stopped properly

### Why are the changes needed?
Otherwise some threads won't be stopped during spark session restart

### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?
It's tested by 
1. create a new spark session in yarn-client mode
2. kill the spark application on yarn
3. check that the spark context is stopped and create a new spark session
4. do the above steps multiple times and verify that no task-result-getter threads number doesn't increase
",https://api.github.com/repos/apache/spark/issues/34098/timeline,,spark,apache,lxian,3442641,MDQ6VXNlcjM0NDI2NDE=,https://avatars.githubusercontent.com/u/3442641?v=4,,https://api.github.com/users/lxian,https://github.com/lxian,https://api.github.com/users/lxian/followers,https://api.github.com/users/lxian/following{/other_user},https://api.github.com/users/lxian/gists{/gist_id},https://api.github.com/users/lxian/starred{/owner}{/repo},https://api.github.com/users/lxian/subscriptions,https://api.github.com/users/lxian/orgs,https://api.github.com/users/lxian/repos,https://api.github.com/users/lxian/events{/privacy},https://api.github.com/users/lxian/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34098,https://github.com/apache/spark/pull/34098,https://github.com/apache/spark/pull/34098.diff,https://github.com/apache/spark/pull/34098.patch,https://api.github.com/repos/apache/spark/issues/34098/reactions,0,0,0,0,0,0,0,0,0
30,https://api.github.com/repos/apache/spark/issues/34096,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34096/labels{/name},https://api.github.com/repos/apache/spark/issues/34096/comments,https://api.github.com/repos/apache/spark/issues/34096/events,https://github.com/apache/spark/pull/34096,1006294467,PR_kwDOAQXtWs4sOzy_,34096,[SPARK-36841][SQL] Add ansi syntax `set catalog xxx` to change the current catalog,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-09-24T10:00:15Z,2021-09-29T10:46:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
1Add the statement of `set catalog xxx` to change the current catalog
2Retain the `USE` statement to change the current catalog
3Forcible loading the new catalog when change the new catalog.

### Why are the changes needed?
Ansi SQL use `SET CATALOG XXX` statement to change the catalog.

[DISCUSS](https://github.com/apache/spark/pull/34030#issuecomment-925936538)

<img width=""521"" alt=""set-catalog"" src=""https://user-images.githubusercontent.com/41178002/134658562-4e4dd879-b6e5-484c-9461-6345c3faaf2e.png"">


### Does this PR introduce _any_ user-facing change?
Yes, User can use `SET CATALOG XXX` to change the current catalog

### How was this patch tested?
Add ut testcase
",https://api.github.com/repos/apache/spark/issues/34096/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34096,https://github.com/apache/spark/pull/34096,https://github.com/apache/spark/pull/34096.diff,https://github.com/apache/spark/pull/34096.patch,https://api.github.com/repos/apache/spark/issues/34096/reactions,0,0,0,0,0,0,0,0,0
31,https://api.github.com/repos/apache/spark/issues/34093,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34093/labels{/name},https://api.github.com/repos/apache/spark/issues/34093/comments,https://api.github.com/repos/apache/spark/issues/34093/events,https://github.com/apache/spark/pull/34093,1006160507,PR_kwDOAQXtWs4sOZ5B,34093,[SPARK-36294][SQL] Refactor fifth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-09-24T07:18:22Z,2021-10-01T17:22:26Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Refactor fifth set of 20 query execution errors to use error classes. as follows:
```
createStreamingSourceNotSpecifySchemaError
streamedOperatorUnsupportedByDataSourceError
multiplePathsSpecifiedError
failedToFindDataSourceError
removedClassInSpark2Error
incompatibleDataSourceRegisterError
unrecognizedFileFormatError
sparkUpgradeInReadingDatesError
sparkUpgradeInWritingDatesError
buildReaderUnsupportedForFileFormatError
jobAbortedError
taskFailedWhileWritingRowsError
readCurrentFileNotFoundError
unsupportedSaveModeError
cannotClearOutputDirectoryError
cannotClearPartitionDirectoryError
failedToCastValueToDataTypeForPartitionColumnError
endOfStreamError
fallbackV1RelationReportsInconsistentSchemaError
cannotDropNonemptyNamespaceError
```


### Why are the changes needed?
[SPARK-36294](https://issues.apache.org/jira/browse/SPARK-36294)


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existed UT Testcase
",https://api.github.com/repos/apache/spark/issues/34093/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34093,https://github.com/apache/spark/pull/34093,https://github.com/apache/spark/pull/34093.diff,https://github.com/apache/spark/pull/34093.patch,https://api.github.com/repos/apache/spark/issues/34093/reactions,0,0,0,0,0,0,0,0,0
32,https://api.github.com/repos/apache/spark/issues/34089,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34089/labels{/name},https://api.github.com/repos/apache/spark/issues/34089/comments,https://api.github.com/repos/apache/spark/issues/34089/events,https://github.com/apache/spark/pull/34089,1006016259,PR_kwDOAQXtWs4sN_AE,34089,[SPARK-36837][BUILD] Upgrade Kafka to 3.0.1,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-09-24T02:34:04Z,2021-09-25T19:17:22Z,,MEMBER,,"### What changes were proposed in this pull request?

This PR aims to upgrade Kafka client library from 2.8.1 to 3.0.0.

### Why are the changes needed?

Kafka 3.0.0 has the following improvements and bug fixes including client side.
- https://downloads.apache.org/kafka/3.0.0/RELEASE_NOTES.html

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs.",https://api.github.com/repos/apache/spark/issues/34089/timeline,,spark,apache,dongjoon-hyun,9700541,MDQ6VXNlcjk3MDA1NDE=,https://avatars.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34089,https://github.com/apache/spark/pull/34089,https://github.com/apache/spark/pull/34089.diff,https://github.com/apache/spark/pull/34089.patch,https://api.github.com/repos/apache/spark/issues/34089/reactions,0,0,0,0,0,0,0,0,0
33,https://api.github.com/repos/apache/spark/issues/34083,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34083/labels{/name},https://api.github.com/repos/apache/spark/issues/34083/comments,https://api.github.com/repos/apache/spark/issues/34083/events,https://github.com/apache/spark/pull/34083,1005856593,PR_kwDOAQXtWs4sNgPR,34083,Add docs about using Shiv for packaging (similar to PEX),"[{'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-09-23T21:20:34Z,2021-09-28T01:28:05Z,,NONE,,"### What changes were proposed in this pull request?

The Shiv packaging tool works similarly to PEX and can be used to distribute Python with its dependencies in an executable. These changes mention Shiv and demonstrate it's use similar to the PEX project.

### Why are the changes needed?

Shiv is a widely used packaging tool similar to PEX. These changes mention Shiv as an alternative tool.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

No code changes.",https://api.github.com/repos/apache/spark/issues/34083/timeline,,spark,apache,grantjenks,118304,MDQ6VXNlcjExODMwNA==,https://avatars.githubusercontent.com/u/118304?v=4,,https://api.github.com/users/grantjenks,https://github.com/grantjenks,https://api.github.com/users/grantjenks/followers,https://api.github.com/users/grantjenks/following{/other_user},https://api.github.com/users/grantjenks/gists{/gist_id},https://api.github.com/users/grantjenks/starred{/owner}{/repo},https://api.github.com/users/grantjenks/subscriptions,https://api.github.com/users/grantjenks/orgs,https://api.github.com/users/grantjenks/repos,https://api.github.com/users/grantjenks/events{/privacy},https://api.github.com/users/grantjenks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34083,https://github.com/apache/spark/pull/34083,https://github.com/apache/spark/pull/34083.diff,https://github.com/apache/spark/pull/34083.patch,https://api.github.com/repos/apache/spark/issues/34083/reactions,0,0,0,0,0,0,0,0,0
34,https://api.github.com/repos/apache/spark/issues/34079,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34079/labels{/name},https://api.github.com/repos/apache/spark/issues/34079/comments,https://api.github.com/repos/apache/spark/issues/34079/events,https://github.com/apache/spark/pull/34079,1005661139,PR_kwDOAQXtWs4sM5eD,34079,[SPARK-36834][SHUFFLE] Add support for namespacing log lines emitted by external shuffle service,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-23T17:04:49Z,2021-09-24T17:28:31Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Added a config `spark.yarn.shuffle.service.logs.namespace` which can be used to add a namespace suffix to log lines emitted by the External Shuffle Service.

### Why are the changes needed?
Since many instances of ESS can be running on the same NM, it would be easier to distinguish between them.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
N/A",https://api.github.com/repos/apache/spark/issues/34079/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34079,https://github.com/apache/spark/pull/34079,https://github.com/apache/spark/pull/34079.diff,https://github.com/apache/spark/pull/34079.patch,https://api.github.com/repos/apache/spark/issues/34079/reactions,0,0,0,0,0,0,0,0,0
35,https://api.github.com/repos/apache/spark/issues/34074,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34074/labels{/name},https://api.github.com/repos/apache/spark/issues/34074/comments,https://api.github.com/repos/apache/spark/issues/34074/events,https://github.com/apache/spark/pull/34074,1004856465,PR_kwDOAQXtWs4sKbtm,34074,[SPARK-33573][SHUFFLE][YARN] Shuffle server side metrics for Push-based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-09-22T23:19:50Z,2021-09-22T23:22:20Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is one of the patches for SPARK-33235: Push-based Shuffle Improvement Tasks.
Added a class `PushMergeMetrics`, to collect below metrics from shuffle server side for Push-based shuffle:
- no opportunity responses
- too late responses
- pushed bytes written
- cached block bytes

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This helps to understand the push based shuffle metrics from shuffle server side.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added a method `verifyMetrics` to verify those metrics in existing unit tests.

Lead-authored by: Chandni Singh chsingh@linkedin.com
Co-authored by: Min Shen mshen@linkedin.com
Co-authored by: Minchu Yang minyang@linkedin.com",https://api.github.com/repos/apache/spark/issues/34074/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34074,https://github.com/apache/spark/pull/34074,https://github.com/apache/spark/pull/34074.diff,https://github.com/apache/spark/pull/34074.patch,https://api.github.com/repos/apache/spark/issues/34074/reactions,0,0,0,0,0,0,0,0,0
36,https://api.github.com/repos/apache/spark/issues/34072,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34072/labels{/name},https://api.github.com/repos/apache/spark/issues/34072/comments,https://api.github.com/repos/apache/spark/issues/34072/events,https://github.com/apache/spark/pull/34072,1004239999,PR_kwDOAQXtWs4sIe0t,34072,[SPARK-36680][SQL] Supports Dynamic Table Options for Spark SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-22T12:31:43Z,2021-09-24T12:17:40Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Add a new hint `OPTIONS`

### Why are the changes needed?
Now a DataFrame API user can implement dynamic options through the DataFrameReader$option method, but Spark SQL users cannot use.
 ```
public interface SupportsRead extends Table {
    ScanBuilder newScanBuilder(CaseInsensitiveStringMap var1);
}
```
The table options were persisted to the Catalog and if we want to modify that, we should use another DDL like ""ALTER TABLE ..."". But there are some cases that user want to modify the table options dynamically just in the query:
- JDBCTable set fetchsize according to the actual situation of the table
- IcebergTable support time travel
```
spark.read
    .option(""snapshot-id"", 10963874102873L)
    .format(""iceberg"")
    .load(""path/to/table"")
```
These parameters setting is very common and ad-hoc, setting them flexibly would promote the user experience with Spark SQL especially for Now we support catalog expansion.

### Does this PR introduce _any_ user-facing change?
##### OPTIONS Hints
```
-- time trave 
SELECT * FROM t /*+ OPTIONS('snapshot-id'='10963874102873L') */
```

### How was this patch tested?
Added Unit test.",https://api.github.com/repos/apache/spark/issues/34072/timeline,,spark,apache,wang-zhun,61445191,MDQ6VXNlcjYxNDQ1MTkx,https://avatars.githubusercontent.com/u/61445191?v=4,,https://api.github.com/users/wang-zhun,https://github.com/wang-zhun,https://api.github.com/users/wang-zhun/followers,https://api.github.com/users/wang-zhun/following{/other_user},https://api.github.com/users/wang-zhun/gists{/gist_id},https://api.github.com/users/wang-zhun/starred{/owner}{/repo},https://api.github.com/users/wang-zhun/subscriptions,https://api.github.com/users/wang-zhun/orgs,https://api.github.com/users/wang-zhun/repos,https://api.github.com/users/wang-zhun/events{/privacy},https://api.github.com/users/wang-zhun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34072,https://github.com/apache/spark/pull/34072,https://github.com/apache/spark/pull/34072.diff,https://github.com/apache/spark/pull/34072.patch,https://api.github.com/repos/apache/spark/issues/34072/reactions,2,1,0,0,0,0,0,0,1
37,https://api.github.com/repos/apache/spark/issues/34071,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34071/labels{/name},https://api.github.com/repos/apache/spark/issues/34071/comments,https://api.github.com/repos/apache/spark/issues/34071/events,https://github.com/apache/spark/pull/34071,1004022086,PR_kwDOAQXtWs4sHyxi,34071,[SPARK-36168][BUILD] Add support for Scala 2.13 in dev/test-dependencies.sh,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-09-22T08:44:24Z,2021-09-26T00:49:15Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

Enable support of Scala 2.13 in the `dev/test-dependencies.sh`

### Why are the changes needed?

At this moment only Scala 2.12 is supported

### Does this PR introduce _any_ user-facing change?

No, this changes only build related scripts

### How was this patch tested?

./dev/change-scala-version.sh 2.13 && ./dev/test-dependencies.sh",https://api.github.com/repos/apache/spark/issues/34071/timeline,,spark,apache,slothspot,760295,MDQ6VXNlcjc2MDI5NQ==,https://avatars.githubusercontent.com/u/760295?v=4,,https://api.github.com/users/slothspot,https://github.com/slothspot,https://api.github.com/users/slothspot/followers,https://api.github.com/users/slothspot/following{/other_user},https://api.github.com/users/slothspot/gists{/gist_id},https://api.github.com/users/slothspot/starred{/owner}{/repo},https://api.github.com/users/slothspot/subscriptions,https://api.github.com/users/slothspot/orgs,https://api.github.com/users/slothspot/repos,https://api.github.com/users/slothspot/events{/privacy},https://api.github.com/users/slothspot/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34071,https://github.com/apache/spark/pull/34071,https://github.com/apache/spark/pull/34071.diff,https://github.com/apache/spark/pull/34071.patch,https://api.github.com/repos/apache/spark/issues/34071/reactions,0,0,0,0,0,0,0,0,0
38,https://api.github.com/repos/apache/spark/issues/34070,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34070/labels{/name},https://api.github.com/repos/apache/spark/issues/34070/comments,https://api.github.com/repos/apache/spark/issues/34070/events,https://github.com/apache/spark/pull/34070,1003981877,PR_kwDOAQXtWs4sHqww,34070,[SPARK-36840][SQL] Support DPP if there is no selective predicate on the filtering side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-09-22T08:11:17Z,2021-09-27T08:59:49Z,,MEMBER,,"### What changes were proposed in this pull request?

This pr makes it insert a DPP if there is no selective predicate on the filtering side and it still has benefits even makes the filter ratio much smaller.


### Why are the changes needed?

In some cases, it may pruning a lot of data even if there is no selective predicate on the filtering side.






Before this PR | After this PR
--- | ---
![image](https://user-images.githubusercontent.com/5399861/134630846-fbec8def-a12d-4c77-bd82-084a04ab89c0.png)  | ![image](https://user-images.githubusercontent.com/5399861/134631736-d170c194-22e0-4ae0-a592-ef9e635866f2.png)


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.",https://api.github.com/repos/apache/spark/issues/34070/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34070,https://github.com/apache/spark/pull/34070,https://github.com/apache/spark/pull/34070.diff,https://github.com/apache/spark/pull/34070.patch,https://api.github.com/repos/apache/spark/issues/34070/reactions,1,1,0,0,0,0,0,0,0
39,https://api.github.com/repos/apache/spark/issues/34069,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34069/labels{/name},https://api.github.com/repos/apache/spark/issues/34069/comments,https://api.github.com/repos/apache/spark/issues/34069/events,https://github.com/apache/spark/pull/34069,1003887215,PR_kwDOAQXtWs4sHXUM,34069,[SPARK-36823][SQL] Support broadcast nested loop join hint for equi-join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-09-22T06:42:15Z,2021-09-26T07:13:52Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add a new hint `BROADCAST_NL`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For the join if one side is small and other side is large, the shuffle overhead is also very big. Due to the
bhj limitation, we can only broadcast right side for left join and left side for right join. So for the other case, we can try to use `BroadcastNestedLoopJoin` as the join strategy.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, new hint

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add new test in `JoinHintSuite`",https://api.github.com/repos/apache/spark/issues/34069/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34069,https://github.com/apache/spark/pull/34069,https://github.com/apache/spark/pull/34069.diff,https://github.com/apache/spark/pull/34069.patch,https://api.github.com/repos/apache/spark/issues/34069/reactions,0,0,0,0,0,0,0,0,0
40,https://api.github.com/repos/apache/spark/issues/34062,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34062/labels{/name},https://api.github.com/repos/apache/spark/issues/34062/comments,https://api.github.com/repos/apache/spark/issues/34062/events,https://github.com/apache/spark/pull/34062,1003178471,PR_kwDOAQXtWs4sE9i-,34062,[SPARK-36819][SQL] Don't insert redundant filters in case static partition pruning can be done,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-09-21T19:58:51Z,2021-09-27T09:25:09Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Don't insert dynamic partition pruning filters in case the filters already added statically. In case the filtering predicate on dimension table is on joinKey, no need to insert DPP filter in that case.

Sample query:
```
SELECT f.date_id, f.pid, f.sid FROM
(select date_id, product_id as pid, store_id as sid from fact_stats) as f
JOIN dim_stats s
ON f.sid = s.store_id WHERE s.store_id = 3
```

Without this PR DPP filter is inserted for above query despite of `store_id#4551 = 3` on fact_stats
```
  == Physical Plan ==
  *(2) Project [date_id#4548, pid#4631, sid#4632]
  +- *(2) BroadcastHashJoin [sid#4632], [store_id#4552], Inner, BuildRight, false
     :- *(2) Project [date_id#4548, product_id#4549 AS pid#4631, store_id#4551 AS sid#4632]
     :  +- *(2) ColumnarToRow
     :     +- FileScan parquet default.fact_stats[date_id#4548,product_id#4549,store_id#4551] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/workspace/spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [(store_id#4551 = 3), isnotnull(store_id#4551), **dynamicpruningexpression(store_id#4551 IN dynamic...,** PushedFilters: [], ReadSchema: struct<date_id:int,product_id:int>
     :           +- SubqueryBroadcast dynamicpruning#4636, 0, [store_id#4552], [id=#2461]
     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2460]
     :                 +- *(1) Filter (isnotnull(store_id#4552) AND (store_id#4552 = 3))
     :                    +- *(1) ColumnarToRow
     :                       +- FileScan parquet default.dim_stats[store_id#4552] Batched: true, DataFilters: [isnotnull(store_id#4552), (store_id#4552 = 3)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/workspace/spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [], PushedFilters: [IsNotNull(store_id), EqualTo(store_id,3)], ReadSchema: struct<store_id:int>
     +- ReusedExchange [store_id#4552], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2460]
```



### Why are the changes needed?
Having redundant dynamic filters can have  unnecessary overheads: extra filtering overhead + in case of DPP subquery case, subquery execution overhead.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing UTs pass. added new test case.",https://api.github.com/repos/apache/spark/issues/34062/timeline,,spark,apache,Swinky,5418286,MDQ6VXNlcjU0MTgyODY=,https://avatars.githubusercontent.com/u/5418286?v=4,,https://api.github.com/users/Swinky,https://github.com/Swinky,https://api.github.com/users/Swinky/followers,https://api.github.com/users/Swinky/following{/other_user},https://api.github.com/users/Swinky/gists{/gist_id},https://api.github.com/users/Swinky/starred{/owner}{/repo},https://api.github.com/users/Swinky/subscriptions,https://api.github.com/users/Swinky/orgs,https://api.github.com/users/Swinky/repos,https://api.github.com/users/Swinky/events{/privacy},https://api.github.com/users/Swinky/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34062,https://github.com/apache/spark/pull/34062,https://github.com/apache/spark/pull/34062.diff,https://github.com/apache/spark/pull/34062.patch,https://api.github.com/repos/apache/spark/issues/34062/reactions,0,0,0,0,0,0,0,0,0
41,https://api.github.com/repos/apache/spark/issues/34060,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34060/labels{/name},https://api.github.com/repos/apache/spark/issues/34060/comments,https://api.github.com/repos/apache/spark/issues/34060/events,https://github.com/apache/spark/pull/34060,1002859597,PR_kwDOAQXtWs4sD3GL,34060,[SPARK-36850][SQL] Migrate CreateTableStatement to v2 command framework,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-21T16:52:03Z,2021-09-29T00:21:51Z,,CONTRIBUTOR,,"
### What changes were proposed in this pull request?
Migrate `CreateTableStatement` to v2 command framework


### Why are the changes needed?
Migrate to the standard V2 framework

### Does this PR introduce _any_ user-facing change?
no


### How was this patch tested?
existing tests
",https://api.github.com/repos/apache/spark/issues/34060/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34060,https://github.com/apache/spark/pull/34060,https://github.com/apache/spark/pull/34060.diff,https://github.com/apache/spark/pull/34060.patch,https://api.github.com/repos/apache/spark/issues/34060/reactions,0,0,0,0,0,0,0,0,0
42,https://api.github.com/repos/apache/spark/issues/34058,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34058/labels{/name},https://api.github.com/repos/apache/spark/issues/34058/comments,https://api.github.com/repos/apache/spark/issues/34058/events,https://github.com/apache/spark/pull/34058,1002614033,PR_kwDOAQXtWs4sDBWG,34058,[SPARK-36711][PYTHON] Support multi-index in new syntax,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2021-09-21T14:36:00Z,2021-09-27T07:29:30Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Support multi-index in new syntax to specify index data type

### Why are the changes needed?
Support multi-index in new syntax to specify index data type

https://issues.apache.org/jira/browse/SPARK-36707

### Does this PR introduce _any_ user-facing change?
After this PR user can use

``` python
>>> ps.DataFrame[[int, int],[int, int]]
typing.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]


>>> arrays = [[1, 1, 2], ['red', 'blue', 'red']]
>>> idx = pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))
>>> pdf = pd.DataFrame([[1,2,3],[2,3,4],[4,5,6]], index=idx, columns=[""a"", ""b"", ""c""])
>>> ps.DataFrame[pdf.index.dtypes, pdf.dtypes]
typing.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]


>>> ps.DataFrame[[(""index"", int), (""index-2"", int)], [(""id"", int), (""A"", int)]]
typing.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]


>>> ps.DataFrame[zip(pdf.index.names, pdf.index.dtypes), zip(pdf.columns, pdf.dtypes)]
typing.Tuple[pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.IndexNameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType, pyspark.pandas.typedef.typehints.NameType]

```

### How was this patch tested?
exist tests
",https://api.github.com/repos/apache/spark/issues/34058/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34058,https://github.com/apache/spark/pull/34058,https://github.com/apache/spark/pull/34058.diff,https://github.com/apache/spark/pull/34058.patch,https://api.github.com/repos/apache/spark/issues/34058/reactions,0,0,0,0,0,0,0,0,0
43,https://api.github.com/repos/apache/spark/issues/34056,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34056/labels{/name},https://api.github.com/repos/apache/spark/issues/34056/comments,https://api.github.com/repos/apache/spark/issues/34056/events,https://github.com/apache/spark/pull/34056,1001956323,PR_kwDOAQXtWs4sA024,34056,"[SPARK-36811][SQL] Add SQL functions for the BINARY data type for AND, OR, XOR, and NOT","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-09-21T06:52:48Z,2021-09-27T22:28:09Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?

This PR introduces four new SQL functions operating on the BINARY data type.
  1. `bitand`: Takes as input two binary strings and returns their bitwise AND.
  2. `bitor`: Takes as input two binary strings and returns their bitwise OR.
  3. `bitxor`: Takes as input two binary strings and returns their bitwise XOR.
  4. `bitnot`: Takes as input a binary string and returns its bitwise NOT.

For the first three functions the byte length of the result is the maximum of the byte length of the two inputs.

For the first three functions the shortest (in terms of byte length) binary string is semantically left-padded by zeros.

### Why are the changes needed?

These functions are useful for performing bitwise operations on `BINARY` values, seen as bit sets.

Other databases offer similar or the same functionality. In more detail:
* Teradata supports them as [functions](https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/zNvtiufFYVtAA~wqxccYKA).
* Postgres supports them as [operators](https://www.postgresql.org/docs/9.4/functions-bitstring.html) over the `BIT` data type with the restriction that the inputs must be of equal length.
* MySQL supports them as [operators](https://dev.mysql.com/doc/refman/8.0/en/bit-functions.html). Arguments must have the same length.
* SQL Server supports them as [operators](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/bitwise-operators-transact-sql?view=sql-server-ver15), but only one of the two arguments can be a binary string.

Oracle, Snowflake, Hive SQL, Big Query do not support the proposed functions for binary strings, but rather only for integral types.

### Does this PR introduce _any_ user-facing change?

Yes. Four new SQL functions.

### How was this patch tested?

Unit tests.
",https://api.github.com/repos/apache/spark/issues/34056/timeline,,spark,apache,mkaravel,6397014,MDQ6VXNlcjYzOTcwMTQ=,https://avatars.githubusercontent.com/u/6397014?v=4,,https://api.github.com/users/mkaravel,https://github.com/mkaravel,https://api.github.com/users/mkaravel/followers,https://api.github.com/users/mkaravel/following{/other_user},https://api.github.com/users/mkaravel/gists{/gist_id},https://api.github.com/users/mkaravel/starred{/owner}{/repo},https://api.github.com/users/mkaravel/subscriptions,https://api.github.com/users/mkaravel/orgs,https://api.github.com/users/mkaravel/repos,https://api.github.com/users/mkaravel/events{/privacy},https://api.github.com/users/mkaravel/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34056,https://github.com/apache/spark/pull/34056,https://github.com/apache/spark/pull/34056.diff,https://github.com/apache/spark/pull/34056.patch,https://api.github.com/repos/apache/spark/issues/34056/reactions,0,0,0,0,0,0,0,0,0
44,https://api.github.com/repos/apache/spark/issues/34046,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34046/labels{/name},https://api.github.com/repos/apache/spark/issues/34046/comments,https://api.github.com/repos/apache/spark/issues/34046/events,https://github.com/apache/spark/pull/34046,1000564705,PR_kwDOAQXtWs4r8Y2M,34046,[SPARK-36804][YARN] Support --verbose option in YARN mode,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2021-09-20T04:37:34Z,2021-10-03T17:12:50Z,,NONE,,"### What changes were proposed in this pull request?
Support --verbose option in YARN mode


### Why are the changes needed?
If we submit the spark application with the --verbose parameter in yarn mode, we will get the following exception:
```bash
Exception in thread ""main"" java.lang.IllegalArgumentException: Unknown/unsupported param List(--verbose)Exception in thread ""main""
```
SparkSubmit invoke YarnClusterApplication with --verbose arguments, however ClientArguments used by YarnClusterApplication don't support that argument by now, as a result, IllegalArgumentException is thrown in ClientArguments. I think we can support --verbose in YARN mode to keep consistency with other module


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
After apply this patch, application will submit successful
",https://api.github.com/repos/apache/spark/issues/34046/timeline,,spark,apache,daugraph,40060929,MDQ6VXNlcjQwMDYwOTI5,https://avatars.githubusercontent.com/u/40060929?v=4,,https://api.github.com/users/daugraph,https://github.com/daugraph,https://api.github.com/users/daugraph/followers,https://api.github.com/users/daugraph/following{/other_user},https://api.github.com/users/daugraph/gists{/gist_id},https://api.github.com/users/daugraph/starred{/owner}{/repo},https://api.github.com/users/daugraph/subscriptions,https://api.github.com/users/daugraph/orgs,https://api.github.com/users/daugraph/repos,https://api.github.com/users/daugraph/events{/privacy},https://api.github.com/users/daugraph/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34046,https://github.com/apache/spark/pull/34046,https://github.com/apache/spark/pull/34046.diff,https://github.com/apache/spark/pull/34046.patch,https://api.github.com/repos/apache/spark/issues/34046/reactions,0,0,0,0,0,0,0,0,0
45,https://api.github.com/repos/apache/spark/issues/34042,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34042/labels{/name},https://api.github.com/repos/apache/spark/issues/34042/comments,https://api.github.com/repos/apache/spark/issues/34042/events,https://github.com/apache/spark/pull/34042,1000390929,PR_kwDOAQXtWs4r76LL,34042,"[SPARK-36801][DOCS] ADD ""All columns are automatically converted to be nullable for compatibility reasons."" IN SPARK SQL JDBC DOCUMENT","[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-09-19T18:57:46Z,2021-09-21T07:25:19Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Added a line ""All columns are automatically converted to be nullable for compatibility reasons."" in Documentation[1].

 Ref:

[1 ]https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases

 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
<p>Reading using Spark SQL jdbc DataSource does not maintain nullable type and changes ""non nullable"" columns to ""nullable"".</p>

<p></p>

<p>For example:</p>

<p>mysql&gt; CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);<br>
Query OK, 0 rows affected (0.04 sec)</p>

Spark-shell:

val df = spark.read.format(""jdbc"").option(""database"",""Test_DB"").option(""user"", ""root"").option(""password"", """").option(""driver"", ""com.mysql.cj.jdbc.Driver"").option(""url"", ""jdbc:mysql://localhost:3306/Test_DB"").option(""query"", ""(select * from Persons)"").load()df.printSchema()

output:

root
 Id: integer (nullable = true)
 FirstName: string (nullable = true)
 LastName: string (nullable = true)
 Age: integer (nullable = true)

<p>So we need to add a note, in Documentation<span class=""error"">[1]</span>, ""All columns are automatically converted to be nullable for compatibility reasons.""</p>

<p>Ref:</p>

<p><span class=""error"">[1 ]</span><a href=""https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases"" class=""external-link"" rel=""nofollow"" title=""Follow link"">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases</a></p>

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, Document changes

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Tested using ""SKIP_API=1 bundle exec jekyll build""",https://api.github.com/repos/apache/spark/issues/34042/timeline,,spark,apache,senthh,9917543,MDQ6VXNlcjk5MTc1NDM=,https://avatars.githubusercontent.com/u/9917543?v=4,,https://api.github.com/users/senthh,https://github.com/senthh,https://api.github.com/users/senthh/followers,https://api.github.com/users/senthh/following{/other_user},https://api.github.com/users/senthh/gists{/gist_id},https://api.github.com/users/senthh/starred{/owner}{/repo},https://api.github.com/users/senthh/subscriptions,https://api.github.com/users/senthh/orgs,https://api.github.com/users/senthh/repos,https://api.github.com/users/senthh/events{/privacy},https://api.github.com/users/senthh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34042,https://github.com/apache/spark/pull/34042,https://github.com/apache/spark/pull/34042.diff,https://github.com/apache/spark/pull/34042.patch,https://api.github.com/repos/apache/spark/issues/34042/reactions,0,0,0,0,0,0,0,0,0
46,https://api.github.com/repos/apache/spark/issues/34041,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34041/labels{/name},https://api.github.com/repos/apache/spark/issues/34041/comments,https://api.github.com/repos/apache/spark/issues/34041/events,https://github.com/apache/spark/pull/34041,1000089920,PR_kwDOAQXtWs4r7M9V,34041,[SPARK-36799][SQL] Pass queryExecution name in CLI when only select query,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-18T17:32:07Z,2021-09-29T10:58:49Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
When sql is only a select query, call `SQLExecution.withNewExecutionId` and specify `collect` as `executionName` so that `QueryExecutionListener` can get the query.
### Why are the changes needed?
Now when in spark-sql CLI, `QueryExecutionListener` can receive command , but not select query.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
manual test.",https://api.github.com/repos/apache/spark/issues/34041/timeline,,spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34041,https://github.com/apache/spark/pull/34041,https://github.com/apache/spark/pull/34041.diff,https://github.com/apache/spark/pull/34041.patch,https://api.github.com/repos/apache/spark/issues/34041/reactions,0,0,0,0,0,0,0,0,0
47,https://api.github.com/repos/apache/spark/issues/34039,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34039/labels{/name},https://api.github.com/repos/apache/spark/issues/34039/comments,https://api.github.com/repos/apache/spark/issues/34039/events,https://github.com/apache/spark/pull/34039,999975696,PR_kwDOAQXtWs4r673y,34039,[SPARK-36798][CORE] Wait for listeners to finish before flushing metrics,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-09-18T08:48:34Z,2021-09-28T08:45:25Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
When `SparkContext` is shutting down, wait for listener bus to finish and then only flush `MetricsSystem`.


### Why are the changes needed?
In current implementation, when `SparkContext.stop()` is called, `metricsSystem.report()` is called before `listenerBus.stop()`. In this case, if some listener is producing some metrics, they would never reach sink.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
NA
",https://api.github.com/repos/apache/spark/issues/34039/timeline,,spark,apache,BOOTMGR,3242264,MDQ6VXNlcjMyNDIyNjQ=,https://avatars.githubusercontent.com/u/3242264?v=4,,https://api.github.com/users/BOOTMGR,https://github.com/BOOTMGR,https://api.github.com/users/BOOTMGR/followers,https://api.github.com/users/BOOTMGR/following{/other_user},https://api.github.com/users/BOOTMGR/gists{/gist_id},https://api.github.com/users/BOOTMGR/starred{/owner}{/repo},https://api.github.com/users/BOOTMGR/subscriptions,https://api.github.com/users/BOOTMGR/orgs,https://api.github.com/users/BOOTMGR/repos,https://api.github.com/users/BOOTMGR/events{/privacy},https://api.github.com/users/BOOTMGR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34039,https://github.com/apache/spark/pull/34039,https://github.com/apache/spark/pull/34039.diff,https://github.com/apache/spark/pull/34039.patch,https://api.github.com/repos/apache/spark/issues/34039/reactions,0,0,0,0,0,0,0,0,0
48,https://api.github.com/repos/apache/spark/issues/34035,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34035/labels{/name},https://api.github.com/repos/apache/spark/issues/34035/comments,https://api.github.com/repos/apache/spark/issues/34035/events,https://github.com/apache/spark/pull/34035,999661296,PR_kwDOAQXtWs4r6Ah1,34035,[SPARK-36793][K8S] Support write container stdout/stderr to file,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-17T19:33:38Z,2021-09-23T22:12:11Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Support write container stdout/stderr to file

### Why are the changes needed?
If users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.

### Does this PR introduce _any_ user-facing change?
Yes. User can enable this feature by spark config.

### How was this patch tested?
Added UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite
",https://api.github.com/repos/apache/spark/issues/34035/timeline,,spark,apache,warrenzhu25,1633312,MDQ6VXNlcjE2MzMzMTI=,https://avatars.githubusercontent.com/u/1633312?v=4,,https://api.github.com/users/warrenzhu25,https://github.com/warrenzhu25,https://api.github.com/users/warrenzhu25/followers,https://api.github.com/users/warrenzhu25/following{/other_user},https://api.github.com/users/warrenzhu25/gists{/gist_id},https://api.github.com/users/warrenzhu25/starred{/owner}{/repo},https://api.github.com/users/warrenzhu25/subscriptions,https://api.github.com/users/warrenzhu25/orgs,https://api.github.com/users/warrenzhu25/repos,https://api.github.com/users/warrenzhu25/events{/privacy},https://api.github.com/users/warrenzhu25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34035,https://github.com/apache/spark/pull/34035,https://github.com/apache/spark/pull/34035.diff,https://github.com/apache/spark/pull/34035.patch,https://api.github.com/repos/apache/spark/issues/34035/reactions,0,0,0,0,0,0,0,0,0
49,https://api.github.com/repos/apache/spark/issues/34030,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34030/labels{/name},https://api.github.com/repos/apache/spark/issues/34030/comments,https://api.github.com/repos/apache/spark/issues/34030/events,https://github.com/apache/spark/pull/34030,999006222,PR_kwDOAQXtWs4r4CLl,34030,[SPARK-36790][SQL] Update user-facing catalog to adapt CatalogPlugin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-09-17T06:49:06Z,2021-09-23T15:47:03Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Change the CatalogImpl for user-facing catalog API.
before:
`private def sessionCatalog: SessionCatalog = sparkSession.sessionState.catalog`
So all the operations we did were based on SessionCatalog any time.

after:
`private def currentCatalog: CatalogPlugin = sparkSession.sessionState.catalogManager.currentCatalog`
When current catalog is `spark-catalog`, we do the all operations based on SessionCatalog.
Others CatalogPlugin did not support operations  at now. it will throw exception temporarily.
eg: `databaseExists` ,`currentDatabase` and so on.

### Why are the changes needed?
[#SPARK-36790](https://issues.apache.org/jira/browse/SPARK-36790)
User-facting catalog should be access the current catalog.
eg:
`
spark.sql(""use testcat"")
`
so: current catalog != spark.catalog

`
spark.sql(""use testcat.ns1"")
`
so: current database != spark.catalog.currentDatabase

`spark.sql(""show tables"")
`
so: result != spark.catalog.listTables

I think the SparkSession.catalog api should be keep exist instead of SparkSession.sessionState.catalogManager. So update user-facing catalog to adapt CatalogPlugin. At now just throw exception for unsupport the CatalogPlugin temporary.

### Does this PR introduce _any_ user-facing change? 
No


### How was this patch tested?
add ut test",https://api.github.com/repos/apache/spark/issues/34030/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34030,https://github.com/apache/spark/pull/34030,https://github.com/apache/spark/pull/34030.diff,https://github.com/apache/spark/pull/34030.patch,https://api.github.com/repos/apache/spark/issues/34030/reactions,0,0,0,0,0,0,0,0,0
50,https://api.github.com/repos/apache/spark/issues/34024,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34024/labels{/name},https://api.github.com/repos/apache/spark/issues/34024/comments,https://api.github.com/repos/apache/spark/issues/34024/events,https://github.com/apache/spark/pull/34024,998527855,PR_kwDOAQXtWs4r2lZh,34024,[SPARK-36784][SHUFFLE][WIP] Handle DNS issues on executor to prevent shuffle nodes from getting added to exclude list,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-16T18:53:36Z,2021-09-29T17:37:18Z,,CONTRIBUTOR,," ### What changes were proposed in this pull request?

 When a DNS issue happens on the executor node, shuffle nodes would get added to the exclude list due to FetchFailed exception. The change here is to have a configuration host value to test DNS resolution against before marking it as a FetchFailed Exception.

 ### Why are the changes needed?

 This would prevent shuffle nodes from getting added to the exclude list due to DNS issues

 ### Does this PR introduce _any_ user-facing change?

 No

 ### How was this patch tested?

 Added Unit Tests
",https://api.github.com/repos/apache/spark/issues/34024/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34024,https://github.com/apache/spark/pull/34024,https://github.com/apache/spark/pull/34024.diff,https://github.com/apache/spark/pull/34024.patch,https://api.github.com/repos/apache/spark/issues/34024/reactions,0,0,0,0,0,0,0,0,0
51,https://api.github.com/repos/apache/spark/issues/34010,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34010/labels{/name},https://api.github.com/repos/apache/spark/issues/34010/comments,https://api.github.com/repos/apache/spark/issues/34010/events,https://github.com/apache/spark/pull/34010,997566830,PR_kwDOAQXtWs4rzp_v,34010,"[SPARK-36770][SQL] Replace Unbounded Following window functions with Unbounded Preceding window function (First, Last)","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-15T22:19:22Z,2021-09-23T22:50:19Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
**Context**

The **UnboundedFollowingWindowFunctionFrame** has the time complexity of O(N^2), N is the number of rows in the current partition, more specific the complexity is O(N* (N - 1)/2).

**What happens internally in UnboundedFollowingWindowFunctionFrame?**
In the window frame, while processing each incoming row, it will go through current row till the end of partition to do re-calculation. This process will be repeated on each incoming row, which causes the high run-time complexity.

But UnboundedPrecedingWindowFunctionFrame has much better time complexity O(N), N is the number of rows in the current partition.

 
**What is the idea of the improvement?**

Give the big time complexity difference between UnboundedFollowingWindowFunctionFrame and UnboundedPrecedingWindowFunctionFrame, we can do following conversions to improve the time complexity of first() and last() from O(N^2) to O(N)

```
case 1:
first() OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING)
converts to
last()  OVER(PARTITION BY colA ORDER BY colB DEAC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)

case 2:
last()  OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING)
converts to 
first() OVER(PARTITION BY colA ORDER BY colB DESC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)
```

**Summary**

Replace ""UNBOUNDED FOLLOWING"" with ""UNBOUNDED PRECEDING"", and flip the ORDER BY for the window functions first() and last() for ROWS.


### Why are the changes needed?
Improve the run-time performance for window function fist() and last() against ROWS UNBOUNDED FOLLOWING.


### Does this PR introduce _any_ user-facing change?
NO


### How was this patch tested?
Added unit test: sql/core/src/test/scala/org/apache/spark/sql/WindowFunctionOptimizerTestSuite.scala

",https://api.github.com/repos/apache/spark/issues/34010/timeline,,spark,apache,guibin,289256,MDQ6VXNlcjI4OTI1Ng==,https://avatars.githubusercontent.com/u/289256?v=4,,https://api.github.com/users/guibin,https://github.com/guibin,https://api.github.com/users/guibin/followers,https://api.github.com/users/guibin/following{/other_user},https://api.github.com/users/guibin/gists{/gist_id},https://api.github.com/users/guibin/starred{/owner}{/repo},https://api.github.com/users/guibin/subscriptions,https://api.github.com/users/guibin/orgs,https://api.github.com/users/guibin/repos,https://api.github.com/users/guibin/events{/privacy},https://api.github.com/users/guibin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34010,https://github.com/apache/spark/pull/34010,https://github.com/apache/spark/pull/34010.diff,https://github.com/apache/spark/pull/34010.patch,https://api.github.com/repos/apache/spark/issues/34010/reactions,0,0,0,0,0,0,0,0,0
52,https://api.github.com/repos/apache/spark/issues/34009,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34009/labels{/name},https://api.github.com/repos/apache/spark/issues/34009/comments,https://api.github.com/repos/apache/spark/issues/34009/events,https://github.com/apache/spark/pull/34009,997562866,PR_kwDOAQXtWs4rzpOV,34009,[SPARK-34378][SQL][AVRO] Enhance AvroSerializer validation to allow extra nullable Avro fields,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1988040187, 'node_id': 'MDU6TGFiZWwxOTg4MDQwMTg3', 'url': 'https://api.github.com/repos/apache/spark/labels/AVRO', 'name': 'AVRO', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2021-09-15T22:12:39Z,2021-09-26T03:21:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Loosen the schema validation logic in `AvroSerializer` to accommodate the situation where a user has provided an explicit schema (via `avroSchema`) and this schema has extra fields which are not present in the Catalyst schema (the DF being written). Specifically, extra _nullable_ fields will be allowed and populated as null. _Required_ fields (non-null) will still be checked for existence.

### Why are the changes needed?
It's common for Avro schemas to evolve in a _compatible_ way (as discussed in Confluent's documentation on [Schema Evolution and Compatibility](https://docs.confluent.io/platform/current/schema-registry/avro.html); here I refer to `FULL` compatibility). Under such a scenario, new _optional_ fields are added to a schema. Producers are free to include the new field if they so choose, and consumers are free to read the new field if they so choose. It is optional on both sides.

Consider the following code:
```
val outputSchema = getOutputSchema()
df.write.format(""avro"").option(""avroSchema"", outputSchema).save(...)
```
If you have a situation where schemas are managed in some centralized repository (e.g. a [schema registry](https://docs.confluent.io/platform/current/schema-registry/index.html)), `outputSchema` may update at some point to add a new optional field, without you necessarily initiating any action on your side as a data producer. With the current code, this would cause the producer job to break, because validation would complain that the newly added field is not present in the DataFrame. Really, the producer should be able to continue producing data as normal even without adding the new field to the DataFrame it is writing out, because the field is optional.

### Does this PR introduce _any_ user-facing change?
Yes, when using the `avroSchema` option on the Avro data source during writes, validation is less strict, and allows for (compatible) schema evolution to be handled more gracefully.

### How was this patch tested?
New unit tests added. We've also been employing this logic internally for a few years, though the implementation was quite different due to recent changes in this area of the code.",https://api.github.com/repos/apache/spark/issues/34009/timeline,,spark,apache,xkrogen,6570401,MDQ6VXNlcjY1NzA0MDE=,https://avatars.githubusercontent.com/u/6570401?v=4,,https://api.github.com/users/xkrogen,https://github.com/xkrogen,https://api.github.com/users/xkrogen/followers,https://api.github.com/users/xkrogen/following{/other_user},https://api.github.com/users/xkrogen/gists{/gist_id},https://api.github.com/users/xkrogen/starred{/owner}{/repo},https://api.github.com/users/xkrogen/subscriptions,https://api.github.com/users/xkrogen/orgs,https://api.github.com/users/xkrogen/repos,https://api.github.com/users/xkrogen/events{/privacy},https://api.github.com/users/xkrogen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34009,https://github.com/apache/spark/pull/34009,https://github.com/apache/spark/pull/34009.diff,https://github.com/apache/spark/pull/34009.patch,https://api.github.com/repos/apache/spark/issues/34009/reactions,0,0,0,0,0,0,0,0,0
53,https://api.github.com/repos/apache/spark/issues/34000,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34000/labels{/name},https://api.github.com/repos/apache/spark/issues/34000/comments,https://api.github.com/repos/apache/spark/issues/34000/events,https://github.com/apache/spark/pull/34000,996465213,PR_kwDOAQXtWs4rwNS7,34000,[SPARK-36620][SHUFFLE] Add client side push based shuffle metrics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-09-14T21:42:22Z,2021-09-16T19:02:39Z,,CONTRIBUTOR,," ### What changes were proposed in this pull request?
This adds the following push based shuffle metrics like :
- Merger count and magnet enabled/disabled for stage
- Time spent on results finalization
- Counting actual number of blocks other than chunks
- Corrupt shuffle blocks chunks and fallback

 ### Why are the changes needed?
These changes help to understand the push based shuffle metrics of the application

 ### Does this PR introduce _any_ user-facing change?
Changes to API responses by SHS (eg: /stages)

 ### How was this patch tested?
Modified existing unit tests and also tested API response on event log files in SHS

Lead-authored by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>
Co-authored by: Chandni Singh <chsingh@linkedin.com>
Co-authored by: Thejdeep Gudivada <tgudivada@linkedin.com>",https://api.github.com/repos/apache/spark/issues/34000/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34000,https://github.com/apache/spark/pull/34000,https://github.com/apache/spark/pull/34000.diff,https://github.com/apache/spark/pull/34000.patch,https://api.github.com/repos/apache/spark/issues/34000/reactions,0,0,0,0,0,0,0,0,0
54,https://api.github.com/repos/apache/spark/issues/33991,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33991/labels{/name},https://api.github.com/repos/apache/spark/issues/33991/comments,https://api.github.com/repos/apache/spark/issues/33991/events,https://github.com/apache/spark/pull/33991,995642623,PR_kwDOAQXtWs4rtjPL,33991,[SPARK-36750][CORE][SQL][MLLIB][YARN] Replace the Guava API with `java.util.Objects` API with the same semantics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-14T06:31:27Z,2021-09-14T09:44:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Java 8 provides the` java.util.Objects`, this pr it to replace some Guava API usages with the same semantics, the replacement rules are as follows:

1. Preconditions.checkNotNull -> Objects.requireNonNull

**Guava**
```java
public static <T> T checkNotNull(T reference,
      @Nullable String errorMessageTemplate,
      @Nullable Object... errorMessageArgs) {
    if (reference == null) {
      // If either of these parameters is null, the right thing happens anyway
      throw new NullPointerException(
          format(errorMessageTemplate, errorMessageArgs));
    }
    return reference;
  }
```

**java.util.Objects**

```java
    public static <T> T requireNonNull(T obj, Supplier<String> messageSupplier) {
        if (obj == null)
            throw new NullPointerException(messageSupplier.get());
        return obj;
    }
```


2. Objects.hashCode -> j.u.Objects.hash

**Guava**
```java
  public static int hashCode(@Nullable Object... objects) {
    return java.util.Arrays.hashCode(objects);
  }
```

**java.util.Objects**

```java
    public static int hash(Object... values) {
        return java.util.Arrays.hashCode(values);
    }
```
3. Objects.equal -> j.u.Objects.equals

**Guava**
```java
  public static boolean equal(@Nullable Object a, @Nullable Object b) {
    return a == b || (a != null && a.equals(b));
  }
```

**java.util.Objects**

```java
    public static boolean equals(Object a, Object b) {
        return (a == b) || (a != null && a.equals(b));
    }
```

### Why are the changes needed?
Reduce dependence on Guava API because Spark still use Guava 14.0.1.



### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Pass the Jenkins or GitHub Action
",https://api.github.com/repos/apache/spark/issues/33991/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33991,https://github.com/apache/spark/pull/33991,https://github.com/apache/spark/pull/33991.diff,https://github.com/apache/spark/pull/33991.patch,https://api.github.com/repos/apache/spark/issues/33991/reactions,0,0,0,0,0,0,0,0,0
55,https://api.github.com/repos/apache/spark/issues/33989,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33989/labels{/name},https://api.github.com/repos/apache/spark/issues/33989/comments,https://api.github.com/repos/apache/spark/issues/33989/events,https://github.com/apache/spark/pull/33989,995623831,PR_kwDOAQXtWs4rtfgI,33989,[SPARK-36676][SQL][BUILD] Create shaded Hive module and upgrade Guava version to 30.1.1-jre,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2021-09-14T06:03:39Z,2021-09-28T19:47:32Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR creates a new module `hive-shaded` which shades & relocates various dependencies from Hive, including Guava. By this means it also upgrades the Guava version from 14.0.1 to 30.1.1-jre.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Spark currently use `hive-exec-core` which leaks a lot of dependencies to Spark, in particular Guava. As consequence, Spark is stuck with an ancient Guava version 14.0.1 which also carries CVE issues described in [SPARK-32502](https://issues.apache.org/jira/browse/SPARK-32502). 

By creating a shaded module, Spark is able to de-couple from those dependencies leaked by Hive and upgrade to newer versions of Guava. This also allows us to upgrade other dependencies whenever necessary, without having to wait for new Hive releases.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No. For Spark users who programmatically depending on modules such as `spark-hive`, the Hive dependencies will be replaced by the newly created `hive-shaded` module.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/33989/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33989,https://github.com/apache/spark/pull/33989,https://github.com/apache/spark/pull/33989.diff,https://github.com/apache/spark/pull/33989.patch,https://api.github.com/repos/apache/spark/issues/33989/reactions,7,7,0,0,0,0,0,0,0
56,https://api.github.com/repos/apache/spark/issues/33986,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33986/labels{/name},https://api.github.com/repos/apache/spark/issues/33986/comments,https://api.github.com/repos/apache/spark/issues/33986/events,https://github.com/apache/spark/pull/33986,995496899,PR_kwDOAQXtWs4rtG9r,33986,Support sql overwrite a path that is also being read from when partit,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-14T01:49:05Z,2021-09-28T08:56:49Z,,NONE,,"### What changes were proposed in this pull request?
```
// non-partitioned table overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET;
INSERT OVERWRITE TABLE tbl SELECT 0,1;
INSERT OVERWRITE TABLE tbl SELECT * FROM tbl;

// partitioned table static overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET PARTITIONED BY (pt1 INT);
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT 0 AS col1,1 AS col2;
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT col1, col2 FROM WHERE p1=2021;
```
When we run the above query, an error will be throwed ""Cannot overwrite a path that is also being read from""
We need to support this operation when the spark.sql.sources.partitionOverwriteMode is dynamic

### How was this patch tested?
Unit tests -> InsertSuite.scala
",https://api.github.com/repos/apache/spark/issues/33986/timeline,,spark,apache,TongWeii,68682646,MDQ6VXNlcjY4NjgyNjQ2,https://avatars.githubusercontent.com/u/68682646?v=4,,https://api.github.com/users/TongWeii,https://github.com/TongWeii,https://api.github.com/users/TongWeii/followers,https://api.github.com/users/TongWeii/following{/other_user},https://api.github.com/users/TongWeii/gists{/gist_id},https://api.github.com/users/TongWeii/starred{/owner}{/repo},https://api.github.com/users/TongWeii/subscriptions,https://api.github.com/users/TongWeii/orgs,https://api.github.com/users/TongWeii/repos,https://api.github.com/users/TongWeii/events{/privacy},https://api.github.com/users/TongWeii/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33986,https://github.com/apache/spark/pull/33986,https://github.com/apache/spark/pull/33986.diff,https://github.com/apache/spark/pull/33986.patch,https://api.github.com/repos/apache/spark/issues/33986/reactions,0,0,0,0,0,0,0,0,0
57,https://api.github.com/repos/apache/spark/issues/33983,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33983/labels{/name},https://api.github.com/repos/apache/spark/issues/33983/comments,https://api.github.com/repos/apache/spark/issues/33983/events,https://github.com/apache/spark/pull/33983,995447730,PR_kwDOAQXtWs4rs9iV,33983,[SPARK-33152] [SQL] New algorithm for ConstraintsPropagation rule to solve the problem of performance & OOM if the query plans have large expressions involving multiple aliases,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-14T00:07:41Z,2021-10-01T19:57:30Z,,NONE,,"### What changes were proposed in this pull request?

This PR proposes new algorithm to create &  store the constraints.
It tracks aliases in projection which eliminates the need of pessimistically generating all the permutations of a given constraint. It is also more effective in correctly identifying the filters which can be pruned , apart from minimizing the memory used as compared to the current code. This also has changes to push compound filters if the join condition is on multiple attributes and the constraint comprises of more than 1 attributes of the join conditions.

Presently I have kept the code which retains the old logic of constraint management along with the new logic. It is controlled by the sql conf property **spark.sql.optimizer.optimizedConstraintPropagation.enabled** which is by default true. Once the PR is approved it would make sense to remove the old code & merge the code of ConstraintSet into ExpressionSet and removing some if else blocks in the Optimizer & the function Optimizer.getAllConstraints and LogicalPlan.getAllValidConstraints.


The new logic is as follows:
In the class ConstraintSet which extends ExpressionSet, we track the aliases , along with the base constraint.
Any constraint which is added to the ConstraintSet is stored in the most canonicalized form (i.e consisting of only those attributes which are part of the output set and NOT the Alias's attribute).

for eg consider a hypothetical plan

>            Filter( z > 10 && a1 + b2 > 10)
                                 |
>         Projection1 ( a, a as a1, a as a2, b , b as b1, b as b2, c, a +b as z)
                                 |
 >                 Filter ( a + b > 10)
                                 |
 >              base relation (a, b, c, d)

At the node Filter the constraint set will just have constraint a + b > 10
At the Node Projection1 , the constraint set will have
constraint a + b > 10
and maintain following buffers
buff1 -> a , a1.attribute, a2. attribute
buff2 -> b, b1.attribute, b2.attribute
buff3 -> a + b, z.attribute

constraint a + b > 10 is already canonicalized in terms of output attributes.

Now there are two filters on top of projection1
Filter( z > 10) and Filter ( a1 + b2 > 10)

To prune the above two filters, we canonicalize z as a + b ( from the data maintained in the ConstraintSet) & check if the underlying set contains a +b > 10 & so can be pruned.
For Filter a1 + b2 > 10, we identify the buffer to which a1 & b2 belong to and replace it with 0th elements of the buffer, which will yield a +b > 10, and so filter can be pruned.

Now suppose there is another Project2 ( a1, a2, b1, b2, z, c)
i.e say attributes a & b are no longer part of OutputSet.

such that the plan looks like:
>         Projection2 ( a1,  a2,  b1,  b2, c,  z)
                                |
>            Filter( z > 10 && a1 + b2 > 10)
                                 |
>         Projection1 ( a, a as a1, a as a2, b , b as b1, b as b2, c, a +b as z)
                                 |
 >                 Filter ( a + b > 10)
                                 |
 >              base relation (a, b, c, d)

**The idea is that ""as much as possible try to make a constraint survive.**

So in Project2 , the atttributes a & b are being eliminated.
we have a constraint a + b > 10 which is dependent on it.
so in the ConstraintSet of the ProjectP2, we update it such that
constraint a + b > 10 becomes ----> a1 + b1 > 10
**buff1   ->  a , a1, a2   will become --> a1, a2
buff2  ->  b , b1, b2.  will become  --> b1, b2
buff3   ->  a +b , z  will become  -->. a1 + b1 , z**

This way by tracking aliases & just storing the canonicalized base constraints we can eliminate the need of pessimistically generating all combination of constraints.

**This PR also eliminates the need of EqualNullSafe constraints for the alias.
It also is able to handle the literal boolean constraints.**

**_ For inferring new Filter from constraints _**
we use following logic
New Filter = Filter.constraints -- ( Filter.child.constraints ++ Filter.constraints.convertToCanonicalizedIfRequired(Filter.conditions) )
So the idea is that new filter conditions without redundancy can be obtained by difference of current node's constraints & the child node's constraints & the condition itself properly canonicalized in terms of base attributes which will be part of the output set of filter node.

**_For inferring new filters for Join push down,_**
 we identify all the equality conditions & then the attributes are segregated on the lines of LHS & RHS of joins. So to identify filters for push down on RHS side, we get all equality atttributes of LHS side & ask the ConstraintSet to return all the constraints which are subset of the passed LHS attributes. The LHS attributes are appropriately canonicalized & the ConstraintSet identified.
Once the constraints are know, we can replace the attributes with the corresponding RHS attributes. This helps in identifying the compound filters for push down & not just single attribute filters.

_**Below is a description of the changes proposed.**_

ConstraintSet: This is the class which does the tracking of the aliases , stores the constraints in the canonicalized form, updates the constraints using available aliases if any of the attribute comprising constraint is getting eliminated. The contains method of this class is used for filter pruning. It also identifies those constraints which can generated new filters for push down in join nodes.
Rest all the changes are just to integrate the new logic as well as retain the old constraints logic.
Pls notice that related to tpcds plan stability , I had to add new golden files for q75. The change as such is trivial.
previously pushed filter was generated as
PushedFilters: [IsNotNull(cr_order_number), IsNotNull(cr_item_sk)]
and with the change it is
PushedFilters: [IsNotNull(cr_item_sk), IsNotNull(cr_order_number)]


### Does this PR introduce any user-facing change?
No


### Why are the changes needed?

1. This issue if not fixed can cause OutOfMemory issue or unacceptable query compilation times.
Added a test **""plan equivalence with case statements and performance comparison with benefit of more than 10x conservatively""  in  org.apache.spark.sql.catalyst.plans.OptimizedConstraintPropagationSuite**. With this PR the compilation _**time is 247 ms vs 13958 ms without the change**_
2. It is more effective in filter pruning as is evident in some of the tests in org.apache.spark.sql.catalyst.plans.OptimizedConstraintPropagationSuite where current code is not able to identify the redundant filter in some cases.
3. It is able to generate a better optimized plan for join queries as it can push compound predicates.
4. The current logic can miss a lot of possible cases of removing redundant predicates, as it fails to take into account if same attribute or its aliases are repeated multiple times in a complex expression.
5. There are cases where some of the optimizer rules involving removal of redundant predicates fail to remove on the basis of constraint data. In some cases the rule works, just by the virtue of previous rules helping it out to cover the inaccuracy. That the ConstraintPropagation rule & its function of removal of redundant filters & addition of new inferred filters is dependent on the working of some of the other unrelated previous optimizer rules is behaving, is indicative of issues.
6. It does away with all the EqualNullSafe constraints as this logic does not need those constraints to be created.
7.  There is atleast one test in existing **ConstraintPropagationSuite** which is missing a IsNotNull constraints because the code incorrectly generated a EqualsNullSafeConstraint instead of EqualTo constraint, when using the existing Constraints code.  With these changes, the test correctly creates an EqualTo constraint, resulting in an inferred IsNotNull constraint
8. It does away with the current combinatorial  logic of evaluation all the constraints can cause compilation to run into hours or cause OOM. The number of constraints stored is exactly the same as the number of filters encountered


### How was this patch tested?
Many new tests are added. All existing tests are passing cleanly.
Code is functional in our env. for many months  without any issue",https://api.github.com/repos/apache/spark/issues/33983/timeline,,spark,apache,ahshahid,12415848,MDQ6VXNlcjEyNDE1ODQ4,https://avatars.githubusercontent.com/u/12415848?v=4,,https://api.github.com/users/ahshahid,https://github.com/ahshahid,https://api.github.com/users/ahshahid/followers,https://api.github.com/users/ahshahid/following{/other_user},https://api.github.com/users/ahshahid/gists{/gist_id},https://api.github.com/users/ahshahid/starred{/owner}{/repo},https://api.github.com/users/ahshahid/subscriptions,https://api.github.com/users/ahshahid/orgs,https://api.github.com/users/ahshahid/repos,https://api.github.com/users/ahshahid/events{/privacy},https://api.github.com/users/ahshahid/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33983,https://github.com/apache/spark/pull/33983,https://github.com/apache/spark/pull/33983.diff,https://github.com/apache/spark/pull/33983.patch,https://api.github.com/repos/apache/spark/issues/33983/reactions,2,2,0,0,0,0,0,0,0
58,https://api.github.com/repos/apache/spark/issues/33980,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33980/labels{/name},https://api.github.com/repos/apache/spark/issues/33980/comments,https://api.github.com/repos/apache/spark/issues/33980/events,https://github.com/apache/spark/pull/33980,994925162,MDExOlB1bGxSZXF1ZXN0NzMyNzc1NTk2,33980,[SPARK-32285][PYTHON] Add PySpark support for nested timestamps with arrow,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-13T13:57:23Z,2021-10-04T12:05:48Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Added nested timestamp support for Pyspark with Arrow
Following code will run through this PR


```
from pyspark.sql.types import StructType, TimestampType, StructField, ArrayType, LongType
spark.conf.set(""spark.sql.execution.arrow.pyspark.fallback.enabled"", ""False"")
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""True"")

import datetime
import pandas as pd
origin = pd.DataFrame({""a"": [[datetime.datetime(2012, 2, 2, 2, 2, 2)]]})
df = spark.createDataFrame(origin, schema=StructType([StructField(""a"", ArrayType(TimestampType()), True)]))
ts = datetime.datetime(2015, 11, 1, 0, 30)

schema = StructType([StructField(""a"", ArrayType(TimestampType()), True)])

df = spark.createDataFrame([([ts, ts],)], schema=schema)

df.toPandas()
```


### Why are the changes needed?
This change is required to convert ArrayType(TimeStamp) to pandas via arrow. 


### Does this PR introduce any user-facing change?
Yes user will be able to convert DF which contain Arraytype(Timestamp) to pandas

### How was this patch tested?
unit tests",https://api.github.com/repos/apache/spark/issues/33980/timeline,,spark,apache,pralabhkumar,16147255,MDQ6VXNlcjE2MTQ3MjU1,https://avatars.githubusercontent.com/u/16147255?v=4,,https://api.github.com/users/pralabhkumar,https://github.com/pralabhkumar,https://api.github.com/users/pralabhkumar/followers,https://api.github.com/users/pralabhkumar/following{/other_user},https://api.github.com/users/pralabhkumar/gists{/gist_id},https://api.github.com/users/pralabhkumar/starred{/owner}{/repo},https://api.github.com/users/pralabhkumar/subscriptions,https://api.github.com/users/pralabhkumar/orgs,https://api.github.com/users/pralabhkumar/repos,https://api.github.com/users/pralabhkumar/events{/privacy},https://api.github.com/users/pralabhkumar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33980,https://github.com/apache/spark/pull/33980,https://github.com/apache/spark/pull/33980.diff,https://github.com/apache/spark/pull/33980.patch,https://api.github.com/repos/apache/spark/issues/33980/reactions,0,0,0,0,0,0,0,0,0
59,https://api.github.com/repos/apache/spark/issues/33974,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33974/labels{/name},https://api.github.com/repos/apache/spark/issues/33974/comments,https://api.github.com/repos/apache/spark/issues/33974/events,https://github.com/apache/spark/pull/33974,994468243,MDExOlB1bGxSZXF1ZXN0NzMyMzk2MTE4,33974,[SPARK-36716][SQL] Join estimation support ExistenceJoin join type,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-09-13T05:22:56Z,2021-09-15T02:22:37Z,,NONE,,"### What changes were proposed in this pull request?

This PR makes join estimation support `ExistenceJoin` join type.

### Why are the changes needed?

To make join estimation more accurate and make the plan better. This change can benefit TPC-DS q10.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.
",https://api.github.com/repos/apache/spark/issues/33974/timeline,,spark,apache,007akuan,74766271,MDQ6VXNlcjc0NzY2Mjcx,https://avatars.githubusercontent.com/u/74766271?v=4,,https://api.github.com/users/007akuan,https://github.com/007akuan,https://api.github.com/users/007akuan/followers,https://api.github.com/users/007akuan/following{/other_user},https://api.github.com/users/007akuan/gists{/gist_id},https://api.github.com/users/007akuan/starred{/owner}{/repo},https://api.github.com/users/007akuan/subscriptions,https://api.github.com/users/007akuan/orgs,https://api.github.com/users/007akuan/repos,https://api.github.com/users/007akuan/events{/privacy},https://api.github.com/users/007akuan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33974,https://github.com/apache/spark/pull/33974,https://github.com/apache/spark/pull/33974.diff,https://github.com/apache/spark/pull/33974.patch,https://api.github.com/repos/apache/spark/issues/33974/reactions,0,0,0,0,0,0,0,0,0
60,https://api.github.com/repos/apache/spark/issues/33957,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33957/labels{/name},https://api.github.com/repos/apache/spark/issues/33957/comments,https://api.github.com/repos/apache/spark/issues/33957/events,https://github.com/apache/spark/pull/33957,993185090,MDExOlB1bGxSZXF1ZXN0NzMxNDEwNTU3,33957,[SPARK-36717][CORE] Incorrect order of variable initialization may lead incorrect behavior ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-09-10T12:22:23Z,2021-10-03T18:00:56Z,,NONE,,"### What changes were proposed in this pull request?
Incorrect order of variable initialization may lead to incorrect behavior, related code: TorrentBroadcast.scala , TorrentBroadCast will get wrong checksumEnabled value after initialization, this may not be what we need, we can move L94 front of setConf(SparkEnv.get.conf) to avoid this.

Supplement:
Snippet 1
```scala
class Broadcast {
  def setConf(): Unit = {
    checksumEnabled = true
  }
  setConf()
  var checksumEnabled = false
}
println(new Broadcast().checksumEnabled)
```
output:
```scala
false
```
 Snippet 2
```scala
class Broadcast {
  var checksumEnabled = false
  def setConf(): Unit = {
    checksumEnabled = true
  }
  setConf()
}
println(new Broadcast().checksumEnabled)
```
output: 
```scala
true
```


### Why are the changes needed?
we can move L94 front of setConf(SparkEnv.get.conf) to avoid this.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
No
",https://api.github.com/repos/apache/spark/issues/33957/timeline,,spark,apache,daugraph,40060929,MDQ6VXNlcjQwMDYwOTI5,https://avatars.githubusercontent.com/u/40060929?v=4,,https://api.github.com/users/daugraph,https://github.com/daugraph,https://api.github.com/users/daugraph/followers,https://api.github.com/users/daugraph/following{/other_user},https://api.github.com/users/daugraph/gists{/gist_id},https://api.github.com/users/daugraph/starred{/owner}{/repo},https://api.github.com/users/daugraph/subscriptions,https://api.github.com/users/daugraph/orgs,https://api.github.com/users/daugraph/repos,https://api.github.com/users/daugraph/events{/privacy},https://api.github.com/users/daugraph/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33957,https://github.com/apache/spark/pull/33957,https://github.com/apache/spark/pull/33957.diff,https://github.com/apache/spark/pull/33957.patch,https://api.github.com/repos/apache/spark/issues/33957/reactions,0,0,0,0,0,0,0,0,0
61,https://api.github.com/repos/apache/spark/issues/33950,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33950/labels{/name},https://api.github.com/repos/apache/spark/issues/33950/comments,https://api.github.com/repos/apache/spark/issues/33950/events,https://github.com/apache/spark/pull/33950,992743093,MDExOlB1bGxSZXF1ZXN0NzMxMDM3MDcz,33950,[SPARK-36698][SQL] Allow expand 'quotedRegexColumnNames' in all expressions when its expanded to one named expression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-09-09T23:34:04Z,2021-09-13T08:05:31Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
With `spark.sql.parser.quotedRegexColumnNames=true` regular expressions are not allowed in expressions like  
``` SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) ``` 

This PR propose to improve this behavior and allow this regular expression when it expands to only one named expression. Its the same behavior in Hive.

Example 1:
```
SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) 
```
col_.* expands to col_a:  OK

Example 2:
```
SELECT `col_.*`/col_b FROM (SELECT 3 AS col_a, 1 as col_b) 
```
col_.* expands to (col_a, col_b) : Fail like now

Example 3:
```
SELECT `col_a`/exp FROM (SELECT 3 AS col_a, 1 as exp) 
```
col_a expands to col_a:  OK

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Improve this feature and approaching hive behavior

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit testing",https://api.github.com/repos/apache/spark/issues/33950/timeline,,spark,apache,planga82,12819544,MDQ6VXNlcjEyODE5NTQ0,https://avatars.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33950,https://github.com/apache/spark/pull/33950,https://github.com/apache/spark/pull/33950.diff,https://github.com/apache/spark/pull/33950.patch,https://api.github.com/repos/apache/spark/issues/33950/reactions,0,0,0,0,0,0,0,0,0
62,https://api.github.com/repos/apache/spark/issues/33946,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33946/labels{/name},https://api.github.com/repos/apache/spark/issues/33946/comments,https://api.github.com/repos/apache/spark/issues/33946/events,https://github.com/apache/spark/pull/33946,992269357,MDExOlB1bGxSZXF1ZXN0NzMwNjI0NjIx,33946,[SPARK-36703][SQL] Remove the Sort if it is the child of RepartitionByExpression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-09-09T14:02:47Z,2021-09-10T14:27:20Z,,MEMBER,,"### What changes were proposed in this pull request?

This pr removes the `Sort` if it is the child of `RepartitionByExpression`. For example:
```sql
spark.range(10).selectExpr(""cast(id AS string)"").sort(""id"").repartition($""id"").explain()
```
Before this PR:
```
== Optimized Logical Plan ==
RepartitionByExpression [id#2]
+- Sort [id#2 ASC NULLS FIRST], true
   +- Project [cast(id#0L as string) AS id#2]
      +- Range (0, 10, step=1, splits=Some(2))
```
After this PR:
```
== Optimized Logical Plan ==
RepartitionByExpression [id#2]
+- Project [cast(id#0L as string) AS id#2]
   +- Range (0, 10, step=1, splits=Some(2))
```

### Why are the changes needed?

Remove useless sort. 

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test.
",https://api.github.com/repos/apache/spark/issues/33946/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33946,https://github.com/apache/spark/pull/33946,https://github.com/apache/spark/pull/33946.diff,https://github.com/apache/spark/pull/33946.patch,https://api.github.com/repos/apache/spark/issues/33946/reactions,0,0,0,0,0,0,0,0,0
63,https://api.github.com/repos/apache/spark/issues/33945,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33945/labels{/name},https://api.github.com/repos/apache/spark/issues/33945/comments,https://api.github.com/repos/apache/spark/issues/33945/events,https://github.com/apache/spark/pull/33945,992140728,MDExOlB1bGxSZXF1ZXN0NzMwNTEyNTgz,33945,[WIP][SPARK-36671][PYTHON] Support Series.__and__ and __or__ for integral,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-09T12:01:04Z,2021-09-09T15:12:13Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Support Series.\_\_and\_\_ and Series.\_\_or\_\_ for integral type

### Why are the changes needed?
Follow Pandas API as much as possible
``` python
>>> pser1 = pd.Series([1, 2, 3])
>>> pser2 = pd.Series([4, 5, 6])
>>> pser1 & pser2
0    False
1    False
2     True
dtype: bool
```

### Does this PR introduce _any_ user-facing change?
Yes.

Before
``` python
>>> pser1 = ps.Series([1, 2, 3])
>>> pser2 = ps.Series([4, 5, 6])
>>> pser3 = ps.Series([True, False, True])
>>> pser1 & pser2
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dgd/spark/python/pyspark/pandas/base.py"", line 420, in __and__
    return self._dtype_op.__and__(self, other)
  File ""/Users/dgd/spark/python/pyspark/pandas/data_type_ops/base.py"", line 317, in __and__
    raise TypeError(""Bitwise and can not be applied to %s."" % self.pretty_name)
TypeError: Bitwise and can not be applied to integrals.
>>> pser1 & pser3
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/dgd/spark/python/pyspark/pandas/base.py"", line 420, in __and__
    return self._dtype_op.__and__(self, other)
  File ""/Users/dgd/spark/python/pyspark/pandas/data_type_ops/base.py"", line 317, in __and__
    raise TypeError(""Bitwise and can not be applied to %s."" % self.pretty_name)
TypeError: Bitwise and can not be applied to integrals.
```

After
``` python
>>> pser1 = ps.Series([1, 2, 3])
>>> pser2 = ps.Series([4, 5, 6])
>>> pser3 = ps.Series([True, False, True])
>>> pser1 & pser2
0    0                                                                          
1    0
2    2
dtype: int64
>>> pser1 & pser3
0     True
1    False
2     True
dtype: bool
>>> pser1 | pser2
0    5
1    7
2    7
dtype: int64
>>> pser1 | pser3
0    True
1    True
2    True
dtype: bool
```

### How was this patch tested?

exist tests and add new unit tests
",https://api.github.com/repos/apache/spark/issues/33945/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33945,https://github.com/apache/spark/pull/33945,https://github.com/apache/spark/pull/33945.diff,https://github.com/apache/spark/pull/33945.patch,https://api.github.com/repos/apache/spark/issues/33945/reactions,0,0,0,0,0,0,0,0,0
64,https://api.github.com/repos/apache/spark/issues/33941,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33941/labels{/name},https://api.github.com/repos/apache/spark/issues/33941/comments,https://api.github.com/repos/apache/spark/issues/33941/events,https://github.com/apache/spark/pull/33941,991698454,MDExOlB1bGxSZXF1ZXN0NzMwMTI2MjI4,33941,[WIP][SPARK-36699][Core] Reuse compatible executors for stage-level scheduling,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-09T02:43:22Z,2021-09-30T15:07:32Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

https://issues.apache.org/jira/browse/SPARK-36699

We proposed to optionally change behavior of stage-level scheduling by reusing compatible executors. Two executors binding to different resource profiles are **compatible** only when the executorResources (cores in particular if not defining custom resources) are the same, but taskResources can be different. When the executors are compatible, the tasks can be allocated to any of them even when in the different profiles. Users defining profiles should make sure the different taskResources are properly specified against the same executorResources. 
A SparkConf option `spark.dynamicAllocation.reuseExecutors` is defined to change the default behavior which is not reusing executors. When this option is turned on, dynamic allocation will count all compatible executors number to meet init/min/max executor number restrictions. 
The first PR will focus on reusing executors with same cores without custom resources.

### Why are the changes needed?
Current stage-level scheduling allocated separated set of executors for different executor profiles. This approach simplified implementation, however is a waste of executor resources when the existing executors have enough resources to run the following tasks.

The typical user scenario is for different stages, user wants to use different core number for the task with same executor resources. For instance in CPU machine learning scenario, to achieve the best performance, given the same executor resources, when in ETL stage, user will allocate 1 core per task and many tasks, and in the following CPU training stage, user will use more cores per task and less tasks. In the existing implementation, two separated profiles and executors are created. Reusing executors will get better CPU resource utilization and better performance.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Unit tests

",https://api.github.com/repos/apache/spark/issues/33941/timeline,,spark,apache,xwu99,23566414,MDQ6VXNlcjIzNTY2NDE0,https://avatars.githubusercontent.com/u/23566414?v=4,,https://api.github.com/users/xwu99,https://github.com/xwu99,https://api.github.com/users/xwu99/followers,https://api.github.com/users/xwu99/following{/other_user},https://api.github.com/users/xwu99/gists{/gist_id},https://api.github.com/users/xwu99/starred{/owner}{/repo},https://api.github.com/users/xwu99/subscriptions,https://api.github.com/users/xwu99/orgs,https://api.github.com/users/xwu99/repos,https://api.github.com/users/xwu99/events{/privacy},https://api.github.com/users/xwu99/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33941,https://github.com/apache/spark/pull/33941,https://github.com/apache/spark/pull/33941.diff,https://github.com/apache/spark/pull/33941.patch,https://api.github.com/repos/apache/spark/issues/33941/reactions,0,0,0,0,0,0,0,0,0
65,https://api.github.com/repos/apache/spark/issues/33936,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33936/labels{/name},https://api.github.com/repos/apache/spark/issues/33936/comments,https://api.github.com/repos/apache/spark/issues/33936/events,https://github.com/apache/spark/pull/33936,990865261,MDExOlB1bGxSZXF1ZXN0NzI5NDExMDcz,33936,[SPARK-36693][REPL] Implement spark-shell idle timeouts,"[{'id': 1406628559, 'node_id': 'MDU6TGFiZWwxNDA2NjI4NTU5', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20SHELL', 'name': 'SPARK SHELL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,30,2021-09-08T08:37:46Z,2021-10-01T17:19:08Z,,NONE,,"### What changes were proposed in this pull request?

Add a shell timeout feature via a configuration value that closes spark-shell and the context unless new lines are entered for a set amount of time.

### Why are the changes needed?

Customers have been asking for this feature because they feel like implementing session timeouts with dynamic allocation is not satisfactory and they would like to close any idle spark-shells automatically.

### Does this PR introduce _any_ user-facing change?

It adds a new config value `spark.repl.inactivityTimeout` that enables a timer for user input. Once this expires, an error is logged and the shell is closed.

### How was this patch tested?

The change was tested manually and two new unit tests have also been added.",https://api.github.com/repos/apache/spark/issues/33936/timeline,,spark,apache,gyogal,27883675,MDQ6VXNlcjI3ODgzNjc1,https://avatars.githubusercontent.com/u/27883675?v=4,,https://api.github.com/users/gyogal,https://github.com/gyogal,https://api.github.com/users/gyogal/followers,https://api.github.com/users/gyogal/following{/other_user},https://api.github.com/users/gyogal/gists{/gist_id},https://api.github.com/users/gyogal/starred{/owner}{/repo},https://api.github.com/users/gyogal/subscriptions,https://api.github.com/users/gyogal/orgs,https://api.github.com/users/gyogal/repos,https://api.github.com/users/gyogal/events{/privacy},https://api.github.com/users/gyogal/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33936,https://github.com/apache/spark/pull/33936,https://github.com/apache/spark/pull/33936.diff,https://github.com/apache/spark/pull/33936.patch,https://api.github.com/repos/apache/spark/issues/33936/reactions,0,0,0,0,0,0,0,0,0
66,https://api.github.com/repos/apache/spark/issues/33934,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33934/labels{/name},https://api.github.com/repos/apache/spark/issues/33934/comments,https://api.github.com/repos/apache/spark/issues/33934/events,https://github.com/apache/spark/pull/33934,990760324,MDExOlB1bGxSZXF1ZXN0NzI5MzIyMDg5,33934,[SPARK-36691][PYTHON] PythonRunner failed should pass error message to ApplicationMaster too,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-09-08T06:21:37Z,2021-09-17T13:52:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
In current pyspark, stderr and stdout are print together, if python script exit, PythonRunner will only throw a `SparkUserAppsException` with exit code 1. Then pass this error to AM.
In cluster mode, client side only got exception `SparkUserAppsException` and show
```
User application exited with 1.
```
Without correct error message. Then user need to  check ApplicationMaster's stdout log file to find out why their job failed. 

In this pr, make PythonRunner can throw exception message to backend.


### Why are the changes needed?
Make user to know error message more easy.


### Does this PR introduce _any_ user-facing change?
In cluster mode, user can directly see pyspark's error message in client side.

### How was this patch tested?
If we run a sql with wrong table in python script. In ApplicationMaster and client side log will show
```
21/09/08 14:08:42 ERROR Client: Application diagnostics message: User application exited with 1.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1630930053097_708441 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1150)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1530)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

Now will show
```
21/09/08 14:08:42 ERROR Client: Application diagnostics message: User application exited with 1 and error message Traceback (most recent call last):
  File ""test.py"", line 68, in <module>
    res = client.sql(exec_sql)
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/pyspark.zip/pyspark/sql/session.py"", line 767, in sql
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/pyspark.zip/pyspark/sql/utils.py"", line 69, in deco
pyspark.sql.utils.AnalysisException: u""Table or view not found: `xxxx`.`xxxxxx`; line 14 pos 9;\n'InsertIntoTable 'UnresolvedRelation `xxxx`.`xxxxxx`, Map(dt -> None, country -> None), true, false\n+- 'Repartition 50, true\n   +- 'Project [cast('get_json_object('data, $.shopid) as bigint) AS shopid#4, cast('get_json_object('data, $.itemid) as bigint) AS itemid#5, cast('get_json_object('data, $.quantity) as bigint) AS quantity#6, 'userid, 'platform, 'page_type, 'log_timestamp, 'utc_date AS dt#7, 'grass_region AS country#8]\n      +- 'Filter ((('utc_date = cast(2021-01-01 as date)) && ('grass_region = ID)) && ('operation = action_add_to_cart_success))\n         +- 'SubqueryAlias `di`\n            +- 'UnresolvedRelation `xxxxxx`.`xxxxxx`\n""

Exception in thread ""main"" org.apache.spark.SparkException: Application application_1630930053097_708441 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1150)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1530)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```
",https://api.github.com/repos/apache/spark/issues/33934/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33934,https://github.com/apache/spark/pull/33934,https://github.com/apache/spark/pull/33934.diff,https://github.com/apache/spark/pull/33934.patch,https://api.github.com/repos/apache/spark/issues/33934/reactions,0,0,0,0,0,0,0,0,0
67,https://api.github.com/repos/apache/spark/issues/33932,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33932/labels{/name},https://api.github.com/repos/apache/spark/issues/33932/comments,https://api.github.com/repos/apache/spark/issues/33932/events,https://github.com/apache/spark/pull/33932,990713129,MDExOlB1bGxSZXF1ZXN0NzI5MjgxOTA3,33932,[SPARK-33781][SHUFFLE] Improve caching of MergeStatus on the executor side to save memory,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-09-08T04:52:23Z,2021-09-08T05:27:27Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is one of the patches for SPARK-33235: Push-based Shuffle Improvement Tasks.
At high level, in `MapOutputTrackerWorker`, if serialized `MergeStatuse` array size is larger than threshold(`spark.shuffle.push.mergeResult.minSizeForReducedCache`), cache the much more compact serialized bytes instead and only cache the deserialized `MergeStatus` objects that are needed (within `startPartitionId` until `endPartitionId`). Then deserialize the `MergeStatus` array from the cached serialized bytes again.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For large shuffles with 10s or 100s of thousands of shuffle partitions, caching the entire deserialized and decompressed MergeStatus array on the executor side, while perhaps only 0.1% of them are going to be used by the tasks running in this executor is a huge waste of memory.
This change helps save memory as well as helps with reducing GC pressure on executor side.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. This PR introduces a client-side config for push-based shuffle(`spark.shuffle.push.mergeResult.minSizeForReducedCache`). If push-based shuffle is turned-off then the users will not see any change.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test.
Verified effectiveness with jobs that would fail due to GC issue otherwise.

Ran the benchmark using GitHub Actions and did not observe any performance penalties. The results are attached in this PR:
```
core/benchmarks/MapStatusesSerDeserBenchmark-jdk11-results.txt
core/benchmarks/MapStatusesSerDeserBenchmark-results.txt
```

Lead-authored-by: Min Shen mshen@linkedin.com
Co-authored-by: Chandni Singh chsingh@linkedin.com
Co-authored-by: Minchu Yang minyang@linkedin.com",https://api.github.com/repos/apache/spark/issues/33932/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33932,https://github.com/apache/spark/pull/33932,https://github.com/apache/spark/pull/33932.diff,https://github.com/apache/spark/pull/33932.patch,https://api.github.com/repos/apache/spark/issues/33932/reactions,0,0,0,0,0,0,0,0,0
68,https://api.github.com/repos/apache/spark/issues/33930,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33930/labels{/name},https://api.github.com/repos/apache/spark/issues/33930/comments,https://api.github.com/repos/apache/spark/issues/33930/events,https://github.com/apache/spark/pull/33930,990466467,MDExOlB1bGxSZXF1ZXN0NzI5MDUzNzM3,33930,[SPARK-36665][SQL] Add more Not operator simplifications,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,56,2021-09-08T00:23:47Z,2021-10-01T01:13:51Z,,NONE,,"### What changes were proposed in this pull request?
This PR proposes to add more Not operator simplifications in `BooleanSimplification` by applying the following rules
  - Not(null) == null
    - e.g. IsNull(Not(...)) can be IsNull(...)
  - (Not(a) = b) == (a = Not(b))
    - e.g. Not(...) = true can be (...) = false
  - (a != b) == (a = Not(b))
    - e.g. (...) != true can be (...) = false


### Why are the changes needed?
This PR simplifies SQL statements that includes Not operators.
In addition, the following query does not push down the filter in the current implementation
```
SELECT * FROM t WHERE (not boolean_col) <=> null
```
although the following equivalent query pushes down the filter as expected.
```
SELECT * FROM t WHERE boolean_col <=> null
```
That is because the first query creates `IsNull(Not(boolean_col))` in the current implementation, which should be able to get simplified further to `IsNull(boolean_col)`
This PR helps optimizing such cases.


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added unit tests
```
build/sbt ""testOnly *BooleanSimplificationSuite  -- -z SPARK-36665""
```
",https://api.github.com/repos/apache/spark/issues/33930/timeline,,spark,apache,kazuyukitanimura,129050,MDQ6VXNlcjEyOTA1MA==,https://avatars.githubusercontent.com/u/129050?v=4,,https://api.github.com/users/kazuyukitanimura,https://github.com/kazuyukitanimura,https://api.github.com/users/kazuyukitanimura/followers,https://api.github.com/users/kazuyukitanimura/following{/other_user},https://api.github.com/users/kazuyukitanimura/gists{/gist_id},https://api.github.com/users/kazuyukitanimura/starred{/owner}{/repo},https://api.github.com/users/kazuyukitanimura/subscriptions,https://api.github.com/users/kazuyukitanimura/orgs,https://api.github.com/users/kazuyukitanimura/repos,https://api.github.com/users/kazuyukitanimura/events{/privacy},https://api.github.com/users/kazuyukitanimura/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33930,https://github.com/apache/spark/pull/33930,https://github.com/apache/spark/pull/33930.diff,https://github.com/apache/spark/pull/33930.patch,https://api.github.com/repos/apache/spark/issues/33930/reactions,0,0,0,0,0,0,0,0,0
69,https://api.github.com/repos/apache/spark/issues/33917,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33917/labels{/name},https://api.github.com/repos/apache/spark/issues/33917/comments,https://api.github.com/repos/apache/spark/issues/33917/events,https://github.com/apache/spark/pull/33917,988872092,MDExOlB1bGxSZXF1ZXN0NzI3NzA1MzQ5,33917,[SPARK-36622][CORE] Making spark.history.kerberos.principal _HOST compliant,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-09-06T07:36:38Z,2021-09-13T13:52:05Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
spark.history.kerberos.principal can have _HOST , which will be replaced by host canonical address

### Why are the changes needed?
This change is required for user to provide prinicipal _HOST complaint . User don't need to hardcode the History server URL . This is in line with Hiveserver2, livy server and other hadoop components.  

### Does this PR introduce _any_ user-facing change?

Yes, users can now add _HOST in the spark.history.kerberos.principal

### How was this patch tested?

unit tests/local testing

",https://api.github.com/repos/apache/spark/issues/33917/timeline,,spark,apache,pralabhkumar,16147255,MDQ6VXNlcjE2MTQ3MjU1,https://avatars.githubusercontent.com/u/16147255?v=4,,https://api.github.com/users/pralabhkumar,https://github.com/pralabhkumar,https://api.github.com/users/pralabhkumar/followers,https://api.github.com/users/pralabhkumar/following{/other_user},https://api.github.com/users/pralabhkumar/gists{/gist_id},https://api.github.com/users/pralabhkumar/starred{/owner}{/repo},https://api.github.com/users/pralabhkumar/subscriptions,https://api.github.com/users/pralabhkumar/orgs,https://api.github.com/users/pralabhkumar/repos,https://api.github.com/users/pralabhkumar/events{/privacy},https://api.github.com/users/pralabhkumar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33917,https://github.com/apache/spark/pull/33917,https://github.com/apache/spark/pull/33917.diff,https://github.com/apache/spark/pull/33917.patch,https://api.github.com/repos/apache/spark/issues/33917/reactions,0,0,0,0,0,0,0,0,0
70,https://api.github.com/repos/apache/spark/issues/33914,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33914/labels{/name},https://api.github.com/repos/apache/spark/issues/33914/comments,https://api.github.com/repos/apache/spark/issues/33914/events,https://github.com/apache/spark/pull/33914,988464013,MDExOlB1bGxSZXF1ZXN0NzI3MzY4MDY0,33914,[SPARK-32268][SQL] Dynamic bloom filter join pruning,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-09-05T09:26:44Z,2021-09-09T12:31:08Z,,MEMBER,,"### What changes were proposed in this pull request?

Reduce the shuffle data can significantly improve the query performance and increase Spark cluster stability.

This PR implements dynamic bloom filter join pruning to reduce the shuffle data. The main changes:
- Add `BuildBloomFilter` and `InBloomFilter` UDF.
- Enhance `RepartitionByExpression` to support shuffle with `ENSURE_REQUIREMENTS` origin. Dynamic bloom filter join pruning use it to reuse exchange.
- Add a new rule `DynamicBloomFilterPruning`.
- Dynamic bloom filter join pruning supports AQE.
- Support dynamic bloom filter nested with dynamic partition pruning when AQE enabled.
- Add an AQE optimization rule(`OptimizeBloomFilterJoin`) to update the `expectedNumItems` base on stats collected by AQE which is more accurate.



### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:

SQL | Before this PR | After this PR
--- | --- | ---
q12 | 11 | 8
q20 | 6 | 6
q37 | 29 | 13
q50 | 78 | 29
q80 | 14 | 14
q82 | 38 | 16
q93 | 126 | 108
q98 | 6 | 6



</body>

</html>
",https://api.github.com/repos/apache/spark/issues/33914/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33914,https://github.com/apache/spark/pull/33914,https://github.com/apache/spark/pull/33914.diff,https://github.com/apache/spark/pull/33914.patch,https://api.github.com/repos/apache/spark/issues/33914/reactions,2,2,0,0,0,0,0,0,0
71,https://api.github.com/repos/apache/spark/issues/33908,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33908/labels{/name},https://api.github.com/repos/apache/spark/issues/33908/comments,https://api.github.com/repos/apache/spark/issues/33908/events,https://github.com/apache/spark/pull/33908,987503098,MDExOlB1bGxSZXF1ZXN0NzI2NTgwMDgy,33908,[SPARK-36662][SQL] Special timestamps support for path filters -  modifiedBefore/modifiedAfter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-09-03T07:41:31Z,2021-09-16T03:35:03Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Special timestamps support for path filters -  modifiedBefore/modifiedAfter

- epoch
- now
- today
- tomorrow
- yesterday
#### examples

```scala
    val beforeTodayDF = spark.read.format(""parquet"")
      // Files modified after the midnight of today are allowed
      .option(""modifiedBefore"", ""today"")
      .load(""examples/src/main/resources/dir1"");
    beforeTodayDF.show();
    // +-------------+
    // |         file|
    // +-------------+
    // |file1.parquet|
    // +-------------+
    val afterYesterdayDF = spark.read.format(""parquet"")
      // Files modified after the midnight of yesterday are allowed
      .option(""modifiedAfter"", ""yesterday"")
      .load(""examples/src/main/resources/dir1"");
    afterYesterdayDF.show();
    // +-------------+
    // |         file|
    // +-------------+
    // +-------------+
    // $example off:load_with_modified_time_filter$
```

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

theses special values can be useful to be supported in path filters -  modifiedBefore/modifiedAfter. e.g. for daily scheduled jobs

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, special timestamps are supported by modifiedBefore/modifiedAfter

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

newly added tests",https://api.github.com/repos/apache/spark/issues/33908/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33908,https://github.com/apache/spark/pull/33908,https://github.com/apache/spark/pull/33908.diff,https://github.com/apache/spark/pull/33908.patch,https://api.github.com/repos/apache/spark/issues/33908/reactions,0,0,0,0,0,0,0,0,0
72,https://api.github.com/repos/apache/spark/issues/33905,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33905/labels{/name},https://api.github.com/repos/apache/spark/issues/33905/comments,https://api.github.com/repos/apache/spark/issues/33905/events,https://github.com/apache/spark/pull/33905,987363612,MDExOlB1bGxSZXF1ZXN0NzI2NDYwMzY5,33905,[SPARK-36658][SQL] Expose execution id to QueryExecutionListener,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-03T03:35:23Z,2021-09-27T08:10:05Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Now in QueryExecutionListenerwe have exposed API to get the query execution information:
def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit
def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit

But we can not get a clear information that which query is this. In Spark SQL, I think that executionId is the identifier of a query execution. It makes sense to expose executionId to the QueryExecutionListener, so that people can easily find the exact query in UI or history server to track more information of the query execution.

### Why are the changes needed?
Easier to find the matching query in UI. 

### Does this PR introduce _any_ user-facing change?
Developer Api change.

### How was this patch tested?
Existing UTs.
",https://api.github.com/repos/apache/spark/issues/33905/timeline,,spark,apache,ivoson,15122230,MDQ6VXNlcjE1MTIyMjMw,https://avatars.githubusercontent.com/u/15122230?v=4,,https://api.github.com/users/ivoson,https://github.com/ivoson,https://api.github.com/users/ivoson/followers,https://api.github.com/users/ivoson/following{/other_user},https://api.github.com/users/ivoson/gists{/gist_id},https://api.github.com/users/ivoson/starred{/owner}{/repo},https://api.github.com/users/ivoson/subscriptions,https://api.github.com/users/ivoson/orgs,https://api.github.com/users/ivoson/repos,https://api.github.com/users/ivoson/events{/privacy},https://api.github.com/users/ivoson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33905,https://github.com/apache/spark/pull/33905,https://github.com/apache/spark/pull/33905.diff,https://github.com/apache/spark/pull/33905.patch,https://api.github.com/repos/apache/spark/issues/33905/reactions,0,0,0,0,0,0,0,0,0
73,https://api.github.com/repos/apache/spark/issues/33900,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33900/labels{/name},https://api.github.com/repos/apache/spark/issues/33900/comments,https://api.github.com/repos/apache/spark/issues/33900/events,https://github.com/apache/spark/pull/33900,987181432,MDExOlB1bGxSZXF1ZXN0NzI2MzAyOTA1,33900,[SPARK-36654][PYTHON] Drop type ignores from numpy imports,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-09-02T21:29:08Z,2021-09-06T13:09:38Z,,MEMBER,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?

This PR removes `type: ignore[import]` annotations from numpy imports.

Additionally, minimum version of numpy  required  for the mypy tests is explicitly stated in the GitHub workflow files.

### Why are the changes needed?

Since version 1.20 numpy is PEP 561 compatible so these ignores are no longer necessary.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Type checker only: NumPy types should be validated now, instead of interpreted as `Any`.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/33900/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33900,https://github.com/apache/spark/pull/33900,https://github.com/apache/spark/pull/33900.diff,https://github.com/apache/spark/pull/33900.patch,https://api.github.com/repos/apache/spark/issues/33900/reactions,0,0,0,0,0,0,0,0,0
74,https://api.github.com/repos/apache/spark/issues/33896,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33896/labels{/name},https://api.github.com/repos/apache/spark/issues/33896/comments,https://api.github.com/repos/apache/spark/issues/33896/events,https://github.com/apache/spark/pull/33896,986156305,MDExOlB1bGxSZXF1ZXN0NzI1Mzg5Nzc4,33896,[SPARK-33701][SHUFFLE] Adaptive shuffle merge finalization for push-based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-09-02T05:21:50Z,2021-09-20T21:22:47Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
As part of SPARK-32920 implemented a simple approach to finalization for push-based shuffle. Shuffle merge finalization is the final operation happens at the end of the stage when all the tasks are completed asking all the external shuffle services to complete the shuffle merge for the stage. Once this request is completed no more shuffle pushes will be accepted. With this approach, `DAGScheduler` waits for a fixed time of 10s (`spark.shuffle.push.finalize.timeout`) to allow some time for the inflight shuffle pushes to complete, but this adds additional overhead to stages with very little shuffles.

In this PR, instead of waiting for fixed amount of time before shuffle merge finalization now this is controlled adaptively if min threshold number of map tasks shuffle push (`spark.shuffle.push.minPushRatio`) completed then shuffle merge finalization will be scheduled. Also additionally if the total shuffle generated is lesser than min threshold shuffle size (`spark.shuffle.push.minShuffleSizeToWait`) then immediately shuffle merge finalization is scheduled.
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This is a performance improvement to the existing functionality

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes additional user facing configs `spark.shuffle.push.minPushRatio` and `spark.shuffle.push.minShuffleSizeToWait`

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests in `DAGSchedulerSuite`

Lead-authored-by: Min Shen <mshen@linkedin.com>
Co-authored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>",https://api.github.com/repos/apache/spark/issues/33896/timeline,,spark,apache,venkata91,8871522,MDQ6VXNlcjg4NzE1MjI=,https://avatars.githubusercontent.com/u/8871522?v=4,,https://api.github.com/users/venkata91,https://github.com/venkata91,https://api.github.com/users/venkata91/followers,https://api.github.com/users/venkata91/following{/other_user},https://api.github.com/users/venkata91/gists{/gist_id},https://api.github.com/users/venkata91/starred{/owner}{/repo},https://api.github.com/users/venkata91/subscriptions,https://api.github.com/users/venkata91/orgs,https://api.github.com/users/venkata91/repos,https://api.github.com/users/venkata91/events{/privacy},https://api.github.com/users/venkata91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33896,https://github.com/apache/spark/pull/33896,https://github.com/apache/spark/pull/33896.diff,https://github.com/apache/spark/pull/33896.patch,https://api.github.com/repos/apache/spark/issues/33896/reactions,0,0,0,0,0,0,0,0,0
75,https://api.github.com/repos/apache/spark/issues/33893,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33893/labels{/name},https://api.github.com/repos/apache/spark/issues/33893/comments,https://api.github.com/repos/apache/spark/issues/33893/events,https://github.com/apache/spark/pull/33893,985082957,MDExOlB1bGxSZXF1ZXN0NzI0NDM3OTE4,33893,[SPARK-36638][SQL] Generalize OptimizeSkewedJoin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-09-01T12:19:25Z,2021-09-26T11:31:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR aims to generalize `OptimizeSkewedJoin` to support all patterns that can be handled by current _split-duplicate_ strategy:

1, find the _splittable_ shuffle query stages by the semantics of internal nodes;

2, for each _splittable_ shuffle query stage, check whether skew partitions exists, if true, split them into specs;

3, handle _Combinatorial Explosion_: for each skew partition, check whether the combination number is too large, if so, re-split the stages to keep a reasonable number of combinations. For example, for partition 0, stage A/B/C are split into 100/100/100 specs, respectively. Then there are 1M combinations, which is too large, and will cause performance regression.

4, attach new specs to shuffle query stages;


### Why are the changes needed?
to Generalize OptimizeSkewedJoin 


### Does this PR introduce _any_ user-facing change?
two additional configs are added


### How was this patch tested?
existing testsuites, added testsuites, some cases on our productive system
",https://api.github.com/repos/apache/spark/issues/33893/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33893,https://github.com/apache/spark/pull/33893,https://github.com/apache/spark/pull/33893.diff,https://github.com/apache/spark/pull/33893.patch,https://api.github.com/repos/apache/spark/issues/33893/reactions,0,0,0,0,0,0,0,0,0
76,https://api.github.com/repos/apache/spark/issues/33890,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33890/labels{/name},https://api.github.com/repos/apache/spark/issues/33890/comments,https://api.github.com/repos/apache/spark/issues/33890/events,https://github.com/apache/spark/pull/33890,984926689,MDExOlB1bGxSZXF1ZXN0NzI0MzAzOTg2,33890,[SPARK-36442][DOCS] Remove UserDefinedAggregateFunction reference,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-09-01T09:31:27Z,2021-09-03T03:24:09Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Remove `UserDefinedAggregateFunction` reference from `sql-ref-syntax-ddl-create-function.md`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
`UserDefinedAggregateFunction` is deprecated thus should not be used by users.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, docs changed.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
`SKIP_API=1 bundle exec jekyll build`
<img width=""561"" alt=""create"" src=""https://user-images.githubusercontent.com/87687356/131647970-167d00c2-63fb-45c9-b8d2-35963cc21c5b.png"">
",https://api.github.com/repos/apache/spark/issues/33890/timeline,,spark,apache,yutoacts,87687356,MDQ6VXNlcjg3Njg3MzU2,https://avatars.githubusercontent.com/u/87687356?v=4,,https://api.github.com/users/yutoacts,https://github.com/yutoacts,https://api.github.com/users/yutoacts/followers,https://api.github.com/users/yutoacts/following{/other_user},https://api.github.com/users/yutoacts/gists{/gist_id},https://api.github.com/users/yutoacts/starred{/owner}{/repo},https://api.github.com/users/yutoacts/subscriptions,https://api.github.com/users/yutoacts/orgs,https://api.github.com/users/yutoacts/repos,https://api.github.com/users/yutoacts/events{/privacy},https://api.github.com/users/yutoacts/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33890,https://github.com/apache/spark/pull/33890,https://github.com/apache/spark/pull/33890.diff,https://github.com/apache/spark/pull/33890.patch,https://api.github.com/repos/apache/spark/issues/33890/reactions,0,0,0,0,0,0,0,0,0
77,https://api.github.com/repos/apache/spark/issues/33889,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33889/labels{/name},https://api.github.com/repos/apache/spark/issues/33889/comments,https://api.github.com/repos/apache/spark/issues/33889/events,https://github.com/apache/spark/pull/33889,984887722,MDExOlB1bGxSZXF1ZXN0NzI0MjcwMzYz,33889,[SPARK-36632][SQL] DivideYMInterval and DivideDTInterval should throw the same exception when divide by zero.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,34,2021-09-01T08:55:21Z,2021-09-06T19:55:20Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
When dividing by zero, `DivideYMInterval` and `DivideDTInterval` output
```
java.lang.ArithmeticException
/ by zero
```
But, in ansi mode, `select 2 / 0` will output
```
org.apache.spark.SparkArithmeticException
divide by zero
```
The behavior looks not inconsistent.


### Why are the changes needed?
Make consistent behavior.


### Does this PR introduce _any_ user-facing change?
'Yes'.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/33889/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33889,https://github.com/apache/spark/pull/33889,https://github.com/apache/spark/pull/33889.diff,https://github.com/apache/spark/pull/33889.patch,https://api.github.com/repos/apache/spark/issues/33889/reactions,0,0,0,0,0,0,0,0,0
78,https://api.github.com/repos/apache/spark/issues/33888,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33888/labels{/name},https://api.github.com/repos/apache/spark/issues/33888/comments,https://api.github.com/repos/apache/spark/issues/33888/events,https://github.com/apache/spark/pull/33888,984860912,MDExOlB1bGxSZXF1ZXN0NzI0MjQ3OTIx,33888,[SPARK-36634][SQL] Support access and read parquet file by column ordinal,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-09-01T08:24:43Z,2021-09-09T05:13:42Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Add a config `spark.sql.parquet.accessByColumnOrdinal` 

When true, we access the parquet files by column original instead of catalyst schema mapping at the executor side. 

This is useful when the parquet file meta is inconsistent with those in Metastore, e.g. creating a table with existing parquet files with column names renamed, the column names are changed by external systems.

- What if the number of columns/inner fields does not match?
  - if the number of requests columns (M)is greater than the one(N) in the parquet file, the [M, N-1] of the outputs will be filled with nulls, which is also the same as the current name-based mapping
  - if the number of requests columns (M)is less than or equal to the one(N) in the parquet file, the [0, M-1] of fields will be token in order.
- What if the types are not compatible?
  - In this case, it throws unsupportedSchemaColumnConvertError 

By default, it is off

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

- What's the concrete use case that requires this feature?

Parquet's schema evolution is implementation-dependent and may causes inconsistency from file to file or file to metastore. Sometimes, 1) the table Spark's processing might be produced by other systems, e.g. renamed by hive https://issues.apache.org/jira/browse/HIVE-6938, 2)some operations that do not introduce a force schema checking, e.g. `set location, `add partition`. With this feature, the users as data consumers can still read those data produced by different providers.

See also, https://issues.apache.org/jira/browse/IMPALA-2835


better data accessibility 

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

NO

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

newly added test",https://api.github.com/repos/apache/spark/issues/33888/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33888,https://github.com/apache/spark/pull/33888,https://github.com/apache/spark/pull/33888.diff,https://github.com/apache/spark/pull/33888.patch,https://api.github.com/repos/apache/spark/issues/33888/reactions,1,0,0,0,0,0,0,0,1
79,https://api.github.com/repos/apache/spark/issues/33886,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33886/labels{/name},https://api.github.com/repos/apache/spark/issues/33886/comments,https://api.github.com/repos/apache/spark/issues/33886/events,https://github.com/apache/spark/pull/33886,984808390,MDExOlB1bGxSZXF1ZXN0NzI0MjAzNDk5,33886,[SPARK-36604][SQL] timestamp type column stats result consistent with the time zone,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-01T07:28:38Z,2021-09-02T01:35:39Z,,CONTRIBUTOR,,"
### What changes were proposed in this pull request?
timestamp type column stats result should consistent with time zone


### Why are the changes needed?
for now timestamp type column stats result is based on UTC TimeZoneindependent from the time zone. Thus the stats result may not correct


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add new test
",https://api.github.com/repos/apache/spark/issues/33886/timeline,,spark,apache,fhygh,25889738,MDQ6VXNlcjI1ODg5NzM4,https://avatars.githubusercontent.com/u/25889738?v=4,,https://api.github.com/users/fhygh,https://github.com/fhygh,https://api.github.com/users/fhygh/followers,https://api.github.com/users/fhygh/following{/other_user},https://api.github.com/users/fhygh/gists{/gist_id},https://api.github.com/users/fhygh/starred{/owner}{/repo},https://api.github.com/users/fhygh/subscriptions,https://api.github.com/users/fhygh/orgs,https://api.github.com/users/fhygh/repos,https://api.github.com/users/fhygh/events{/privacy},https://api.github.com/users/fhygh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33886,https://github.com/apache/spark/pull/33886,https://github.com/apache/spark/pull/33886.diff,https://github.com/apache/spark/pull/33886.patch,https://api.github.com/repos/apache/spark/issues/33886/reactions,0,0,0,0,0,0,0,0,0
80,https://api.github.com/repos/apache/spark/issues/33881,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33881/labels{/name},https://api.github.com/repos/apache/spark/issues/33881/comments,https://api.github.com/repos/apache/spark/issues/33881/events,https://github.com/apache/spark/pull/33881,984405126,MDExOlB1bGxSZXF1ZXN0NzIzODQzMzE4,33881,[SPARK-36628][PYTHON] Implement `__getitem__`  of label-based MultiIndex select,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-08-31T23:00:09Z,2021-09-02T22:18:12Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Implement `__getitem__`  of label-based MultiIndex select


### Why are the changes needed?
Increase pandas API coverage


### Does this PR introduce _any_ user-facing change?
Yes, reading values by label-based MultiIndex select is supported now.

```py
>>> psdf = ps.DataFrame(
...             {""a"": [1, 2, 3, 4, 5, 6, 7, 8, 9], ""b"": [4, 5, 6, 3, 2, 1, 0, 0, 0]},
...             index=[0, 1, 3, 5, 6, 8, 9, 9, 9],
...         )
>>> psdf = psdf.set_index(""b"", append=True)
>>> psdf
     a
  b   
0 4  1
1 5  2
3 6  3
5 3  4
6 2  5
8 1  6
9 0  7
  0  8
  0  9
>>> psdf.loc[6, 2]
     a
  b   
6 2  5
```

### How was this patch tested?
Unit tests.
",https://api.github.com/repos/apache/spark/issues/33881/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33881,https://github.com/apache/spark/pull/33881,https://github.com/apache/spark/pull/33881.diff,https://github.com/apache/spark/pull/33881.patch,https://api.github.com/repos/apache/spark/issues/33881/reactions,0,0,0,0,0,0,0,0,0
81,https://api.github.com/repos/apache/spark/issues/33879,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33879/labels{/name},https://api.github.com/repos/apache/spark/issues/33879/comments,https://api.github.com/repos/apache/spark/issues/33879/events,https://github.com/apache/spark/pull/33879,984095496,MDExOlB1bGxSZXF1ZXN0NzIzNTcxMjU2,33879,[SPARK-36627][CORE] Fix java deserialization of proxy classes,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-08-31T17:12:48Z,2021-09-26T00:50:44Z,,CONTRIBUTOR,,"## Upstream SPARK-XXXXX ticket and PR link (if not applicable, explain)
https://issues.apache.org/jira/browse/SPARK-36627

## What changes were proposed in this pull request?
See issue above for issue description.

### Why are the changes needed?
Spark deserialization fails with no recourse for the user.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Unit tests.",https://api.github.com/repos/apache/spark/issues/33879/timeline,,spark,apache,fsamuel-bs,2558069,MDQ6VXNlcjI1NTgwNjk=,https://avatars.githubusercontent.com/u/2558069?v=4,,https://api.github.com/users/fsamuel-bs,https://github.com/fsamuel-bs,https://api.github.com/users/fsamuel-bs/followers,https://api.github.com/users/fsamuel-bs/following{/other_user},https://api.github.com/users/fsamuel-bs/gists{/gist_id},https://api.github.com/users/fsamuel-bs/starred{/owner}{/repo},https://api.github.com/users/fsamuel-bs/subscriptions,https://api.github.com/users/fsamuel-bs/orgs,https://api.github.com/users/fsamuel-bs/repos,https://api.github.com/users/fsamuel-bs/events{/privacy},https://api.github.com/users/fsamuel-bs/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33879,https://github.com/apache/spark/pull/33879,https://github.com/apache/spark/pull/33879.diff,https://github.com/apache/spark/pull/33879.patch,https://api.github.com/repos/apache/spark/issues/33879/reactions,0,0,0,0,0,0,0,0,0
82,https://api.github.com/repos/apache/spark/issues/33878,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33878/labels{/name},https://api.github.com/repos/apache/spark/issues/33878/comments,https://api.github.com/repos/apache/spark/issues/33878/events,https://github.com/apache/spark/pull/33878,983990492,MDExOlB1bGxSZXF1ZXN0NzIzNDgyNjIy,33878,[SPARK-36303][SQL] Refactor fourthteenth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-08-31T15:22:54Z,2021-09-28T01:30:09Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->Adds error classes to some of the exceptions in QueryExecutionErrors.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->Existing tests
",https://api.github.com/repos/apache/spark/issues/33878/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33878,https://github.com/apache/spark/pull/33878,https://github.com/apache/spark/pull/33878.diff,https://github.com/apache/spark/pull/33878.patch,https://api.github.com/repos/apache/spark/issues/33878/reactions,0,0,0,0,0,0,0,0,0
83,https://api.github.com/repos/apache/spark/issues/33874,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33874/labels{/name},https://api.github.com/repos/apache/spark/issues/33874/comments,https://api.github.com/repos/apache/spark/issues/33874/events,https://github.com/apache/spark/pull/33874,983683389,MDExOlB1bGxSZXF1ZXN0NzIzMjIwOTU3,33874,[SPARK-35624][CORE]Support reading inputbytes and inputrecords of the,"[{'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,26,2021-08-31T10:10:04Z,2021-09-12T03:38:09Z,,NONE,,"What changes were proposed in this pull request?
add inputBytes and inputRecords interface in SparkStageInfo

Why are the changes needed?
One of our projects needs to count the amount of data scanned and the number of scanned data rows during the execution of sparksql statements, but the current version of spark does not provide an interface to view these data, so I want to obtain this type of data through the spark context interface

Does this PR introduce any user-facing change?
expose new interface in spark context

How was this patch tested?
Manual test
",https://api.github.com/repos/apache/spark/issues/33874/timeline,,spark,apache,allendang001,40447216,MDQ6VXNlcjQwNDQ3MjE2,https://avatars.githubusercontent.com/u/40447216?v=4,,https://api.github.com/users/allendang001,https://github.com/allendang001,https://api.github.com/users/allendang001/followers,https://api.github.com/users/allendang001/following{/other_user},https://api.github.com/users/allendang001/gists{/gist_id},https://api.github.com/users/allendang001/starred{/owner}{/repo},https://api.github.com/users/allendang001/subscriptions,https://api.github.com/users/allendang001/orgs,https://api.github.com/users/allendang001/repos,https://api.github.com/users/allendang001/events{/privacy},https://api.github.com/users/allendang001/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33874,https://github.com/apache/spark/pull/33874,https://github.com/apache/spark/pull/33874.diff,https://github.com/apache/spark/pull/33874.patch,https://api.github.com/repos/apache/spark/issues/33874/reactions,0,0,0,0,0,0,0,0,0
84,https://api.github.com/repos/apache/spark/issues/33872,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33872/labels{/name},https://api.github.com/repos/apache/spark/issues/33872/comments,https://api.github.com/repos/apache/spark/issues/33872/events,https://github.com/apache/spark/pull/33872,983576788,MDExOlB1bGxSZXF1ZXN0NzIzMTMxMTkz,33872,[SPARK-36575][CORE] Should ignore task finished event if its task set is gone in TaskSchedulerImpl.handleSuccessfulTask,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-08-31T08:15:36Z,2021-09-29T10:42:09Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When a executor finished a task of some stage, the driver will receive a `StatusUpdate` event to handle it. At the same time the driver found the executor heartbeat timed out, so the dirver also need handle ExecutorLost event simultaneously. There was a race condition issues here, which will make `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` inconsistent.

The problem is that `TaskResultGetter.enqueueSuccessfulTask` use asynchronous thread to handle successful task, that mean the synchronized lock of `TaskSchedulerImpl` was released prematurely during midway https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L61. So `TaskSchedulerImpl` may handle executorLost first, then the asynchronous thread will go on to handle successful task. It cause `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` wrong result.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
 It will cause `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` wrong result. 

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add a new test.",https://api.github.com/repos/apache/spark/issues/33872/timeline,,spark,apache,sleep1661,8802096,MDQ6VXNlcjg4MDIwOTY=,https://avatars.githubusercontent.com/u/8802096?v=4,,https://api.github.com/users/sleep1661,https://github.com/sleep1661,https://api.github.com/users/sleep1661/followers,https://api.github.com/users/sleep1661/following{/other_user},https://api.github.com/users/sleep1661/gists{/gist_id},https://api.github.com/users/sleep1661/starred{/owner}{/repo},https://api.github.com/users/sleep1661/subscriptions,https://api.github.com/users/sleep1661/orgs,https://api.github.com/users/sleep1661/repos,https://api.github.com/users/sleep1661/events{/privacy},https://api.github.com/users/sleep1661/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33872,https://github.com/apache/spark/pull/33872,https://github.com/apache/spark/pull/33872.diff,https://github.com/apache/spark/pull/33872.patch,https://api.github.com/repos/apache/spark/issues/33872/reactions,0,0,0,0,0,0,0,0,0
85,https://api.github.com/repos/apache/spark/issues/33858,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33858/labels{/name},https://api.github.com/repos/apache/spark/issues/33858/comments,https://api.github.com/repos/apache/spark/issues/33858/events,https://github.com/apache/spark/pull/33858,981164324,MDExOlB1bGxSZXF1ZXN0NzIxMjY1Mjgy,33858,[SPARK-36402][PYTHON] Implement Series.combine,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2021-08-27T11:42:49Z,2021-09-20T23:58:33Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

Implement Series.combine

### Why are the changes needed?

Increase pandas API coverage in PySpark
### Does this PR introduce _any_ user-facing change?

User can use

```python
>>> s1 = ps.Series({'falcon': 330.0, 'eagle': 160.0})
>>> s1
falcon    330.0
eagle     160.0
dtype: float64

>>> s2 = ps.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})
>>> s2
falcon    345.0
eagle     200.0
duck       30.0
dtype: float64

>>> s1.combine(s2, max)
duck        NaN
eagle     200.0
falcon    345.0
dtype: float64
```
### How was this patch tested?

unit tests and docstest",https://api.github.com/repos/apache/spark/issues/33858/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33858,https://github.com/apache/spark/pull/33858,https://github.com/apache/spark/pull/33858.diff,https://github.com/apache/spark/pull/33858.patch,https://api.github.com/repos/apache/spark/issues/33858/reactions,0,0,0,0,0,0,0,0,0
86,https://api.github.com/repos/apache/spark/issues/33849,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33849/labels{/name},https://api.github.com/repos/apache/spark/issues/33849/comments,https://api.github.com/repos/apache/spark/issues/33849/events,https://github.com/apache/spark/pull/33849,980078389,MDExOlB1bGxSZXF1ZXN0NzIwMzgwNjQ4,33849,[SPARK-36599][CORE] Fix the http class server to work again,"[{'id': 1406628559, 'node_id': 'MDU6TGFiZWwxNDA2NjI4NTU5', 'url': 'https://api.github.com/repos/apache/spark/labels/SPARK%20SHELL', 'name': 'SPARK SHELL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-08-26T10:19:46Z,2021-09-03T05:59:34Z,,NONE,,HTTP based class servers no longer work because Spark switched to Hadoop Filesystem based implementation for HTTP class servers and the hadoop http filesystem is quirky in the way it accepts paths. ,https://api.github.com/repos/apache/spark/issues/33849/timeline,,spark,apache,yellowflash,2022778,MDQ6VXNlcjIwMjI3Nzg=,https://avatars.githubusercontent.com/u/2022778?v=4,,https://api.github.com/users/yellowflash,https://github.com/yellowflash,https://api.github.com/users/yellowflash/followers,https://api.github.com/users/yellowflash/following{/other_user},https://api.github.com/users/yellowflash/gists{/gist_id},https://api.github.com/users/yellowflash/starred{/owner}{/repo},https://api.github.com/users/yellowflash/subscriptions,https://api.github.com/users/yellowflash/orgs,https://api.github.com/users/yellowflash/repos,https://api.github.com/users/yellowflash/events{/privacy},https://api.github.com/users/yellowflash/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33849,https://github.com/apache/spark/pull/33849,https://github.com/apache/spark/pull/33849.diff,https://github.com/apache/spark/pull/33849.patch,https://api.github.com/repos/apache/spark/issues/33849/reactions,0,0,0,0,0,0,0,0,0
87,https://api.github.com/repos/apache/spark/issues/33848,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33848/labels{/name},https://api.github.com/repos/apache/spark/issues/33848/comments,https://api.github.com/repos/apache/spark/issues/33848/events,https://github.com/apache/spark/pull/33848,979986049,MDExOlB1bGxSZXF1ZXN0NzIwMzAzNTI5,33848,[SPARK-36598][SHUFFLE][SQL] Avoid the memory leak of Guava cache by add maximumWeight limit,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2021-08-26T08:38:26Z,2021-09-14T14:07:08Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
The Guava Cache use in Spark with weight eviction mechanism has risk of memory leak because `LocalCache` weight eviction does not work when `maxSegmentWeight is >= Int.MAX_VALUE` [Guava#1761](https://github.com/google/guava/issues/1761)

The main change of this pr is add the `maximumWeight` value limit for Guava cache instances that use the weight eviction strategy,  include:

1.  `spark.shuffle.service.index.cache.size` should <= 8g bytes
2. `spark.shuffle.push.server.mergedIndexCacheSize` should <= 8g bytes
3. `spark.sql.hive.filesourcePartitionFileCacheSize` should <=  256g



### Why are the changes needed?
Avoiding the weight eviction bug of Guava Cache(Guava#1761)

Before this pr, the following test will fail,  some cache entries was not evicted as expected and memory leak will occur

```java
  @Test
  public void testShuffleIndexCacheEvictionBehavior() throws IOException, ExecutionException {
    Map<String, String> config = new HashMap<>();
    String indexCacheSize = ""8192m"";
    config.put(""spark.shuffle.service.index.cache.size"", indexCacheSize);
    TransportConf transportConf = new TransportConf(""shuffle"", new MapConfigProvider(config));
    ExternalShuffleBlockResolver resolver = new ExternalShuffleBlockResolver(transportConf, null);
    resolver.registerExecutor(""app0"", ""exec0"", dataContext.createExecutorInfo(SORT_MANAGER));

    // need change access scope of shuffleIndexCache
    LoadingCache<File, ShuffleIndexInformation> shuffleIndexCache = resolver.shuffleIndexCache;

    // 8g -> 8589934592 bytes
    long maximumWeight = JavaUtils.byteStringAsBytes(indexCacheSize);
    int unitSize = 1048575;
    // CacheBuilder.DEFAULT_CONCURRENCY_LEVEL
    int concurrencyLevel = 4;
    int totalGetCount = 16384;
    // maxCacheCount is 8192
    long maxCacheCount = maximumWeight / concurrencyLevel / unitSize * concurrencyLevel;
    for (int i = 0; i < totalGetCount; i++) {
      File indexFile = new File(""shuffle_"" + 0 + ""_"" + i + ""_0.index"");
      ShuffleIndexInformation indexInfo = Mockito.mock(ShuffleIndexInformation.class);
      Mockito.when(indexInfo.getSize()).thenReturn(unitSize);
      shuffleIndexCache.get(indexFile, () -> indexInfo);
    }

    long totalWeight =
      shuffleIndexCache.asMap().values().stream().mapToLong(ShuffleIndexInformation::getSize).sum();
    long size = shuffleIndexCache.size();
    // Both of the following assertions fail
    Assert.assertTrue(size <= maxCacheCount);
    Assert.assertTrue(totalWeight < maximumWeight);
  }
```

And through the debug view,  there are 2 `segments.totalWeight` overflowed.

![LocalCache](https://user-images.githubusercontent.com/1475305/130931637-2faa5e45-451b-47c6-b11c-6ec49de4c0f6.jpg)

After this pr, the above issue are avoided through the maximum limit because `maximumWeight` can no longer be greater than 2g.

### Does this PR introduce _any_ user-facing change?
The following 3 configs add the maximum limit:

1.  `spark.shuffle.service.index.cache.size` should <= 8g bytes
2. `spark.shuffle.push.server.mergedIndexCacheSize` should <= 8g bytes
3. `spark.sql.hive.filesourcePartitionFileCacheSize` should <=  256g


### How was this patch tested?
Pass GA or Jenkins Tests.",https://api.github.com/repos/apache/spark/issues/33848/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33848,https://github.com/apache/spark/pull/33848,https://github.com/apache/spark/pull/33848.diff,https://github.com/apache/spark/pull/33848.patch,https://api.github.com/repos/apache/spark/issues/33848/reactions,0,0,0,0,0,0,0,0,0
88,https://api.github.com/repos/apache/spark/issues/33839,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33839/labels{/name},https://api.github.com/repos/apache/spark/issues/33839/comments,https://api.github.com/repos/apache/spark/issues/33839/events,https://github.com/apache/spark/pull/33839,979434339,MDExOlB1bGxSZXF1ZXN0NzE5ODQ3OTQx,33839,[SPARK-36291][SQL] Refactor second set of 20 in QueryExecutionErrors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-08-25T17:00:58Z,2021-09-27T01:03:58Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

Refactor some exceptions in QueryExecutionErrors to use error classes.

```
inputTypeUnsupportedError
invalidFractionOfSecondError
overflowInSumOfDecimalError
overflowInIntegralDivideError
mapSizeExceedArraySizeWhenZipMapError
copyNullFieldNotAllowedError
literalTypeUnsupportedError
noDefaultForDataTypeError
doGenCodeOfAliasShouldNotBeCalledError
orderedOperationUnsupportedByDataTypeError
regexGroupIndexLessThanZeroError
regexGroupIndexExceedGroupCountError
invalidUrlError
dataTypeOperationUnsupportedError
mergeUnsupportedByWindowFunctionError
dataTypeUnexpectedError
typeUnsupportedError
negativeValueUnexpectedError
addNewFunctionMismatchedWithFunctionError
cannotGenerateCodeForUncomparableTypeError
```
### Why are the changes needed?

There are currently ~350 exceptions in this file; so this PR only focuses on the second set of 20.

### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existed UT Testcase",https://api.github.com/repos/apache/spark/issues/33839/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33839,https://github.com/apache/spark/pull/33839,https://github.com/apache/spark/pull/33839.diff,https://github.com/apache/spark/pull/33839.patch,https://api.github.com/repos/apache/spark/issues/33839/reactions,0,0,0,0,0,0,0,0,0
89,https://api.github.com/repos/apache/spark/issues/33836,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33836/labels{/name},https://api.github.com/repos/apache/spark/issues/33836/comments,https://api.github.com/repos/apache/spark/issues/33836/events,https://github.com/apache/spark/pull/33836,979211924,MDExOlB1bGxSZXF1ZXN0NzE5NjU0MDQ0,33836,[SPARK-36293][SQL] Refactor fourth set of 20 query execution errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-08-25T14:25:11Z,2021-10-01T01:45:49Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Refactor some exceptions in QueryExecutionErrors to use error classes.
There are currently ~350 exceptions in this file; so this PR only focuses on the fourth set of 20.

> unableToCreateDatabaseAsFailedToCreateDirectoryError
> unableToDropDatabaseAsFailedToDeleteDirectoryError
> unableToCreateTableAsFailedToCreateDirectoryError
> unableToDeletePartitionPathError
> unableToDropTableAsFailedToDeleteDirectoryError
> unableToRenameTableAsFailedToRenameDirectoryError
> unableToCreatePartitionPathError
> unableToRenamePartitionPathError
> methodNotImplementedError
> tableStatsNotSpecifiedError
> unaryMinusCauseOverflowError
> binaryArithmeticCauseOverflowError
> failedSplitSubExpressionMsg
> failedSplitSubExpressionError
> failedToCompileMsg
> internalCompilerError
> compilerError
> unsupportedTableChangeError
> notADatasourceRDDPartitionError
> dataPathNotSpecifiedError

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
https://issues.apache.org/jira/browse/SPARK-36094

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Pass all current tests",https://api.github.com/repos/apache/spark/issues/33836/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33836,https://github.com/apache/spark/pull/33836,https://github.com/apache/spark/pull/33836.diff,https://github.com/apache/spark/pull/33836.patch,https://api.github.com/repos/apache/spark/issues/33836/reactions,0,0,0,0,0,0,0,0,0
90,https://api.github.com/repos/apache/spark/issues/33830,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33830/labels{/name},https://api.github.com/repos/apache/spark/issues/33830/comments,https://api.github.com/repos/apache/spark/issues/33830/events,https://github.com/apache/spark/pull/33830,978810900,MDExOlB1bGxSZXF1ZXN0NzE5MzMxNTA2,33830,[SPARK-36582][UI] Spark HistoryPage show 'NotFound' in not logged multiple attempts,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-08-25T07:26:15Z,2021-09-06T01:48:16Z,,NONE,,"### What changes were proposed in this pull request?

`attemptId` should be always, so deleted `hasMultipleAttempts`

### Why are the changes needed?

Described in https://issues.apache.org/jira/browse/SPARK-36582


### Does this PR introduce _any_ user-facing change?

#### before change

When all applications in spark history had an attempt only, it doesn't show attemptId

![Screen Shot 2021-08-26 at 10 47 23 AM](https://user-images.githubusercontent.com/13159599/130886943-36666846-4dca-4687-9e3f-9c6d339207bd.png)

Now it shows a attemptId column regardless of hasMultipleAttempts.

![Screen Shot 2021-08-26 at 10 04 43 AM](https://user-images.githubusercontent.com/13159599/130883769-b10916ef-ccaa-4811-8e70-fa27e8a8fceb.png)


### How was this patch tested?

I checked chrome developer tool's console in changed web ui. (History Server's home page)
",https://api.github.com/repos/apache/spark/issues/33830/timeline,,spark,apache,sungpeo,13159599,MDQ6VXNlcjEzMTU5NTk5,https://avatars.githubusercontent.com/u/13159599?v=4,,https://api.github.com/users/sungpeo,https://github.com/sungpeo,https://api.github.com/users/sungpeo/followers,https://api.github.com/users/sungpeo/following{/other_user},https://api.github.com/users/sungpeo/gists{/gist_id},https://api.github.com/users/sungpeo/starred{/owner}{/repo},https://api.github.com/users/sungpeo/subscriptions,https://api.github.com/users/sungpeo/orgs,https://api.github.com/users/sungpeo/repos,https://api.github.com/users/sungpeo/events{/privacy},https://api.github.com/users/sungpeo/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33830,https://github.com/apache/spark/pull/33830,https://github.com/apache/spark/pull/33830.diff,https://github.com/apache/spark/pull/33830.patch,https://api.github.com/repos/apache/spark/issues/33830/reactions,0,0,0,0,0,0,0,0,0
91,https://api.github.com/repos/apache/spark/issues/33828,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33828/labels{/name},https://api.github.com/repos/apache/spark/issues/33828/comments,https://api.github.com/repos/apache/spark/issues/33828/events,https://github.com/apache/spark/pull/33828,978663528,MDExOlB1bGxSZXF1ZXN0NzE5MjE1ODU3,33828,[SPARK-36579][SQL] Make spark source stagingDir can use user defined,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-08-25T03:14:48Z,2021-08-27T04:45:28Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Make spark source stagingDir can use user defined like hive

### Why are the changes needed?
Make spark source stagingDir can use user defined then we can do some optimization on this


### Does this PR introduce _any_ user-facing change?
User can define staging dir by `spark.sql.source.stagingDir`

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33828/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33828,https://github.com/apache/spark/pull/33828,https://github.com/apache/spark/pull/33828.diff,https://github.com/apache/spark/pull/33828.patch,https://api.github.com/repos/apache/spark/issues/33828/reactions,0,0,0,0,0,0,0,0,0
92,https://api.github.com/repos/apache/spark/issues/33820,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33820/labels{/name},https://api.github.com/repos/apache/spark/issues/33820/comments,https://api.github.com/repos/apache/spark/issues/33820/events,https://github.com/apache/spark/pull/33820,977839928,MDExOlB1bGxSZXF1ZXN0NzE4NTI1NDAy,33820,[WIP][36571][SQL] Add new NewSQLHadoopMapReduceCommitProtocol resolve conflict when write into partition table's different partition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-08-24T08:22:36Z,2021-08-25T02:28:36Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?



### Why are the changes needed?

### Does this PR introduce _any_ user-facing change?



### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33820/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33820,https://github.com/apache/spark/pull/33820,https://github.com/apache/spark/pull/33820.diff,https://github.com/apache/spark/pull/33820.patch,https://api.github.com/repos/apache/spark/issues/33820/reactions,0,0,0,0,0,0,0,0,0
93,https://api.github.com/repos/apache/spark/issues/33814,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33814/labels{/name},https://api.github.com/repos/apache/spark/issues/33814/comments,https://api.github.com/repos/apache/spark/issues/33814/events,https://github.com/apache/spark/pull/33814,977158249,MDExOlB1bGxSZXF1ZXN0NzE3OTQxNjEx,33814,[SPARK-36396][PYTHON] Implement DataFrame.cov,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-08-23T15:29:43Z,2021-09-23T09:05:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

implement DataFrame.cov

### Why are the changes needed?

Increase pandas API coverage in PySpark

### Does this PR introduce _any_ user-facing change?
User can use

``` python
>>> psdf = ps.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],
...                   columns=['dogs', 'cats'])
>>> psdf.cov()
       dogs      cats
dogs  0.666667 -1.000000
cats -1.000000  1.666667

>>> pdf = pd.DataFrame(
...     {
...         ""a"": [1, np.nan, 3, 4],
...         ""b"": [True, False, False, True],
...         ""c"": [True, True, False, True],
...     }
... )
>>> psdf = ps.from_pandas(pdf)
>>> psdf.cov()
          a         b         c
a  2.333333 -0.166667 -0.166667
b -0.166667  0.333333  0.166667
c -0.166667  0.166667  0.250000
```

### How was this patch tested?

unit tests",https://api.github.com/repos/apache/spark/issues/33814/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33814,https://github.com/apache/spark/pull/33814,https://github.com/apache/spark/pull/33814.diff,https://github.com/apache/spark/pull/33814.patch,https://api.github.com/repos/apache/spark/issues/33814/reactions,0,0,0,0,0,0,0,0,0
94,https://api.github.com/repos/apache/spark/issues/33811,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33811/labels{/name},https://api.github.com/repos/apache/spark/issues/33811/comments,https://api.github.com/repos/apache/spark/issues/33811/events,https://github.com/apache/spark/pull/33811,976999386,MDExOlB1bGxSZXF1ZXN0NzE3ODA1Njc2,33811, [SPARK-36563][SQL] dynamicPartitionOverwrite can direct rename to targetPath instead of partition path one by one when targetPath is empty,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-08-23T12:51:28Z,2021-08-25T05:48:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
When target path is empty, we can directly rename stagingDir to targetPath avoid to rename path one by one


### Why are the changes needed?
Optimize file commit logic


### Does this PR introduce _any_ user-facing change?
User can set `spark.sql.source.stagingDir` to enable  direct rename to targetPath instead of partition path one by one when targetPath is empty for dynamicPartitionOverWrite


### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33811/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33811,https://github.com/apache/spark/pull/33811,https://github.com/apache/spark/pull/33811.diff,https://github.com/apache/spark/pull/33811.patch,https://api.github.com/repos/apache/spark/issues/33811/reactions,0,0,0,0,0,0,0,0,0
95,https://api.github.com/repos/apache/spark/issues/33780,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33780/labels{/name},https://api.github.com/repos/apache/spark/issues/33780/comments,https://api.github.com/repos/apache/spark/issues/33780/events,https://github.com/apache/spark/pull/33780,973416423,MDExOlB1bGxSZXF1ZXN0NzE0ODY0MjU1,33780,[SPARK-36540][YARN]YARN-CLIENT mode should check Shutdown message when AMEndpoint disconencted,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,58,2021-08-18T08:35:18Z,2021-10-01T16:39:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
We meet a case AM lose connection
```
21/08/18 02:14:15 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5675952834716124039, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to xx.xx.xx.xx:41420; closing connection
java.nio.channels.ClosedChannelException
        at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
        at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
```

Check the code about client, when AMEndpoint disconnected, will finish Application with SUCCESS final status
```
override def onDisconnected(remoteAddress: RpcAddress): Unit = {
      // In cluster mode or unmanaged am case, do not rely on the disassociated event to exit
      // This avoids potentially reporting incorrect exit codes if the driver fails
      if (!(isClusterMode || sparkConf.get(YARN_UNMANAGED_AM))) {
        logInfo(s""Driver terminated or disconnected! Shutting down. $remoteAddress"")
        finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
      }
    }
```
Normally say in client mode, when application success, driver will stop and AM loss connection, it's ok that exit with SUCCESS, but if there is a not work problem cause disconnected. Still finish with final status is not correct.
Then YarnClientSchedulerBackend will receive application report with final status with success and stop SparkContext cause application failed but mark it as a normal stop.
```
  private class MonitorThread extends Thread {
    private var allowInterrupt = true

    override def run() {
      try {
        val YarnAppReport(_, state, diags) =
          client.monitorApplication(appId.get, logApplicationReport = false)
        logError(s""YARN application has exited unexpectedly with state $state! "" +
          ""Check the YARN application logs for more details."")
        diags.foreach { err =>
          logError(s""Diagnostics message: $err"")
        }
        allowInterrupt = false
        sc.stop()
      } catch {
        case e: InterruptedException => logInfo(""Interrupting monitor thread"")
      }
    }

    def stopMonitor(): Unit = {
      if (allowInterrupt) {
        this.interrupt()
      }
```
IMO, we should send a `Shutdown` message to yarn client mode AM to make sure the shut down case

### Why are the changes needed?
Fix bug

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?

",https://api.github.com/repos/apache/spark/issues/33780/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33780,https://github.com/apache/spark/pull/33780,https://github.com/apache/spark/pull/33780.diff,https://github.com/apache/spark/pull/33780.patch,https://api.github.com/repos/apache/spark/issues/33780/reactions,0,0,0,0,0,0,0,0,0
96,https://api.github.com/repos/apache/spark/issues/33760,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33760/labels{/name},https://api.github.com/repos/apache/spark/issues/33760/comments,https://api.github.com/repos/apache/spark/issues/33760/events,https://github.com/apache/spark/pull/33760,972422001,MDExOlB1bGxSZXF1ZXN0NzE0MDE3Mjgx,33760,[SPARK-36518][Deploy] Spark should support distribute directory to cluster,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-08-17T08:14:36Z,2021-08-18T01:49:31Z,,CONTRIBUTOR,,"
### What changes were proposed in this pull request?
support distribute directory to cluster via --files
before:
[root@kwephispra41893 spark]# ll /opt/ygh/testdir/
total 8
drwxr-xr-x 2 root root 4096 Aug 17 16:07 dd1
drwxr-xr-x 2 root root 4096 Aug 17 16:07 dd2
-rw-r--r-- 1 root root    0 Aug 17 16:07 t1.txt
-rw-r--r-- 1 root root    0 Aug 17 16:07 t2.txt
-rw-r--r-- 1 root root    0 Aug 17 16:07 t3.conf

spark-shell --master yarn --files file:///opt/ygh/testdir
![image](https://user-images.githubusercontent.com/25889738/129689226-d63cc7f6-c529-4c6f-a94d-d48c062dbf29.png)

after:
spark-shell --master yarn --files file:///opt/ygh/testdir
![image](https://user-images.githubusercontent.com/25889738/129689406-432c796c-52e5-49c1-8016-9eec151257fb.png)


### Why are the changes needed?
when we submit spark application we can't directly distribute a directory to cluster


### Does this PR introduce _any_ user-facing change?
No, user do not need change any code


### How was this patch tested?
tested by existing UT
",https://api.github.com/repos/apache/spark/issues/33760/timeline,,spark,apache,fhygh,25889738,MDQ6VXNlcjI1ODg5NzM4,https://avatars.githubusercontent.com/u/25889738?v=4,,https://api.github.com/users/fhygh,https://github.com/fhygh,https://api.github.com/users/fhygh/followers,https://api.github.com/users/fhygh/following{/other_user},https://api.github.com/users/fhygh/gists{/gist_id},https://api.github.com/users/fhygh/starred{/owner}{/repo},https://api.github.com/users/fhygh/subscriptions,https://api.github.com/users/fhygh/orgs,https://api.github.com/users/fhygh/repos,https://api.github.com/users/fhygh/events{/privacy},https://api.github.com/users/fhygh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33760,https://github.com/apache/spark/pull/33760,https://github.com/apache/spark/pull/33760.diff,https://github.com/apache/spark/pull/33760.patch,https://api.github.com/repos/apache/spark/issues/33760/reactions,0,0,0,0,0,0,0,0,0
97,https://api.github.com/repos/apache/spark/issues/33748,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33748/labels{/name},https://api.github.com/repos/apache/spark/issues/33748/comments,https://api.github.com/repos/apache/spark/issues/33748/events,https://github.com/apache/spark/pull/33748,971410345,MDExOlB1bGxSZXF1ZXN0NzEzMTU0ODM0,33748,[SPARK-36516][SQL] Support File Metadata Cache for ORC,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,81,2021-08-16T06:28:07Z,2021-09-14T16:50:59Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
The main purpose of this pr is to introduce the File Meta Cache mechanism for Spark SQL and the basic File Meta Cache implementation for Orc is provided at the same time. There was originally a [PR](https://github.com/apache/spark/pull/30483) that supports file meta cache both Parquet and ORC, but `Parquet` has non Deprecated API that can be used to pass footer to create new `ParquetFileReader` and both Apache Spark and Parquet community are reluctant to advertise the deprecated API, so this pr spin off ORC-only part.

The main change of this pr as follows:

- Defined a `FileMetaCacheManager` to cache the mapping `FileMetaKey` to `FileMeta`. The `FileMetaKey` is the cache key, `equals` is determined by the file path by default. The `FileMeta` used to represent the cache value and It is generated by the  `FileMetaKey#getFileMeta `  method. 

- Currently, the `FileMetaCacheManager` supports a simple cache expiration elimination mechanism, the expiration time is determined by the new config  `FILE_META_CACHE_TTL_SINCE_LAST_ACCESS ` and the maximum number of file meta entries the meta cache contains for each executor is determined by the new config `FILE_META_CACHE_MAXIMUM_SIZE`

- For Orc file format, this pr added `OrcFileMetaKey ` and `OrcFileMeta` to cache Orc file Tail and and the Tail cache can be used by Vectorized read scene both in DS API V1 and V2, the feature will be enabled when `FILE_META_CACHE_ENABLED_SOURCE_LIST ` configured `orc`

Currently, the file meta cache mechanism cannot be used by `RowBasedReader`, and it needs the completion of [ORC-746](https://issues.apache.org/jira/browse/ORC-746) for further support.

The fileMetaCache need users to pay special attention to the following situations:

**If the fileMetaCache is enabled, the existing data files should not be replaced with the same file name, otherwise there will be a risk of job failure or wrong data reading before the cache entry expires.**


### Why are the changes needed?
Support Orc datasource use File Meta Cache mechanism to reduce the times of metadata reads multiple queries are performed on the same dataset. 

### Does this PR introduce _any_ user-facing change?
Add 3 new config:

- `FILE_META_CACHE_ENABLED_SOURCE_LIST(spark.sql.fileMetaCache.enabledSourceList)`:  A comma-separated list of data source short names for which data source enabled file meta cache, now the file meta cache only support ORC, it is recommended to enabled this config when multiple queries are performed on the same dataset, default is false.

- `FILE_META_CACHE_TTL_SINCE_LAST_ACCESS(spark.sql.fileMetaCache.ttlSinceLastAccess)` to represent Time-to-live for file metadata cache entry after last access, the unit is seconds.

-  `FILE_META_CACHE_MAXIMUM_SIZE(spark.sql.fileMetaCache.maximumSize)` to represent maximum number of file meta entries the file meta cache contains.

### How was this patch tested?

- Pass the Jenkins or GitHub Action
- Add new test suites to `FileMetaCacheSuite` and `SQLConfSuite`
- Add `FileMetaCacheReadBenchmark` to measure benefits

",https://api.github.com/repos/apache/spark/issues/33748/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33748,https://github.com/apache/spark/pull/33748,https://github.com/apache/spark/pull/33748.diff,https://github.com/apache/spark/pull/33748.patch,https://api.github.com/repos/apache/spark/issues/33748/reactions,0,0,0,0,0,0,0,0,0
98,https://api.github.com/repos/apache/spark/issues/33747,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33747/labels{/name},https://api.github.com/repos/apache/spark/issues/33747/comments,https://api.github.com/repos/apache/spark/issues/33747/events,https://github.com/apache/spark/pull/33747,971242591,MDExOlB1bGxSZXF1ZXN0NzEzMDE1MTEw,33747,[SPARK-36453][SQL] Improve consistency processing floating point special literals,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-08-15T22:57:46Z,2021-08-18T01:41:27Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Special literals in floating point are not consistent between cast and json expressions
```
scala> spark.sql(""SELECT CAST('+Inf' as Double)"").show
+--------------------+                                                        
|CAST(+Inf AS DOUBLE)|
+--------------------+
|            Infinity|
+--------------------+
```
```
scala> val schema =  StructType(StructField(""a"", DoubleType) :: Nil)

scala> Seq(""""""{""a"" : ""+Inf""}"""""").toDF(""col1"").select(from_json(col(""col1""),schema)).show
+---------------+
|from_json(col1)|
+---------------+
|         {null}|
+---------------+

scala> Seq(""""""{""a"" : ""+Inf""}"""""").toDF(""col"").withColumn(""col"", from_json(col(""col""), StructType.fromDDL(""a DOUBLE""))).write.json(""/tmp/jsontests12345"")
scala> spark.read.schema(StructType(Seq(StructField(""col"",schema)))).json(""/tmp/jsontests12345"").show
+------+
|   col|
+------+
|{null}|
+------+
```
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Improve consistency between operations

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, we are going to support the same special literal in Cast and Json expressions

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit testing and manual testing",https://api.github.com/repos/apache/spark/issues/33747/timeline,,spark,apache,planga82,12819544,MDQ6VXNlcjEyODE5NTQ0,https://avatars.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33747,https://github.com/apache/spark/pull/33747,https://github.com/apache/spark/pull/33747.diff,https://github.com/apache/spark/pull/33747.patch,https://api.github.com/repos/apache/spark/issues/33747/reactions,0,0,0,0,0,0,0,0,0
99,https://api.github.com/repos/apache/spark/issues/33746,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33746/labels{/name},https://api.github.com/repos/apache/spark/issues/33746/comments,https://api.github.com/repos/apache/spark/issues/33746/events,https://github.com/apache/spark/pull/33746,971094748,MDExOlB1bGxSZXF1ZXN0NzEyOTA2ODUx,33746,[SPARK-36514][SQL] Support to use the meta conf in HiveMetastoreClient,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-08-15T09:46:06Z,2021-08-19T00:50:07Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

support to set the meta conf in HiveMetastoreClient. you can pass meta info to an external multi-tenant metasotre by utilizing the meta conf, such as user name, auth info etc. 

for example,  you should add the following config in SparkConf, to use the meta conf `user.name` .

```
spark.sql.hive.metastore.metaconf.prefix     hive.metastore.metaconf.
spark.hive.metastore.metaconf.user.name      xxx
``` 


### Why are the changes needed?

currently, we can't set the meta conf in `HiveMetastoreClient`, so add this feature.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass existed tests.",https://api.github.com/repos/apache/spark/issues/33746/timeline,,spark,apache,kevincmchen,68981916,MDQ6VXNlcjY4OTgxOTE2,https://avatars.githubusercontent.com/u/68981916?v=4,,https://api.github.com/users/kevincmchen,https://github.com/kevincmchen,https://api.github.com/users/kevincmchen/followers,https://api.github.com/users/kevincmchen/following{/other_user},https://api.github.com/users/kevincmchen/gists{/gist_id},https://api.github.com/users/kevincmchen/starred{/owner}{/repo},https://api.github.com/users/kevincmchen/subscriptions,https://api.github.com/users/kevincmchen/orgs,https://api.github.com/users/kevincmchen/repos,https://api.github.com/users/kevincmchen/events{/privacy},https://api.github.com/users/kevincmchen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33746,https://github.com/apache/spark/pull/33746,https://github.com/apache/spark/pull/33746.diff,https://github.com/apache/spark/pull/33746.patch,https://api.github.com/repos/apache/spark/issues/33746/reactions,0,0,0,0,0,0,0,0,0
100,https://api.github.com/repos/apache/spark/issues/33744,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33744/labels{/name},https://api.github.com/repos/apache/spark/issues/33744/comments,https://api.github.com/repos/apache/spark/issues/33744/events,https://github.com/apache/spark/pull/33744,970956310,MDExOlB1bGxSZXF1ZXN0NzEyODExMjI1,33744,[SPARK-36403][PYTHON] Implement `Index.putmask` ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2021-08-14T17:04:02Z,2021-09-07T02:36:03Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Implement `Index.putmask`

This pull request is based on https://github.com/databricks/koalas/pull/1560


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

`putmask` returns a new Index of the values set with the mask.
`putmask` is supported in pandas. PySpark should support that as well.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes. `Index.putmask` can be used.
```python
>>> pidx = pd.Index([""a"", ""b"", ""c"", ""d"", ""e""])
>>> psidx = ps.from_pandas(pidx)
>>> psidx.putmask(psidx < ""c"", ""k"").sort_values()
Index(['c', 'd', 'e', 'k', 'k'], dtype='object')
>>> psidx.putmask(psidx < ""c"", [""g"", ""h"", ""i"", ""j"", ""k""]).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", (""g"", ""h"", ""i"", ""j"", ""k"")).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", ps.Index([""g"", ""h"", ""i"", ""j"", ""k""])).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", ""MASKED"").sort_values()
Index(['MASKED', 'MASKED', 'c', 'd', 'e'], dtype='object')
```


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit tests.
",https://api.github.com/repos/apache/spark/issues/33744/timeline,,spark,apache,beobest2,7010554,MDQ6VXNlcjcwMTA1NTQ=,https://avatars.githubusercontent.com/u/7010554?v=4,,https://api.github.com/users/beobest2,https://github.com/beobest2,https://api.github.com/users/beobest2/followers,https://api.github.com/users/beobest2/following{/other_user},https://api.github.com/users/beobest2/gists{/gist_id},https://api.github.com/users/beobest2/starred{/owner}{/repo},https://api.github.com/users/beobest2/subscriptions,https://api.github.com/users/beobest2/orgs,https://api.github.com/users/beobest2/repos,https://api.github.com/users/beobest2/events{/privacy},https://api.github.com/users/beobest2/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33744,https://github.com/apache/spark/pull/33744,https://github.com/apache/spark/pull/33744.diff,https://github.com/apache/spark/pull/33744.patch,https://api.github.com/repos/apache/spark/issues/33744/reactions,0,0,0,0,0,0,0,0,0
101,https://api.github.com/repos/apache/spark/issues/33740,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33740/labels{/name},https://api.github.com/repos/apache/spark/issues/33740/comments,https://api.github.com/repos/apache/spark/issues/33740/events,https://github.com/apache/spark/pull/33740,970757551,MDExOlB1bGxSZXF1ZXN0NzEyNjY2MzMz,33740,[SPARK-36510][DOCS] Add spark.redaction.string.regex property to the docs,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-08-13T23:04:41Z,2021-08-22T00:18:22Z,,NONE,,"### What changes were proposed in this pull request?
The PR fixes [SPARK-36510](https://issues.apache.org/jira/browse/SPARK-36510) by adding missing `spark.redaction.string.regex` property to the docs

### Why are the changes needed?
The property referred by `spark.sql.redaction.string.regex` description as its default value

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Not needed for docs
",https://api.github.com/repos/apache/spark/issues/33740/timeline,,spark,apache,dnskr,36439732,MDQ6VXNlcjM2NDM5NzMy,https://avatars.githubusercontent.com/u/36439732?v=4,,https://api.github.com/users/dnskr,https://github.com/dnskr,https://api.github.com/users/dnskr/followers,https://api.github.com/users/dnskr/following{/other_user},https://api.github.com/users/dnskr/gists{/gist_id},https://api.github.com/users/dnskr/starred{/owner}{/repo},https://api.github.com/users/dnskr/subscriptions,https://api.github.com/users/dnskr/orgs,https://api.github.com/users/dnskr/repos,https://api.github.com/users/dnskr/events{/privacy},https://api.github.com/users/dnskr/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33740,https://github.com/apache/spark/pull/33740,https://github.com/apache/spark/pull/33740.diff,https://github.com/apache/spark/pull/33740.patch,https://api.github.com/repos/apache/spark/issues/33740/reactions,0,0,0,0,0,0,0,0,0
102,https://api.github.com/repos/apache/spark/issues/33726,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33726/labels{/name},https://api.github.com/repos/apache/spark/issues/33726/comments,https://api.github.com/repos/apache/spark/issues/33726/events,https://github.com/apache/spark/pull/33726,968735877,MDExOlB1bGxSZXF1ZXN0NzEwODMyNDEw,33726,[SPARK-36493] Skip Retrieving keytab with SparkFiles.get if keytab found in the CWD of Yarn Container,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-08-12T12:33:16Z,2021-08-19T16:25:34Z,,NONE,,"### Why are the changes needed?
Currently we have the following logic to deal with the JDBC keytab provided by the `--files` option

`if (keytabParam != null && FilenameUtils.getPath(keytabParam).isEmpty)`
`{`     
`    ` `val result = SparkFiles.get(keytabParam)`     
`    ` `logDebug(s""Keytab path not found, assuming --files, file name used on executor: $result"")`     
`    ` `result` 
`}` ` else {`
`    ` `logDebug(""Keytab path found, assuming manual upload"")`
`    ` `keytabParam`
`}`



Spark has already created the soft link in the current working directory of driver (application master) for the file submitted by the `--files` option in cluster mode. Here is an example.
`testusera1.keytab -> /var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/filecache/12/testusera1.keytab`



Moreover, SparkFiles.get will get a wrong path of keytab for the driver in cluster mode. In cluster mode, the keytab is available at the following location for both the driver and executors
`/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/container_1628584679772_0030_01_000001/testusera1.keytab`
while `SparkFiles.get` brings the following wrong location
`/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/spark-8fb0f437-c842-4a9f-9612-39de40082e40/userFiles-5075388b-0928-4bc3-a498-7f6c84b27808/testusera1.keytab`

So there is no need to call the function `SparkFiles.get` to get the absolute path of the keytab file if it exists at the current working directory of Yarn container. We can directly use the variable `keytabParam` as the keytab file path.

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
We need to check whether the keytab exists in the current directory. If it is in the current directory, we do not need to call SparkFiles.get to obtain the file. To check whether the keytab exists in the current directory, we use the function call `!Files.exists(Paths.get(keytabParam))`.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing unit tests and manual integration tests.",https://api.github.com/repos/apache/spark/issues/33726/timeline,,spark,apache,xuzikun2003,25806935,MDQ6VXNlcjI1ODA2OTM1,https://avatars.githubusercontent.com/u/25806935?v=4,,https://api.github.com/users/xuzikun2003,https://github.com/xuzikun2003,https://api.github.com/users/xuzikun2003/followers,https://api.github.com/users/xuzikun2003/following{/other_user},https://api.github.com/users/xuzikun2003/gists{/gist_id},https://api.github.com/users/xuzikun2003/starred{/owner}{/repo},https://api.github.com/users/xuzikun2003/subscriptions,https://api.github.com/users/xuzikun2003/orgs,https://api.github.com/users/xuzikun2003/repos,https://api.github.com/users/xuzikun2003/events{/privacy},https://api.github.com/users/xuzikun2003/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33726,https://github.com/apache/spark/pull/33726,https://github.com/apache/spark/pull/33726.diff,https://github.com/apache/spark/pull/33726.patch,https://api.github.com/repos/apache/spark/issues/33726/reactions,0,0,0,0,0,0,0,0,0
103,https://api.github.com/repos/apache/spark/issues/33725,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33725/labels{/name},https://api.github.com/repos/apache/spark/issues/33725/comments,https://api.github.com/repos/apache/spark/issues/33725/events,https://github.com/apache/spark/pull/33725,968712639,MDExOlB1bGxSZXF1ZXN0NzEwODEwNzc0,33725,[SPARK-36494]Add param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-08-12T12:19:05Z,2021-08-16T10:08:49Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If bucketSpec is not used, SortMergeJoin will do unnecessary shuffle.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. See in SPARK-36494.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Test it in my develop enviroinment.
",https://api.github.com/repos/apache/spark/issues/33725/timeline,,spark,apache,zengruios,26316365,MDQ6VXNlcjI2MzE2MzY1,https://avatars.githubusercontent.com/u/26316365?v=4,,https://api.github.com/users/zengruios,https://github.com/zengruios,https://api.github.com/users/zengruios/followers,https://api.github.com/users/zengruios/following{/other_user},https://api.github.com/users/zengruios/gists{/gist_id},https://api.github.com/users/zengruios/starred{/owner}{/repo},https://api.github.com/users/zengruios/subscriptions,https://api.github.com/users/zengruios/orgs,https://api.github.com/users/zengruios/repos,https://api.github.com/users/zengruios/events{/privacy},https://api.github.com/users/zengruios/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33725,https://github.com/apache/spark/pull/33725,https://github.com/apache/spark/pull/33725.diff,https://github.com/apache/spark/pull/33725.patch,https://api.github.com/repos/apache/spark/issues/33725/reactions,0,0,0,0,0,0,0,0,0
104,https://api.github.com/repos/apache/spark/issues/33723,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33723/labels{/name},https://api.github.com/repos/apache/spark/issues/33723/comments,https://api.github.com/repos/apache/spark/issues/33723/events,https://github.com/apache/spark/pull/33723,968682497,MDExOlB1bGxSZXF1ZXN0NzEwNzgyNjMy,33723,[SPARK-36496][SQL] Remove literals from grouping expressions when using the DataFrame withColumn API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-08-12T12:00:32Z,2021-08-17T11:16:10Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Move the `RemoveLiteralFromGroupExpressions` and `RemoveRepetitionFromGroupExpressions` rules from a separate batch to the `operatorOptimizationBatch`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The `RemoveLiteralFromGroupExpressions` does not work in some cases if it is in a separate batch.
The added UT would fail with:
```
[info] - SPARK-36496: Remove literals from grouping expressions *** FAILED *** (2 seconds, 955 milliseconds)
[info]   == FAIL: Plans do not match ===
[info]   !Aggregate [*id#0L, null], [*id#0L, null AS a#0, count(1) AS count#0L]   Aggregate [*id#0L], [*id#0L, null AS a#0, count(1) AS count#0L]
[info]    +- Range (0, 100, step=1, splits=Some(2))                               +- Range (0, 100, step=1, splits=Some(2)) (PlanTest.scala:174)
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UT",https://api.github.com/repos/apache/spark/issues/33723/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33723,https://github.com/apache/spark/pull/33723,https://github.com/apache/spark/pull/33723.diff,https://github.com/apache/spark/pull/33723.patch,https://api.github.com/repos/apache/spark/issues/33723/reactions,0,0,0,0,0,0,0,0,0
105,https://api.github.com/repos/apache/spark/issues/33717,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33717/labels{/name},https://api.github.com/repos/apache/spark/issues/33717/comments,https://api.github.com/repos/apache/spark/issues/33717/events,https://github.com/apache/spark/pull/33717,967975103,MDExOlB1bGxSZXF1ZXN0NzEwMTIwMDg0,33717,[SPARK-36486][SQL] Support cast type constructed same string to day time interval,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-08-12T03:54:58Z,2021-08-12T09:12:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Spark support type constructed string as  day time interval such as 
```
interval '1 day 2 hour'
```
And PGSQL support
```
cast('1 day 2 hour' as interval day)
cast('1 day 2 hour' as interval day to hour)
```
etc, spark can support this too.


### Why are the changes needed?
Support cast  same type constructed string as day time interval


### Does this PR introduce _any_ user-facing change?
user can cast such as `1 day 2 hour` string as day time interval

### How was this patch tested?
Added UT",https://api.github.com/repos/apache/spark/issues/33717/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33717,https://github.com/apache/spark/pull/33717,https://github.com/apache/spark/pull/33717.diff,https://github.com/apache/spark/pull/33717.patch,https://api.github.com/repos/apache/spark/issues/33717/reactions,0,0,0,0,0,0,0,0,0
106,https://api.github.com/repos/apache/spark/issues/33716,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33716/labels{/name},https://api.github.com/repos/apache/spark/issues/33716/comments,https://api.github.com/repos/apache/spark/issues/33716/events,https://github.com/apache/spark/pull/33716,967923867,MDExOlB1bGxSZXF1ZXN0NzEwMDcxMzgx,33716,[SPARK-36485][SQL] Support cast type constructed string as year month interval,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-08-12T03:11:57Z,2021-08-13T12:34:59Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Spark support type constructed string as  year month interval such as 
```
interval '1 year 2 month'
interval '3 year'
```
And PGSQL support
```
cast('1 year 2 month' as interval year)
cast('1 year 2 month' as interval year to month)
cast('1 year 2 month' as interval month)
```
etc, spark can support this too.


### Why are the changes needed?
Support cast  same type constructed string as year month interval


### Does this PR introduce _any_ user-facing change?
user can cast such as `1 year 2 month` string as year month interval

### How was this patch tested?
Added UT",https://api.github.com/repos/apache/spark/issues/33716/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33716,https://github.com/apache/spark/pull/33716,https://github.com/apache/spark/pull/33716.diff,https://github.com/apache/spark/pull/33716.patch,https://api.github.com/repos/apache/spark/issues/33716/reactions,0,0,0,0,0,0,0,0,0
107,https://api.github.com/repos/apache/spark/issues/33706,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33706/labels{/name},https://api.github.com/repos/apache/spark/issues/33706/comments,https://api.github.com/repos/apache/spark/issues/33706/events,https://github.com/apache/spark/pull/33706,966330382,MDExOlB1bGxSZXF1ZXN0NzA4NTg0MDY3,33706,[SPARK-36477][SQL] Inferring schema from JSON file shall handle CharConversionException/MalformedInputException,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,39,2021-08-11T09:30:46Z,2021-08-22T00:58:34Z,,CONTRIBUTOR,,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

When set `ignoreCorruptFiles=true`, reading JSON still fails with corrupt files during inferring schema.

```scala
java.io.CharConversionException: Unsupported UCS-4 endianness (2143) detected
	at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.reportWeirdUCS4(ByteSourceJsonBootstrapper.java:504)
	at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.checkUTF32(ByteSourceJsonBootstrapper.java:471)
	at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:144)
	at com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:247)
	at com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1528)
	at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1034)
	at org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:86)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:107)
	at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:66)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2621)
	at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)
	at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)
	at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)
	at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)
	at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)
	at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)
	at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)
	at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)
	at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

bugfix

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, ignoreCorruptFiles will ignore JSON files corrupted

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

new test",https://api.github.com/repos/apache/spark/issues/33706/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33706,https://github.com/apache/spark/pull/33706,https://github.com/apache/spark/pull/33706.diff,https://github.com/apache/spark/pull/33706.patch,https://api.github.com/repos/apache/spark/issues/33706/reactions,0,0,0,0,0,0,0,0,0
108,https://api.github.com/repos/apache/spark/issues/33696,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33696/labels{/name},https://api.github.com/repos/apache/spark/issues/33696/comments,https://api.github.com/repos/apache/spark/issues/33696/events,https://github.com/apache/spark/pull/33696,965766262,MDExOlB1bGxSZXF1ZXN0NzA4MDU3MDUx,33696,[SPARK-36473][CORE] Add non-unified memory peak metrics,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-08-11T02:51:15Z,2021-09-06T07:42:46Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Add non-unified memory metrics to have evidence  for change config of spark.memory.fraction


### Why are the changes needed?
We can adjust `spark.memory.fraction` according to this metrics.

### Does this PR introduce _any_ user-facing change?
Use can got non-managed memory usage from metrics sink and restful API

### How was this patch tested?
Existed UT
",https://api.github.com/repos/apache/spark/issues/33696/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33696,https://github.com/apache/spark/pull/33696,https://github.com/apache/spark/pull/33696.diff,https://github.com/apache/spark/pull/33696.patch,https://api.github.com/repos/apache/spark/issues/33696/reactions,0,0,0,0,0,0,0,0,0
109,https://api.github.com/repos/apache/spark/issues/33679,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33679/labels{/name},https://api.github.com/repos/apache/spark/issues/33679/comments,https://api.github.com/repos/apache/spark/issues/33679/events,https://github.com/apache/spark/pull/33679,963430155,MDExOlB1bGxSZXF1ZXN0NzA2MDQxNjc3,33679,[SPARK-36452][SQL]: Add the support in Spark for having group by map datatype column for the scenario that works in Hive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-08-08T13:14:15Z,2021-08-09T05:57:06Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Add the support in Spark for having group by map datatype column for the scenario that works in Hive.
In hive this scenario works fine 

```
describe extended complex2;
OK
id                  string 
c1                  map<int, string>   
Detailed Table Information Table(tableName:complex2, dbName:default, owner:abc, createTime:1627994412, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:string, comment:null), FieldSchema(name:c1, type:map<int,string>, comment:null)], location:/user/hive/warehouse/complex2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1

select * from complex2;
OK
1 {1:""u""}
2 {1:""u"",2:""uo""}
1 {1:""u"",2:""uo""}
Time taken: 0.363 seconds, Fetched: 3 row(s)

Working Scenario in Hive -: 

select id, c1, count(*) from complex2 group by id, c1;
OK
1 {1:""u""} 1
1 {1:""u"",2:""uo""} 1
2 {1:""u"",2:""uo""} 1
Time taken: 1.621 seconds, Fetched: 3 row(s)

Failed Scenario in Hive -: 
When map type is present in aggregated expression 
select id, max(c1), count(*) from complex2 group by id, c1; 
FAILED: UDFArgumentTypeException Cannot support comparison of map<> type or complex type containing map<>.
```
But in spark where the group by map column failed for this scenario where the map column is used in the select without any aggregation, The one that works in hive.

```
scala> spark.sql(""select id,c1, count(*) from complex2 group by id, c1"").show
org.apache.spark.sql.AnalysisException: expression spark_catalog.default.complex2.`c1` cannot be used as a grouping expression because its data type map<int,string> is not an orderable data type.;
Aggregate [id#1, c1#2], [id#1, c1#2, count(1) AS count(1)#3L]
+- SubqueryAlias spark_catalog.default.complex2
 +- HiveTableRelation [`default`.`complex2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#1, c1#2], Partition Cols: []]
at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:50)
```

### Why are the changes needed?
There is need to add the this scenario where grouping expression can have map type if aggregated expression does not have the that map type reference. This helps in migrating the user from hive to Spark.

After the code change 

```
scala> spark.sql(""select id,c1, count(*) from complex2 group by id, c1"").show
+---+-----------------+--------+                                                
| id|               c1|count(1)|
+---+-----------------+--------+
|  1|         {1 -> u}|       1|
|  2|{1 -> u, 2 -> uo}|       1|
|  1|{1 -> u, 2 -> uo}|       1|
+---+-----------------+--------+
```


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Added the unit test and also tested using spark-shell the scenario
",https://api.github.com/repos/apache/spark/issues/33679/timeline,,spark,apache,SaurabhChawla100,34540906,MDQ6VXNlcjM0NTQwOTA2,https://avatars.githubusercontent.com/u/34540906?v=4,,https://api.github.com/users/SaurabhChawla100,https://github.com/SaurabhChawla100,https://api.github.com/users/SaurabhChawla100/followers,https://api.github.com/users/SaurabhChawla100/following{/other_user},https://api.github.com/users/SaurabhChawla100/gists{/gist_id},https://api.github.com/users/SaurabhChawla100/starred{/owner}{/repo},https://api.github.com/users/SaurabhChawla100/subscriptions,https://api.github.com/users/SaurabhChawla100/orgs,https://api.github.com/users/SaurabhChawla100/repos,https://api.github.com/users/SaurabhChawla100/events{/privacy},https://api.github.com/users/SaurabhChawla100/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33679,https://github.com/apache/spark/pull/33679,https://github.com/apache/spark/pull/33679.diff,https://github.com/apache/spark/pull/33679.patch,https://api.github.com/repos/apache/spark/issues/33679/reactions,0,0,0,0,0,0,0,0,0
110,https://api.github.com/repos/apache/spark/issues/33675,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33675/labels{/name},https://api.github.com/repos/apache/spark/issues/33675/comments,https://api.github.com/repos/apache/spark/issues/33675/events,https://github.com/apache/spark/pull/33675,963293573,MDExOlB1bGxSZXF1ZXN0NzA1OTM4MDYx,33675,[SPARK-27997][K8S] Add support for kubernetes OAuth Token refresh,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-08-07T19:52:11Z,2021-09-15T04:46:08Z,,NONE,,"### What changes were proposed in this pull request?

This change allows a spark user to provide a class which implements fabric's OAuthTokenProvider to refresh tokens throughout the life of the spark app.

```
spark.kubernetes.authenticate.submission.oauthTokenProvider=<token>
spark.kubernetes.authenticate.driver.oauthTokenProvider=<token>
spark.kubernetes.authenticate.oauthTokenProvider=<token>
```

https://javadoc.io/doc/io.fabric8/kubernetes-client/5.6.0/io/fabric8/kubernetes/client/OAuthTokenProvider.html


### Why are the changes needed?

Currently, while running spark on kubernetes, one should specify oauth tokens via config before starting an application.
```
spark.kubernetes.authenticate.submission.oauthToken=<token>
spark.kubernetes.authenticate.driver.oauthToken=<token>
spark.kubernetes.authenticate.oauthToken=<token>
```

The token has an expiration time (usually an hour, for GKE) and there is no way to update the token in the runtime. The spark app starts to throw exceptions.
```
io.fabric8.kubernetes.client.KubernetesClientException: Unauthorized
	at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:202)
	at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
	at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198)
	at okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)
	at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
```


### Does this PR introduce _any_ user-facing change?
Yes, a configuration option `spark.kubernetes.client.oauth.token.provider.class` is added. 


### How was this patch tested?
A class which implements OAuthTokenProvider interface[0] was added into the classpath on driver node with no other spark options for tokens specified 
It was also tested with expired tokens specified, and the token was updated via the user-provided class.
```
--conf spark.kubernetes.authenticate.submission.oauthToken=<expired>
--conf spark.kubernetes.authenticate.driver.oauthToken=<expired> 
--conf spark.kubernetes.authenticate.oauthToken=<expired>
```
There is no need to use any other token-related configuration options if this class is provided.

An example of the user-provided class for GKE
[0] https://gist.github.com/haodemon/5490fefdb258275c1f805d584319090b

```scala
import io.fabric8.kubernetes.client.OAuthTokenProvider

class OAuthGoogleTokenProvider extends OAuthTokenProvider {
  private val binary = ""gcloud""
  private val args = ""config config-helper --format=json""

  override def getToken: String = {
    val response = (binary + "" "" + args).!!
    val token = new ObjectMapper().readTree(response)
      .get(""credential"")
      .get(""access_token"")
    token.getTextValue
  }
}
```
",https://api.github.com/repos/apache/spark/issues/33675/timeline,,spark,apache,haodemon,3372489,MDQ6VXNlcjMzNzI0ODk=,https://avatars.githubusercontent.com/u/3372489?v=4,,https://api.github.com/users/haodemon,https://github.com/haodemon,https://api.github.com/users/haodemon/followers,https://api.github.com/users/haodemon/following{/other_user},https://api.github.com/users/haodemon/gists{/gist_id},https://api.github.com/users/haodemon/starred{/owner}{/repo},https://api.github.com/users/haodemon/subscriptions,https://api.github.com/users/haodemon/orgs,https://api.github.com/users/haodemon/repos,https://api.github.com/users/haodemon/events{/privacy},https://api.github.com/users/haodemon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33675,https://github.com/apache/spark/pull/33675,https://github.com/apache/spark/pull/33675.diff,https://github.com/apache/spark/pull/33675.patch,https://api.github.com/repos/apache/spark/issues/33675/reactions,0,0,0,0,0,0,0,0,0
111,https://api.github.com/repos/apache/spark/issues/33674,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33674/labels{/name},https://api.github.com/repos/apache/spark/issues/33674/comments,https://api.github.com/repos/apache/spark/issues/33674/events,https://github.com/apache/spark/pull/33674,963224719,MDExOlB1bGxSZXF1ZXN0NzA1ODg3MzMz,33674,[Spark-36328][CORE][SQL] Reuse the FileSystem delegation token while querying partitioned hive table.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-08-07T12:51:00Z,2021-09-01T17:56:12Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

Add the credentials from previous JobConf into the new JobConf to reuse the FileSystem Delegation Token.

### Why are the changes needed?

Spark Job creates a new JobConf (which will have a new Credentials) for every hive table partition, the token is not reused and gets fetched for every partition. This is slowing down the query as each delegation token has to go through KDC and SSL handshake on Secure Clusters.

### Does this PR introduce _any_ user-facing change?

Yes, while user querying partitioned hive table.

### How was this patch tested?

new test added.
",https://api.github.com/repos/apache/spark/issues/33674/timeline,,spark,apache,Shockang,28219857,MDQ6VXNlcjI4MjE5ODU3,https://avatars.githubusercontent.com/u/28219857?v=4,,https://api.github.com/users/Shockang,https://github.com/Shockang,https://api.github.com/users/Shockang/followers,https://api.github.com/users/Shockang/following{/other_user},https://api.github.com/users/Shockang/gists{/gist_id},https://api.github.com/users/Shockang/starred{/owner}{/repo},https://api.github.com/users/Shockang/subscriptions,https://api.github.com/users/Shockang/orgs,https://api.github.com/users/Shockang/repos,https://api.github.com/users/Shockang/events{/privacy},https://api.github.com/users/Shockang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33674,https://github.com/apache/spark/pull/33674,https://github.com/apache/spark/pull/33674.diff,https://github.com/apache/spark/pull/33674.patch,https://api.github.com/repos/apache/spark/issues/33674/reactions,0,0,0,0,0,0,0,0,0
112,https://api.github.com/repos/apache/spark/issues/33662,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33662/labels{/name},https://api.github.com/repos/apache/spark/issues/33662/comments,https://api.github.com/repos/apache/spark/issues/33662/events,https://github.com/apache/spark/pull/33662,962370145,MDExOlB1bGxSZXF1ZXN0NzA1MTY5MDU2,33662,[SPARK-36162][SQL] Support estimation of equal null safe join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-08-06T03:50:39Z,2021-08-11T04:57:49Z,,NONE,,"### What changes were proposed in this pull request?
Support estimation of equal null safe join

### Why are the changes needed?
Support estimation of equal null safe join

### Does this PR introduce _any_ user-facing change?
User can estimate more accurate estimation result when using equal null safe join

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33662/timeline,,spark,apache,changvvb,13730772,MDQ6VXNlcjEzNzMwNzcy,https://avatars.githubusercontent.com/u/13730772?v=4,,https://api.github.com/users/changvvb,https://github.com/changvvb,https://api.github.com/users/changvvb/followers,https://api.github.com/users/changvvb/following{/other_user},https://api.github.com/users/changvvb/gists{/gist_id},https://api.github.com/users/changvvb/starred{/owner}{/repo},https://api.github.com/users/changvvb/subscriptions,https://api.github.com/users/changvvb/orgs,https://api.github.com/users/changvvb/repos,https://api.github.com/users/changvvb/events{/privacy},https://api.github.com/users/changvvb/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33662,https://github.com/apache/spark/pull/33662,https://github.com/apache/spark/pull/33662.diff,https://github.com/apache/spark/pull/33662.patch,https://api.github.com/repos/apache/spark/issues/33662/reactions,0,0,0,0,0,0,0,0,0
113,https://api.github.com/repos/apache/spark/issues/33641,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33641/labels{/name},https://api.github.com/repos/apache/spark/issues/33641/comments,https://api.github.com/repos/apache/spark/issues/33641/events,https://github.com/apache/spark/pull/33641,960864373,MDExOlB1bGxSZXF1ZXN0NzAzODQ1NDI2,33641,[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-08-04T18:43:22Z,2021-09-07T17:17:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

Add ""num broadcast joins conversions"" and ""num skew join conversions""
metrics to AdaptiveSparkPlanExec to report how many joins were changed to BHJ or had skew mitigated due to AQE.

### Why are the changes needed?

To make it easy to get a sense of how much impact AQE had on an a complex query.

It's also useful for systems that collect metrics for later analysis of AQE effectiveness in large production deployment.


### Does this PR introduce _any_ user-facing change?

2 new SQL metrics will now be emitted

### How was this patch tested?

Existing and new Unit tests.",https://api.github.com/repos/apache/spark/issues/33641/timeline,,spark,apache,ekoifman,4297661,MDQ6VXNlcjQyOTc2NjE=,https://avatars.githubusercontent.com/u/4297661?v=4,,https://api.github.com/users/ekoifman,https://github.com/ekoifman,https://api.github.com/users/ekoifman/followers,https://api.github.com/users/ekoifman/following{/other_user},https://api.github.com/users/ekoifman/gists{/gist_id},https://api.github.com/users/ekoifman/starred{/owner}{/repo},https://api.github.com/users/ekoifman/subscriptions,https://api.github.com/users/ekoifman/orgs,https://api.github.com/users/ekoifman/repos,https://api.github.com/users/ekoifman/events{/privacy},https://api.github.com/users/ekoifman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33641,https://github.com/apache/spark/pull/33641,https://github.com/apache/spark/pull/33641.diff,https://github.com/apache/spark/pull/33641.patch,https://api.github.com/repos/apache/spark/issues/33641/reactions,0,0,0,0,0,0,0,0,0
114,https://api.github.com/repos/apache/spark/issues/33639,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33639/labels{/name},https://api.github.com/repos/apache/spark/issues/33639/comments,https://api.github.com/repos/apache/spark/issues/33639/events,https://github.com/apache/spark/pull/33639,960711936,MDExOlB1bGxSZXF1ZXN0NzAzNzA2ODIz,33639,[SPARK-36645][SQL] Aggregate (Min/Max/Count) push down for Parquet,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,56,2021-08-04T16:39:19Z,2021-10-02T18:36:57Z,,CONTRIBUTOR,,"
### What changes were proposed in this pull request?
Push down Min/Max/Count to Parquet with the following restrictions:

- nested types such as Array, Map or Struct will not be pushed down
- Timestamp not pushed down because INT96 sort order is undefined, Parquet doesn't return statistics for INT96
- If the aggregate column is on partition column, only Count will be pushed, Min or Max will not be pushed down because Parquet doesn't return max/min for partition column. 
- If somehow the file doesn't have stats for the aggregate columns, Spark will throw Exception.
- Currently, if filter/GROUP BY is involved, Min/Max/Count will not be pushed down, but the restriction will be lifted if the filter or GROUP BY is on partition column (https://issues.apache.org/jira/browse/SPARK-36646 and https://issues.apache.org/jira/browse/SPARK-36647)


### Why are the changes needed?
Since parquet has the statistics information for min, max and count, we want to take advantage of this info and push down Min/Max/Count to parquet layer for better performance.


### Does this PR introduce _any_ user-facing change?
Yes, `SQLConf.PARQUET_AGGREGATE_PUSHDOWN_ENABLED` was added. If sets to true, we will push down Min/Max/Count to Parquet.


### How was this patch tested?
new test suites
",https://api.github.com/repos/apache/spark/issues/33639/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33639,https://github.com/apache/spark/pull/33639,https://github.com/apache/spark/pull/33639.diff,https://github.com/apache/spark/pull/33639.patch,https://api.github.com/repos/apache/spark/issues/33639/reactions,0,0,0,0,0,0,0,0,0
115,https://api.github.com/repos/apache/spark/issues/33632,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33632/labels{/name},https://api.github.com/repos/apache/spark/issues/33632/comments,https://api.github.com/repos/apache/spark/issues/33632/events,https://github.com/apache/spark/pull/33632,959980767,MDExOlB1bGxSZXF1ZXN0NzAzMDQ3MzUx,33632,[SPARK-36360][Streaming] Delete appName from StreamingSource sourceName,"[{'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-08-04T07:06:57Z,2021-08-11T15:12:46Z,,NONE,,"### What changes were proposed in this pull request?
This pull request proposes to exclude the appName from the sourceName in StreamingSource following the example of the other sources, e.g. `ExecutorMetricsSource`.

### Why are the changes needed?
The StreamingSource includes the appName in its sourceName. However, the appName should not be handled by the StreamingSource. It is already handled by the MetricsSystem. See all other MetricSources, e.g. ExecutorMetricsSource.

Why is this important? See this part from the [documentation](https://spark.apache.org/docs/latest/monitoring.html#metrics):

> Often times, users want to be able to track the metrics across apps for driver and executors, which is hard to do with application ID (i.e. spark.app.id) since it changes with every invocation of the app. For such use cases, a custom namespace can be specified for metrics reporting using spark.metrics.namespace configuration property. If, say, users wanted to set the metrics namespace to the name of the application, they can set the spark.metrics.namespace property to a value like ${spark.app.name}. This value is then expanded appropriately by Spark and is used as the root namespace of the metrics system.

This is only possible if the MetricsSystem handles the namespace which it does. But the StreamingSource additionally adds the appName in its sourceName, thus there is no way to configure a namespace that does not include the appName.

### Does this PR introduce _any_ user-facing change?
The proposed change conforms to the documentation. See [here](https://spark.apache.org/docs/latest/monitoring.html#metrics).

### How was this patch tested?
No tests added.
",https://api.github.com/repos/apache/spark/issues/33632/timeline,,spark,apache,mrclneumann,7224709,MDQ6VXNlcjcyMjQ3MDk=,https://avatars.githubusercontent.com/u/7224709?v=4,,https://api.github.com/users/mrclneumann,https://github.com/mrclneumann,https://api.github.com/users/mrclneumann/followers,https://api.github.com/users/mrclneumann/following{/other_user},https://api.github.com/users/mrclneumann/gists{/gist_id},https://api.github.com/users/mrclneumann/starred{/owner}{/repo},https://api.github.com/users/mrclneumann/subscriptions,https://api.github.com/users/mrclneumann/orgs,https://api.github.com/users/mrclneumann/repos,https://api.github.com/users/mrclneumann/events{/privacy},https://api.github.com/users/mrclneumann/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33632,https://github.com/apache/spark/pull/33632,https://github.com/apache/spark/pull/33632.diff,https://github.com/apache/spark/pull/33632.patch,https://api.github.com/repos/apache/spark/issues/33632/reactions,0,0,0,0,0,0,0,0,0
116,https://api.github.com/repos/apache/spark/issues/33628,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33628/labels{/name},https://api.github.com/repos/apache/spark/issues/33628/comments,https://api.github.com/repos/apache/spark/issues/33628/events,https://github.com/apache/spark/pull/33628,959846500,MDExOlB1bGxSZXF1ZXN0NzAyOTI3Njk5,33628,[SPARK-36406][CORE] Avoid unnecessary file operations before delete a write failed file held by DiskBlockObjectWriter,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-08-04T03:45:22Z,2021-09-14T15:08:11Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
We always do file truncate operation before delete a write failed file held by `DiskBlockObjectWriter`, a typical process is as follows:

```
if (!success) {
  // This code path only happens if an exception was thrown above before we set success;
  // close our stuff and let the exception be thrown further
  writer.revertPartialWritesAndClose()
  if (file.exists()) {
    if (!file.delete()) {
      logWarning(s""Error deleting ${file}"")
    }
  }
}
```
The `revertPartialWritesAndClose` method will reverts writes that haven't been committed yet,  but it doesn't seem necessary in the current scene. 

So this pr add a new method  to `DiskBlockObjectWriter` named `deleteHeldFile()`,  the new method just revert write metrics and delete the write failed file.

### Why are the changes needed?
Avoid unnecessary file operations.


### Does this PR introduce _any_ user-facing change?
Add a new method  to `DiskBlockObjectWriter` named `deleteHeldFile().

### How was this patch tested?
Pass the Jenkins or GitHub Action
",https://api.github.com/repos/apache/spark/issues/33628/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33628,https://github.com/apache/spark/pull/33628,https://github.com/apache/spark/pull/33628.diff,https://github.com/apache/spark/pull/33628.patch,https://api.github.com/repos/apache/spark/issues/33628/reactions,0,0,0,0,0,0,0,0,0
117,https://api.github.com/repos/apache/spark/issues/33625,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33625/labels{/name},https://api.github.com/repos/apache/spark/issues/33625/comments,https://api.github.com/repos/apache/spark/issues/33625/events,https://github.com/apache/spark/pull/33625,959587708,MDExOlB1bGxSZXF1ZXN0NzAyNjg4MDkz,33625,[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-08-03T22:45:25Z,2021-08-19T16:51:34Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Implement DataFrame.mode (along index axis).


### Why are the changes needed?
Get the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.

### Does this PR introduce _any_ user-facing change?
Yes. `DataFrame.mode` can be used now.

```py
>>> psdf = ps.DataFrame(
...     [(""bird"", 2, 2), (""mammal"", 4, np.nan), (""arthropod"", 8, 0), (""bird"", 2, np.nan)],
...     index=(""falcon"", ""horse"", ""spider"", ""ostrich""),
...     columns=(""species"", ""legs"", ""wings""),
... )
>>> psdf
           species  legs  wings                                                 
falcon        bird     2    2.0
horse       mammal     4    NaN
spider   arthropod     8    0.0
ostrich       bird     2    NaN

>>> psdf.mode()
  species  legs  wings
0    bird   2.0    0.0
1    None   NaN    2.0

>>> psdf.mode(dropna=False)
  species  legs  wings
0    bird     2    NaN

>>> psdf.mode(numeric_only=True)
   legs  wings
0   2.0    0.0
1   NaN    2.0
```

### How was this patch tested?
Unit tests.
",https://api.github.com/repos/apache/spark/issues/33625/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33625,https://github.com/apache/spark/pull/33625,https://github.com/apache/spark/pull/33625.diff,https://github.com/apache/spark/pull/33625.patch,https://api.github.com/repos/apache/spark/issues/33625/reactions,0,0,0,0,0,0,0,0,0
118,https://api.github.com/repos/apache/spark/issues/33621,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33621/labels{/name},https://api.github.com/repos/apache/spark/issues/33621/comments,https://api.github.com/repos/apache/spark/issues/33621/events,https://github.com/apache/spark/pull/33621,958889235,MDExOlB1bGxSZXF1ZXN0NzAyMDg4NzYx,33621,[SPARK-36390][SQL] Replace SessionState.close with SessionState.detachSession,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-08-03T08:55:55Z,2021-08-31T10:47:31Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
SPARK-35286 replace `SessionState.start` with `SessionState.setCurrentSessionState`, but `SessionState.close` will create a `HiveMetaStoreClient` , connect to the Hive Meta Store Server, and then load all functions.

SPARK-35556 (Remove close HiveClient's SessionState) When the Hive version used is greater than or equal to 2.1, `SessionState.close` is not called and the resource dir of HiveClient is not cleaned up.

### Why are the changes needed?
Clean up the hive resources dir temporary directory.
Avoid wasting resources and accelerate the exit speed.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
add UT
",https://api.github.com/repos/apache/spark/issues/33621/timeline,,spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33621,https://github.com/apache/spark/pull/33621,https://github.com/apache/spark/pull/33621.diff,https://github.com/apache/spark/pull/33621.patch,https://api.github.com/repos/apache/spark/issues/33621/reactions,0,0,0,0,0,0,0,0,0
119,https://api.github.com/repos/apache/spark/issues/33588,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33588/labels{/name},https://api.github.com/repos/apache/spark/issues/33588/comments,https://api.github.com/repos/apache/spark/issues/33588/events,https://github.com/apache/spark/pull/33588,956652976,MDExOlB1bGxSZXF1ZXN0NzAwMjEzMzg2,33588,[SPARK-36346][SQL] Support TimestampNTZ type in Orc file source,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,44,2021-07-30T11:17:30Z,2021-08-31T08:05:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
As per https://orc.apache.org/docs/types.html, ORC includes two different forms of timestamps from the SQL world:

`Timestamp` is a date and time without a time zone, which does not change based on the time zone of the reader.
`Timestamp with local time zone` is a fixed instant in time, which does change based on the time zone of the reader.

So the contrast relationship of timestamp between Spark and ORC as follows:
- `TIMESTAMP_LTZ` => `Timestamp with local time zone`.
- `TIMESTAMP_NTZ` => `Timestamp`

Unfortunately, in Spark 3.1 or prior, Spark considered ORC `Timestamp` as `TIMESTAMP_LTZ` mistakely.
The behavior is not correct. To keep backward compatibility, we not change the mistake now.
Since 3.2, with the support of timestamp without time zone type:

- Orc writer follows the definition and uses `Timestamp` on writing `TIMESTAMP_NTZ`.
- Orc reader converts the `Timestamp` to `TIMESTAMP_NTZ`.


### Why are the changes needed?
Docking `TimestampNTZ` type in Spark and `Timestamp` in ORC.


### Does this PR introduce _any_ user-facing change?
'Yes'.
Spark will read/write the `TimestampNTZ`/`Timestamp` in ORC.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/33588/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33588,https://github.com/apache/spark/pull/33588,https://github.com/apache/spark/pull/33588.diff,https://github.com/apache/spark/pull/33588.patch,https://api.github.com/repos/apache/spark/issues/33588/reactions,0,0,0,0,0,0,0,0,0
120,https://api.github.com/repos/apache/spark/issues/33572,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33572/labels{/name},https://api.github.com/repos/apache/spark/issues/33572/comments,https://api.github.com/repos/apache/spark/issues/33572/events,https://github.com/apache/spark/pull/33572,955563966,MDExOlB1bGxSZXF1ZXN0Njk5Mjk2MTk3,33572,[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-07-29T07:52:57Z,2021-08-19T16:51:22Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

The hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.
FYI, In hive 3.0, the will be a timestamp with local timezone added.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

no, timestamp_ntz is new and not public yet
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

new test",https://api.github.com/repos/apache/spark/issues/33572/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33572,https://github.com/apache/spark/pull/33572,https://github.com/apache/spark/pull/33572.diff,https://github.com/apache/spark/pull/33572.patch,https://api.github.com/repos/apache/spark/issues/33572/reactions,0,0,0,0,0,0,0,0,0
121,https://api.github.com/repos/apache/spark/issues/33559,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33559/labels{/name},https://api.github.com/repos/apache/spark/issues/33559/comments,https://api.github.com/repos/apache/spark/issues/33559/events,https://github.com/apache/spark/pull/33559,954819646,MDExOlB1bGxSZXF1ZXN0Njk4NjU2MjM2,33559,[SPARK-34265][WIP][PYTHON] Instrument Python UDFs using SQL metrics ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-07-28T12:47:57Z,2021-08-26T14:34:44Z,,CONTRIBUTOR,,"### What changes are proposed in this pull request?

This proposes to add SQLMetrics instrumentation for Python UDF execution.
The proposed metrics are:

- data sent to Python workers
- data returned from Python workers
- number of rows processed


### Why are the changes needed?
This aims at improving monitoring and performance troubleshooting of Python UDFs.
In particular as an aid to answer performance-related questions such as:
why is the UDF slow?, how much work it has done so far?, etc.

### Does this PR introduce _any_ user-facing change?
SQL metrics are made available in the WEB UI.  
See the following examples:  

![image1](https://user-images.githubusercontent.com/5243162/127323340-f0132da1-e19c-4d81-b5dc-a534ea9346ee.png)
  
![image2](https://issues.apache.org/jira/secure/attachment/13031153/Python_UDF_instrumentation_lite_BatchEvalPython.png)

### How was this patch tested?

Manually tested + a Python unit test has been added.

Example code used for testing:

```
from pyspark.sql.functions import col, pandas_udf
import time

@pandas_udf(""long"")
def test_pandas(col1):
  time.sleep(0.02)
  return col1 * col1

spark.udf.register(""test_pandas"", test_pandas)
spark.sql(""select rand(42)*rand(51)*rand(12) col1 from range(10000000)"").createOrReplaceTempView(""t1"")
spark.sql(""select max(test_pandas(col1)) from t1"").collect()
```

This is used to test with more data pushed to the Python workers

```
from pyspark.sql.functions import col, pandas_udf
import time

@pandas_udf(""long"")
def test_pandas(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12,col13,col14,col15,col16,col17):
  time.sleep(0.02)
  return col1

spark.udf.register(""test_pandas"", test_pandas)
spark.sql(""select rand(42)*rand(51)*rand(12) col1 from range(10000000)"").createOrReplaceTempView(""t1"")
spark.sql(""select max(test_pandas(col1,col1+1,col1+2,col1+3,col1+4,col1+5,col1+6,col1+7,col1+8,col1+9,col1+10,col1+11,col1+12,col1+13,col1+14,col1+15,col1+16)) from t1"").collect()
```

This is for testing Python UDF (non pandas)

`from pyspark.sql.functions import udf; spark.range(100).select(udf(lambda x: x/1)(""id"")).collect()`
  ",https://api.github.com/repos/apache/spark/issues/33559/timeline,,spark,apache,LucaCanali,5243162,MDQ6VXNlcjUyNDMxNjI=,https://avatars.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33559,https://github.com/apache/spark/pull/33559,https://github.com/apache/spark/pull/33559.diff,https://github.com/apache/spark/pull/33559.patch,https://api.github.com/repos/apache/spark/issues/33559/reactions,0,0,0,0,0,0,0,0,0
122,https://api.github.com/repos/apache/spark/issues/33555,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33555/labels{/name},https://api.github.com/repos/apache/spark/issues/33555/comments,https://api.github.com/repos/apache/spark/issues/33555/events,https://github.com/apache/spark/pull/33555,954569598,MDExOlB1bGxSZXF1ZXN0Njk4NDQzMzA5,33555,[SPARK-36100][CORE] Grouping exception in core/status,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-07-28T07:36:59Z,2021-08-19T05:22:42Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/status

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/33555/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33555,https://github.com/apache/spark/pull/33555,https://github.com/apache/spark/pull/33555.diff,https://github.com/apache/spark/pull/33555.patch,https://api.github.com/repos/apache/spark/issues/33555/reactions,0,0,0,0,0,0,0,0,0
123,https://api.github.com/repos/apache/spark/issues/33554,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33554/labels{/name},https://api.github.com/repos/apache/spark/issues/33554/comments,https://api.github.com/repos/apache/spark/issues/33554/events,https://github.com/apache/spark/pull/33554,954567389,MDExOlB1bGxSZXF1ZXN0Njk4NDQxMzM2,33554,[SPARK-36096][CORE] Grouping exception in core/resource,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-07-28T07:34:33Z,2021-09-20T10:26:16Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/resource

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/33554/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33554,https://github.com/apache/spark/pull/33554,https://github.com/apache/spark/pull/33554.diff,https://github.com/apache/spark/pull/33554.patch,https://api.github.com/repos/apache/spark/issues/33554/reactions,0,0,0,0,0,0,0,0,0
124,https://api.github.com/repos/apache/spark/issues/33550,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33550/labels{/name},https://api.github.com/repos/apache/spark/issues/33550/comments,https://api.github.com/repos/apache/spark/issues/33550/events,https://github.com/apache/spark/pull/33550,954435490,MDExOlB1bGxSZXF1ZXN0Njk4MzMxNTI3,33550,[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}]",open,False,,[],,39,2021-07-28T03:23:33Z,2021-08-23T04:23:00Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Use short string as executor pod name prefix if app name is long.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If we have a long spark app name and start with k8s master, we will get the execption.
```
java.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47
	at org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)
	at org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)
	at org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)
	at org.apache.spark.SparkConf.get(SparkConf.scala:261)
	at org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)
	at org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)
	at org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)
```
Use app name as the executor pod name is the Spark internal behavior and we should not make application failure.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add test

the new log:
```
21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.
21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926
21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.
21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
```

verify the config:
![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)

verify the executor pod name
![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)
",https://api.github.com/repos/apache/spark/issues/33550/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33550,https://github.com/apache/spark/pull/33550,https://github.com/apache/spark/pull/33550.diff,https://github.com/apache/spark/pull/33550.patch,https://api.github.com/repos/apache/spark/issues/33550/reactions,0,0,0,0,0,0,0,0,0
125,https://api.github.com/repos/apache/spark/issues/33544,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33544/labels{/name},https://api.github.com/repos/apache/spark/issues/33544/comments,https://api.github.com/repos/apache/spark/issues/33544/events,https://github.com/apache/spark/pull/33544,954209748,MDExOlB1bGxSZXF1ZXN0Njk4MTQwOTkz,33544,[SPARK-34927][INFRA] Support TPCDSQueryBenchmark in Benchmarks,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-07-27T19:42:41Z,2021-08-13T16:40:21Z,,NONE,,"### What changes were proposed in this pull request?

This PR adds a new option(run-tcp-ds-query-benchmark) in 'Run benchmarks' workflow of Github actions in order to support TPCDSQueryBenchmark. 
When this option is true, the job will populate a specific matrix with `org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark` class and `""--data-location tpcds-sf-1""` argument which then executes the TPCDSQueryBenchmark class with the specific argument. Other benchmark classes runs in parallel from separate jobs.

To set up TPC-DS (SF=1) table data, a previous [merged PR](https://github.com/apache/spark/pull/31886/files) was referred 

(NOTE: Also, we need to update 'Running benchmarks in your forked repository' section of  https://spark.apache.org/developer-tools.html to address the new option.)

### Why are the changes needed?

To support a new benchmark class(TPCDSQueryBenchmark.scala) via Github Actions. Currently [this class is excluded](https://github.com/apache/spark/blob/71effba5f2f6f6e4755d0af3411d8d15d0e24577/core/src/test/scala/org/apache/spark/benchmark/Benchmarks.scala#L100) from this feature because the current structure doesn't take an argument for a specific benchmark class. 


### Does this PR introduce _any_ user-facing change?

No. It's only for developers.


### How was this patch tested?

1. Manually tested in my fork repository
- [without TPCDSQueryBenchmark(the new option value equals to 'false')](https://github.com/MyeongKim/spark/actions/runs/1065162581)
- [with TPCDSQueryBenchmark(the new option value equals to 'true')](https://github.com/MyeongKim/spark/actions/runs/1065365313)

2. More test cases with the combinations below

| input class string | run-tcp-ds-query-benchmark flag | GA url |
| ------------- | ------------- | ------------- |
| org.apache.spark.ml.linalg.BLASBenchmark | false |  https://github.com/MyeongKim/spark/actions/runs/1065165135 |
| org.apache.spark.ml.linalg.BLASBenchmark | true | https://github.com/MyeongKim/spark/actions/runs/1065369228 |
| org.apache.spark.sql.* | false | https://github.com/MyeongKim/spark/actions/runs/1066119336 |
| org.apache.spark.sql.* | true | https://github.com/MyeongKim/spark/actions/runs/1066398591 |

3. GA produces an expected `TPCDSQueryBenchmark-results.txt` result file.
",https://api.github.com/repos/apache/spark/issues/33544/timeline,,spark,apache,MyeongKim,8112305,MDQ6VXNlcjgxMTIzMDU=,https://avatars.githubusercontent.com/u/8112305?v=4,,https://api.github.com/users/MyeongKim,https://github.com/MyeongKim,https://api.github.com/users/MyeongKim/followers,https://api.github.com/users/MyeongKim/following{/other_user},https://api.github.com/users/MyeongKim/gists{/gist_id},https://api.github.com/users/MyeongKim/starred{/owner}{/repo},https://api.github.com/users/MyeongKim/subscriptions,https://api.github.com/users/MyeongKim/orgs,https://api.github.com/users/MyeongKim/repos,https://api.github.com/users/MyeongKim/events{/privacy},https://api.github.com/users/MyeongKim/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33544,https://github.com/apache/spark/pull/33544,https://github.com/apache/spark/pull/33544.diff,https://github.com/apache/spark/pull/33544.patch,https://api.github.com/repos/apache/spark/issues/33544/reactions,0,0,0,0,0,0,0,0,0
126,https://api.github.com/repos/apache/spark/issues/33540,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33540/labels{/name},https://api.github.com/repos/apache/spark/issues/33540/comments,https://api.github.com/repos/apache/spark/issues/33540/events,https://github.com/apache/spark/pull/33540,953912818,MDExOlB1bGxSZXF1ZXN0Njk3ODg2ODc0,33540,[SPARK-36102][CORE] Grouping exception in core/deploy,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-07-27T13:57:36Z,2021-08-19T08:55:29Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/deploy

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/33540/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33540,https://github.com/apache/spark/pull/33540,https://github.com/apache/spark/pull/33540.diff,https://github.com/apache/spark/pull/33540.patch,https://api.github.com/repos/apache/spark/issues/33540/reactions,0,0,0,0,0,0,0,0,0
127,https://api.github.com/repos/apache/spark/issues/33535,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33535/labels{/name},https://api.github.com/repos/apache/spark/issues/33535/comments,https://api.github.com/repos/apache/spark/issues/33535/events,https://github.com/apache/spark/pull/33535,953649242,MDExOlB1bGxSZXF1ZXN0Njk3NjU5Mjkx,33535,[SPARK-36108][SQL] Refactor first set of 20 query parsing errors to use error classes,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-07-27T08:44:54Z,2021-09-16T20:56:14Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR refactor some exceptions in `QueryParsingErrors` to use error classes.

There are currently ~100 exceptions in this file; so this PR only focuses on the first set of 20.


### Why are the changes needed?
To improve auditing, reduce duplication, and improve quality of error messages thrown from Spark, we should group them in a single JSON file (as discussed in the[ mailing list](http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Add-error-IDs-td31126.html) and introduced in SPARK-34920).


### Does this PR introduce _any_ user-facing change?
'No'.
Just use new error classes.


### How was this patch tested?
Jenkins test.
",https://api.github.com/repos/apache/spark/issues/33535/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33535,https://github.com/apache/spark/pull/33535,https://github.com/apache/spark/pull/33535.diff,https://github.com/apache/spark/pull/33535.patch,https://api.github.com/repos/apache/spark/issues/33535/reactions,0,0,0,0,0,0,0,0,0
128,https://api.github.com/repos/apache/spark/issues/33534,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33534/labels{/name},https://api.github.com/repos/apache/spark/issues/33534/comments,https://api.github.com/repos/apache/spark/issues/33534/events,https://github.com/apache/spark/pull/33534,953647026,MDExOlB1bGxSZXF1ZXN0Njk3NjU3Mzg3,33534,[SPARK-36099][CORE] Grouping exception in core/util,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-07-27T08:42:11Z,2021-08-19T09:06:29Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR group exception messages in core/src/main/scala/org/apache/spark/util

### Why are the changes needed?
It will largely help with standardization of error messages and its maintenance.

### Does this PR introduce _any_ user-facing change?
No. Error messages remain unchanged.

### How was this patch tested?
No new tests - pass all original tests to make sure it doesn't break any existing behavior.",https://api.github.com/repos/apache/spark/issues/33534/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33534,https://github.com/apache/spark/pull/33534,https://github.com/apache/spark/pull/33534.diff,https://github.com/apache/spark/pull/33534.patch,https://api.github.com/repos/apache/spark/issues/33534/reactions,0,0,0,0,0,0,0,0,0
129,https://api.github.com/repos/apache/spark/issues/33522,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33522/labels{/name},https://api.github.com/repos/apache/spark/issues/33522/comments,https://api.github.com/repos/apache/spark/issues/33522/events,https://github.com/apache/spark/pull/33522,953034263,MDExOlB1bGxSZXF1ZXN0Njk3MTM5NDU4,33522,[SPARK-36290][SQL] Pull out join condition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,27,2021-07-26T15:37:56Z,2021-08-19T16:50:57Z,,MEMBER,,"### What changes were proposed in this pull request?

Similar to [`PullOutGroupingExpressions`](https://github.com/wangyum/spark/blob/7fd3f8f9ec55b364525407213ba1c631705686c5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutGroupingExpressions.scala#L48). This pr add a new rule(`PullOutJoinCondition`) to pull out join condition. Otherwise the expression in join condition may be evaluated three times(`ShuffleExchangeExec`, `SortExec` and the join itself). For example:
```sql
CREATE TABLE t1 using parquet AS select id as a, id as b from range(100000000L);
CREATE TABLE t2 using parquet AS select id as a, id as b from range(200000000L);
SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc');
```
Before this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#6L, b#7L]
   +- SortMergeJoin [translate(cast(a#6L as string), 123, abc)], [translate(cast(a#8L as string), 123, abc)], Inner
      :- Sort [translate(cast(a#6L as string), 123, abc) ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(translate(cast(a#6L as string), 123, abc), 5), ENSURE_REQUIREMENTS, [id=#89]
      :     +- Filter isnotnull(a#6L)
      :        +- FileScan parquet default.t1[a#6L,b#7L]
      +- Sort [translate(cast(a#8L as string), 123, abc) ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(translate(cast(a#8L as string), 123, abc), 5), ENSURE_REQUIREMENTS, [id=#90]
            +- Filter isnotnull(a#8L)
               +- FileScan parquet default.t2[a#8L]
```
After this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#6L, b#7L]
   +- SortMergeJoin [translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12], [translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13], Inner
      :- Sort [translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12 ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12, 5), ENSURE_REQUIREMENTS, [id=#53]
      :     +- Project [a#6L, b#7L, translate(cast(a#6L as string), 123, abc) AS translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12]
      :        +- Filter isnotnull(translate(cast(a#6L as string), 123, abc))
      :           +- FileScan parquet default.t1[a#6L,b#7L]
      +- Sort [translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13, 5), ENSURE_REQUIREMENTS, [id=#54]
            +- Project [translate(cast(a#8L as string), 123, abc) AS translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13]
               +- Filter isnotnull(translate(cast(a#8L as string), 123, abc))
                  +- FileScan parquet default.t2[a#8L]
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 15
spark.sql(s""CREATE TABLE t1 using parquet AS select id as a, id as b from range(${numRows}L)"")
spark.sql(s""CREATE TABLE t2 using parquet AS select id as a, id as b from range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark pull out join condition"", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s""Pull out join condition ${if (pullOutEnabled) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.pullOutJoinCondition"" -> s""$pullOutEnabled"") {
      spark.sql(""SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc')"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
Benchmark result:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out join condition:        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out join condition (Disabled)                30197          34046         690          0.5        1919.9       1.0X
Pull out join condition (Enabled)                 19631          20484         535          0.8        1248.1       1.5X
```
",https://api.github.com/repos/apache/spark/issues/33522/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33522,https://github.com/apache/spark/pull/33522,https://github.com/apache/spark/pull/33522.diff,https://github.com/apache/spark/pull/33522.patch,https://api.github.com/repos/apache/spark/issues/33522/reactions,0,0,0,0,0,0,0,0,0
130,https://api.github.com/repos/apache/spark/issues/33520,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33520/labels{/name},https://api.github.com/repos/apache/spark/issues/33520/comments,https://api.github.com/repos/apache/spark/issues/33520/events,https://github.com/apache/spark/pull/33520,952891175,MDExOlB1bGxSZXF1ZXN0Njk3MDE4MDQz,33520,[SPARK-36289][SQL] Rewrite distinct count case when expressions without Expand node,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-07-26T13:13:48Z,2021-08-03T03:33:12Z,,NONE,,"### What changes were proposed in this pull request?

Currently, `RewriteDistinctAggregates` rule will rewrite distinct aggregates with extra Expand node. This causes unnecessary memory waste and performance fallback for some specific aggregates (e.g. count distinct case when).

for query:

```sql
   SELECT
     cat1,
     COUNT(DISTINCT CASE WHEN cond1 THEN cat2 ELSE null end) as cat2_cnt1,
     COUNT(DISTINCT CASE WHEN cond2 THEN cat2 ELSE null end) as cat2_cnt2,
     FROM
       data
     GROUP BY
       key
```


Currently, we rewrite it to :

```
Aggregate(
   key = ['key]
   functions = [count(if (('gid = 1) and 'casewhen1) 'cat2_1 else null),
                count(if (('gid = 2) and 'casewhen2) 'cat2_2 else null)]
   output = ['key, 'cat2_cnt1, 'cat2_cnt2])
   Aggregate(
      key = ['key, 'casewhen1, 'casewhen2, 'gid]
      functions = []
      output = ['key, 'casewhen1, 'casewhen2, 'gid])
     Expand(
        projections = [('key, 'cat1, CASE WHEN cond1 THEN cat2 ELSE null END, null, 1),
                       ('key, 'cat1, 'null, CASE WHEN cond1 THEN cat2 ELSE null END, 2)]
        output = ['key, 'cat1, 'casewhen1, 'casewhen2, 'gid])
       LocalTableScan [...]
```


This PR will improve performance and reduce memory waste. And query will be rewrite to:

- if # of case when expr < 64

    ```
    Aggregate(
       key = ['key]
       functions = [count(if (01 & 'bit_vector != 0) 0 else null),
                    count(if (10 & 'bit_vector != 0) 0 else null)]
       output = ['key, 'cat2_cnt1, 'cat2_cnt2])
       Aggregate(
          key = ['key, 'cat1]
          functions = [bit_or(if (cond1) 01 else 00, if (cond2) 10 else 00)]
          output = ['key, 'cat1, 'bit_vector])
           LocalTableScan [...]
    ```

- if # of case when expr >= 64

    ```
    Aggregate(
       key = ['key]
       functions = [count(if ('sum1 >= 1) 1 else null),
                    count(if ('sum2 >= 1) 1 else null)]
       output = ['key, 'cat1_cnt1, 'cat1_cnt2])
       Aggregate(
          key = ['key, 'cat1]
          functions = [sum(if (cond1) 1 else 0), sum(if (cond2) 1 else 0)]
          output = ['key, 'cat1, 'sum1, 'sum2])
           LocalTableScan [...]
    ```

### Why are the changes needed?
we can rewrite count distinct case when more efficiently

### Does this PR introduce _any_ user-facing change?
no

### How was this patch tested?
UT in `RewriteDistinctAggregatesSuite`",https://api.github.com/repos/apache/spark/issues/33520/timeline,,spark,apache,Gabriel39,37700562,MDQ6VXNlcjM3NzAwNTYy,https://avatars.githubusercontent.com/u/37700562?v=4,,https://api.github.com/users/Gabriel39,https://github.com/Gabriel39,https://api.github.com/users/Gabriel39/followers,https://api.github.com/users/Gabriel39/following{/other_user},https://api.github.com/users/Gabriel39/gists{/gist_id},https://api.github.com/users/Gabriel39/starred{/owner}{/repo},https://api.github.com/users/Gabriel39/subscriptions,https://api.github.com/users/Gabriel39/orgs,https://api.github.com/users/Gabriel39/repos,https://api.github.com/users/Gabriel39/events{/privacy},https://api.github.com/users/Gabriel39/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33520,https://github.com/apache/spark/pull/33520,https://github.com/apache/spark/pull/33520.diff,https://github.com/apache/spark/pull/33520.patch,https://api.github.com/repos/apache/spark/issues/33520/reactions,0,0,0,0,0,0,0,0,0
131,https://api.github.com/repos/apache/spark/issues/33510,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33510/labels{/name},https://api.github.com/repos/apache/spark/issues/33510/comments,https://api.github.com/repos/apache/spark/issues/33510/events,https://github.com/apache/spark/pull/33510,952043163,MDExOlB1bGxSZXF1ZXN0Njk2MzI2NTk4,33510,[SPARK-36223][SQL][TEST] Cover 3 kinds of join in the TPCDSQueryTestSuite,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-07-24T09:58:33Z,2021-09-26T09:01:36Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
In current github actions we run TPCDSQueryTestSuite for tpcds benchmark. But it's only tested under default configurations. Since we have added the `spark.sql.join.forceApplyShuffledHashJoin` config. Now we can test all 3 join strategies in TPCDS to improve the coverage.

### Why are the changes needed?
Improve the coverage of join strategies in the TPCDS.

### Does this PR introduce _any_ user-facing change?
No, only for testing.

### How was this patch tested?
No need.",https://api.github.com/repos/apache/spark/issues/33510/timeline,,spark,apache,jerqi,8159038,MDQ6VXNlcjgxNTkwMzg=,https://avatars.githubusercontent.com/u/8159038?v=4,,https://api.github.com/users/jerqi,https://github.com/jerqi,https://api.github.com/users/jerqi/followers,https://api.github.com/users/jerqi/following{/other_user},https://api.github.com/users/jerqi/gists{/gist_id},https://api.github.com/users/jerqi/starred{/owner}{/repo},https://api.github.com/users/jerqi/subscriptions,https://api.github.com/users/jerqi/orgs,https://api.github.com/users/jerqi/repos,https://api.github.com/users/jerqi/events{/privacy},https://api.github.com/users/jerqi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33510,https://github.com/apache/spark/pull/33510,https://github.com/apache/spark/pull/33510.diff,https://github.com/apache/spark/pull/33510.patch,https://api.github.com/repos/apache/spark/issues/33510/reactions,0,0,0,0,0,0,0,0,0
132,https://api.github.com/repos/apache/spark/issues/33491,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33491/labels{/name},https://api.github.com/repos/apache/spark/issues/33491/comments,https://api.github.com/repos/apache/spark/issues/33491/events,https://github.com/apache/spark/pull/33491,951316126,MDExOlB1bGxSZXF1ZXN0Njk1NzI0MTA3,33491,[WIP][SPARK-34851][CORE][SQL] Add tag for each configuration,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-07-23T07:15:25Z,2021-07-25T05:12:08Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Add tag for each configuration


### Why are the changes needed?
Add tag for each configuration


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

",https://api.github.com/repos/apache/spark/issues/33491/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33491,https://github.com/apache/spark/pull/33491,https://github.com/apache/spark/pull/33491.diff,https://github.com/apache/spark/pull/33491.patch,https://api.github.com/repos/apache/spark/issues/33491/reactions,0,0,0,0,0,0,0,0,0
133,https://api.github.com/repos/apache/spark/issues/33482,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33482/labels{/name},https://api.github.com/repos/apache/spark/issues/33482/comments,https://api.github.com/repos/apache/spark/issues/33482/events,https://github.com/apache/spark/pull/33482,950709737,MDExOlB1bGxSZXF1ZXN0Njk1MjE2NTcy,33482,[SPARK-36259] Expose localtimestamp in pyspark.sql.functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-07-22T14:07:37Z,2021-07-26T11:19:12Z,,CONTRIBUTOR,,"

### What changes were proposed in this pull request?
Exposing localtimestamp in pyspark.sql.functions


### Why are the changes needed?
Was previously only available in scala


### Does this PR introduce _any_ user-facing change?
new localtimestamp in pyspark.sql.functions


### How was this patch tested?
test added inline
",https://api.github.com/repos/apache/spark/issues/33482/timeline,,spark,apache,dominikgehl,98841,MDQ6VXNlcjk4ODQx,https://avatars.githubusercontent.com/u/98841?v=4,,https://api.github.com/users/dominikgehl,https://github.com/dominikgehl,https://api.github.com/users/dominikgehl/followers,https://api.github.com/users/dominikgehl/following{/other_user},https://api.github.com/users/dominikgehl/gists{/gist_id},https://api.github.com/users/dominikgehl/starred{/owner}{/repo},https://api.github.com/users/dominikgehl/subscriptions,https://api.github.com/users/dominikgehl/orgs,https://api.github.com/users/dominikgehl/repos,https://api.github.com/users/dominikgehl/events{/privacy},https://api.github.com/users/dominikgehl/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33482,https://github.com/apache/spark/pull/33482,https://github.com/apache/spark/pull/33482.diff,https://github.com/apache/spark/pull/33482.patch,https://api.github.com/repos/apache/spark/issues/33482/reactions,0,0,0,0,0,0,0,0,0
134,https://api.github.com/repos/apache/spark/issues/33465,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33465/labels{/name},https://api.github.com/repos/apache/spark/issues/33465/comments,https://api.github.com/repos/apache/spark/issues/33465/events,https://github.com/apache/spark/pull/33465,949809958,MDExOlB1bGxSZXF1ZXN0Njk0NDU2Mjg0,33465,[SPARK-36245][SQL] Deduplicate the right side of left semi/anti join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2021-07-21T14:56:54Z,2021-08-19T16:50:43Z,,MEMBER,,"### What changes were proposed in this pull request?

Add a new rule to deduplicate the right side of left semi/anti join if it cannot be planed as broadcast hash join and there are many duplicate values. For example:
```scala
sql(""CREATE TABLE t1 USING PARQUET AS SELECT id a, id AS b, id AS c FROM range(100000000)"")
sql(""CREATE TABLE t2 USING PARQUET AS SELECT id % 500000 AS a, id % 500000 AS b FROM range(50000000)"")
sql(""ANALYZE TABLE t1 COMPUTE STATISTICS FOR ALL COLUMNS"")
sql(""ANALYZE TABLE t2 COMPUTE STATISTICS FOR ALL COLUMNS"")
sql(""set spark.sql.cbo.enabled=true"")
sql(""SELECT * FROM t1 LEFT SEMI JOIN t2 ON t1.a = t2.a"").explain(""cost"")
```
Before this pr:
```
== Optimized Logical Plan ==
Join LeftSemi, (a#11575L = a#11578L), Statistics(sizeInBytes=3.0 GiB, rowCount=1.00E+8)
:- Filter isnotnull(a#11575L), Statistics(sizeInBytes=3.0 GiB, rowCount=1.00E+8)
:  +- Relation default.t1[a#11575L,b#11576L,c#11577L] parquet, Statistics(sizeInBytes=3.0 GiB, rowCount=1.00E+8)
+- Project [a#11578L], Statistics(sizeInBytes=762.9 MiB, rowCount=5.00E+7)
   +- Filter isnotnull(a#11578L), Statistics(sizeInBytes=1144.4 MiB, rowCount=5.00E+7)
      +- Relation default.t2[a#11578L,b#11579L] parquet, Statistics(sizeInBytes=1144.4 MiB, rowCount=5.00E+7)
```
After this pr:
```
== Optimized Logical Plan ==
Join LeftSemi, (a#11575L = a#11578L), Statistics(sizeInBytes=3.0 GiB, rowCount=1.00E+8)
:- Filter isnotnull(a#11575L), Statistics(sizeInBytes=3.0 GiB, rowCount=1.00E+8)
:  +- Relation default.t1[a#11575L,b#11576L,c#11577L] parquet, Statistics(sizeInBytes=3.0 GiB, rowCount=1.00E+8)
+- Aggregate [a#11578L], [a#11578L], Statistics(sizeInBytes=7.2 MiB, rowCount=4.70E+5)
   +- Project [a#11578L], Statistics(sizeInBytes=762.9 MiB, rowCount=5.00E+7)
      +- Filter isnotnull(a#11578L), Statistics(sizeInBytes=1144.4 MiB, rowCount=5.00E+7)
         +- Relation default.t2[a#11578L,b#11579L] parquet, Statistics(sizeInBytes=1144.4 MiB, rowCount=5.00E+7)
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and TPC-DS benchmark test.

1. spark.sql.cbo.enabled=true and spark.sql.cbo.joinReorder.enabled=true

SQL | Before this PR(Seconds) | After this PR(Seconds)
-- | -- | --
q10 | 15 | 10
q14a | 174 | 78
q35 | 34 | 23
q69 | 14 | 8
q95 | 38 | 26

2. spark.sql.cbo.enabled=false

SQL | Before this PR(Seconds) | After this PR(Seconds)
-- | -- | --
q10 | 20 | 11
q35 | 38 | 29
q69 | 27 | 13",https://api.github.com/repos/apache/spark/issues/33465/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33465,https://github.com/apache/spark/pull/33465,https://github.com/apache/spark/pull/33465.diff,https://github.com/apache/spark/pull/33465.patch,https://api.github.com/repos/apache/spark/issues/33465/reactions,0,0,0,0,0,0,0,0,0
135,https://api.github.com/repos/apache/spark/issues/33446,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33446/labels{/name},https://api.github.com/repos/apache/spark/issues/33446/comments,https://api.github.com/repos/apache/spark/issues/33446/events,https://github.com/apache/spark/pull/33446,948883378,MDExOlB1bGxSZXF1ZXN0NjkzNjU5MzA1,33446,[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-07-20T17:33:49Z,2021-07-28T01:16:22Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Add logging to `ShuffleBlockFetcherIterator` to log ""slow"" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Currently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is ""slow"".
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Adds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test",https://api.github.com/repos/apache/spark/issues/33446/timeline,,spark,apache,shardulm94,6961317,MDQ6VXNlcjY5NjEzMTc=,https://avatars.githubusercontent.com/u/6961317?v=4,,https://api.github.com/users/shardulm94,https://github.com/shardulm94,https://api.github.com/users/shardulm94/followers,https://api.github.com/users/shardulm94/following{/other_user},https://api.github.com/users/shardulm94/gists{/gist_id},https://api.github.com/users/shardulm94/starred{/owner}{/repo},https://api.github.com/users/shardulm94/subscriptions,https://api.github.com/users/shardulm94/orgs,https://api.github.com/users/shardulm94/repos,https://api.github.com/users/shardulm94/events{/privacy},https://api.github.com/users/shardulm94/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33446,https://github.com/apache/spark/pull/33446,https://github.com/apache/spark/pull/33446.diff,https://github.com/apache/spark/pull/33446.patch,https://api.github.com/repos/apache/spark/issues/33446/reactions,0,0,0,0,0,0,0,0,0
136,https://api.github.com/repos/apache/spark/issues/33428,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33428/labels{/name},https://api.github.com/repos/apache/spark/issues/33428/comments,https://api.github.com/repos/apache/spark/issues/33428/events,https://github.com/apache/spark/pull/33428,948199030,MDExOlB1bGxSZXF1ZXN0NjkzMDcyODIy,33428,[SPARK-36220][PYTHON] Fix pyspark.sql.types.Row type annotation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-07-20T02:53:28Z,2021-07-26T07:02:06Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This change changes the type annotations for `pyspark.sql.types.Row`'s `__new__` and `__init__` methods when invoked without keyword arguments (_i.e._, `*args` rather than `**kwargs`) from `str` to `Any`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
When creating a [Row](https://hyukjin-spark.readthedocs.io/en/latest/reference/api/pyspark.sql.types.Row.html) with unnamed fields which are not of type `str` (_e.g._, `row1 = Row(""Alice"", 11)` appears in the `Row` documentation) type checkers produce an error.

The implementation doesn't assume the arguments are of type `str`, and in fact the documentation includes an example where non-`str` types are provided in this way (see [the final example here](https://hyukjin-spark.readthedocs.io/en/latest/reference/api/pyspark.sql.types.Row.html)).

An example of the type error produced by [pyright](https://github.com/microsoft/pyright) is
```
error: No overloads for ""__init__"" match the provided arguments
  Argument types: (Literal['Alice'], Literal[11]) (reportGeneralTypeIssues)
```


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
It changes the python type annotation for `Row`, which is user facing. The change makes the type annotation more general, and should not be a breaking change in any sense.

The previous type annotation behaves like so:
```python
# Will not produce type errors:
Row(""Alice"", ""Bob"")
# Will produce type errors:
Row(""Alice"", 11)
```
After this change, both lines will be accepted by a type checker.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

- [x] MyPy tests of PySpark source code
    ```
    mypy --no-incremental --config python/mypy.ini python/pyspark
    ```
    For me, this currently fails on `master` with the following errors:
    ```
    python/pyspark/mllib/tree.pyi:29: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/tree.pyi:38: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:34: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:42: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:48: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:54: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:76: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:124: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/feature.pyi:165: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/clustering.pyi:45: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/clustering.pyi:72: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/classification.pyi:39: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    python/pyspark/mllib/classification.pyi:52: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
    Found 13 errors in 4 files (checked 314 source files)
    ```
    Re-running after my change doesn't introduce any new errors. MyPy version 0.910 was used.

Tagging @HyukjinKwon and @zero323 as you both seem to have worked on type annotations recently. Thanks in advance for your help  ",https://api.github.com/repos/apache/spark/issues/33428/timeline,,spark,apache,tobiasedwards,3261881,MDQ6VXNlcjMyNjE4ODE=,https://avatars.githubusercontent.com/u/3261881?v=4,,https://api.github.com/users/tobiasedwards,https://github.com/tobiasedwards,https://api.github.com/users/tobiasedwards/followers,https://api.github.com/users/tobiasedwards/following{/other_user},https://api.github.com/users/tobiasedwards/gists{/gist_id},https://api.github.com/users/tobiasedwards/starred{/owner}{/repo},https://api.github.com/users/tobiasedwards/subscriptions,https://api.github.com/users/tobiasedwards/orgs,https://api.github.com/users/tobiasedwards/repos,https://api.github.com/users/tobiasedwards/events{/privacy},https://api.github.com/users/tobiasedwards/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33428,https://github.com/apache/spark/pull/33428,https://github.com/apache/spark/pull/33428.diff,https://github.com/apache/spark/pull/33428.patch,https://api.github.com/repos/apache/spark/issues/33428/reactions,1,1,0,0,0,0,0,0,0
137,https://api.github.com/repos/apache/spark/issues/33404,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33404/labels{/name},https://api.github.com/repos/apache/spark/issues/33404/comments,https://api.github.com/repos/apache/spark/issues/33404/events,https://github.com/apache/spark/pull/33404,946850336,MDExOlB1bGxSZXF1ZXN0NjkxOTUyMTk4,33404,[SPARK-36194][SQL] Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-07-17T15:59:39Z,2021-08-03T07:38:24Z,,MEMBER,,"### What changes were proposed in this pull request?

Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side. For example:
```sql
set spark.sql.autoBroadcastJoinThreshold=-1; -- avoid PushDownLeftSemiAntiJoin
create table t1 using parquet as select id a, id as b from range(10);
create table t2 using parquet as select id as a, id as b from range(8);
select t11.a, t11.b from (select distinct a, b from t1) t11 left semi join t2 on (t11.a = t2.a) group by t11.a, t11.b;
```

Before this PR:
```
== Optimized Logical Plan ==
Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
+- Join LeftSemi, (a#6L = a#8L), Statistics(sizeInBytes=1492.0 B)
   :- Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
   :  +- Filter isnotnull(a#6L), Statistics(sizeInBytes=1492.0 B)
   :     +- Relation default.t1[a#6L,b#7L] parquet, Statistics(sizeInBytes=1492.0 B)
   +- Project [a#8L], Statistics(sizeInBytes=984.0 B)
      +- Filter isnotnull(a#8L), Statistics(sizeInBytes=1476.0 B)
         +- Relation default.t2[a#8L,b#9L] parquet, Statistics(sizeInBytes=1476.0 B)
```

After this PR:
```
== Optimized Logical Plan ==
Join LeftSemi, (a#6L = a#8L), Statistics(sizeInBytes=1492.0 B)
:- Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
:  +- Filter isnotnull(a#6L), Statistics(sizeInBytes=1492.0 B)
:     +- Relation default.t1[a#6L,b#7L] parquet, Statistics(sizeInBytes=1492.0 B)
+- Project [a#8L], Statistics(sizeInBytes=984.0 B)
   +- Filter isnotnull(a#8L), Statistics(sizeInBytes=1476.0 B)
      +- Relation default.t2[a#8L,b#9L] parquet, Statistics(sizeInBytes=1476.0 B)
```

This rule can be disabled by:
```sql
set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAggregatesInLeftSemiAntiJoin;
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and TPC-DS benchmark test.

SQL | Before this PR(Seconds) | After this PR(Seconds)
-- | -- | --
q14a | 174 | 165
q38 | 26 | 23
q87 | 30 | 26
",https://api.github.com/repos/apache/spark/issues/33404/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33404,https://github.com/apache/spark/pull/33404,https://github.com/apache/spark/pull/33404.diff,https://github.com/apache/spark/pull/33404.patch,https://api.github.com/repos/apache/spark/issues/33404/reactions,0,0,0,0,0,0,0,0,0
138,https://api.github.com/repos/apache/spark/issues/33395,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33395/labels{/name},https://api.github.com/repos/apache/spark/issues/33395/comments,https://api.github.com/repos/apache/spark/issues/33395/events,https://github.com/apache/spark/pull/33395,946372192,MDExOlB1bGxSZXF1ZXN0NjkxNTU2Mjcy,33395,[WIP][SPARK-36182][SQL] Support TimestampNTZ type in Parquet file source,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-07-16T15:01:06Z,2021-07-16T20:39:09Z,,MEMBER,,This is still WIP. I am deciding the behaviors of the Parquet reader for both schema inference and user-provided schema.,https://api.github.com/repos/apache/spark/issues/33395/timeline,,spark,apache,gengliangwang,1097932,MDQ6VXNlcjEwOTc5MzI=,https://avatars.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33395,https://github.com/apache/spark/pull/33395,https://github.com/apache/spark/pull/33395.diff,https://github.com/apache/spark/pull/33395.patch,https://api.github.com/repos/apache/spark/issues/33395/reactions,0,0,0,0,0,0,0,0,0
139,https://api.github.com/repos/apache/spark/issues/33361,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33361/labels{/name},https://api.github.com/repos/apache/spark/issues/33361/comments,https://api.github.com/repos/apache/spark/issues/33361/events,https://github.com/apache/spark/pull/33361,945139869,MDExOlB1bGxSZXF1ZXN0NjkwNTEyNjU2,33361,[SPARK-36155][SQL] Eliminate outer join base uniqueness,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-07-15T08:36:49Z,2021-08-03T12:49:09Z,,MEMBER,,"### What changes were proposed in this pull request?

Removes outer join if it only references the streamed side and the uniqueness can be guaranteed on the buffered side.

For example:
```sql
create table t1(id int, name varchar(255)) using parquet;
create table t2(id int, name varchar(255)) using parquet;
select t1.name from t1 left join (select distinct id from t2) t21 on t1.id = t21.id;
```

Before this PR:
```
== Optimized Logical Plan ==
Project [name#1]
+- Join LeftOuter, (id#0 = id#2)
   :- Relation default.t1[id#0,name#1] parquet
   +- Aggregate [id#2], [id#2]
      +- Project [id#2]
         +- Filter isnotnull(id#2)
            +- Relation default.t2[id#2,name#3] parquet
```

After this PR:
```
== Optimized Logical Plan ==
Project [name#1]
+- Relation default.t1[id#0,name#1] parquet
```

### Why are the changes needed?

1. Improve query performance.
2. PostgreSQL support this optimization:
   ```sql
   postgres=# create table t1(id int, name varchar(255));
   CREATE TABLE
   postgres=# create table t2(id int, name varchar(255));
   CREATE TABLE
   postgres=# explain select t1.name from t1 left join (select distinct id from t2) t21 on t1.id = t21.id;
                         QUERY PLAN
   -------------------------------------------------------
    Seq Scan on t1  (cost=0.00..11.40 rows=140 width=516)
   (1 row)
   
   postgres=# select version();
                                                        version
   ------------------------------------------------------------------------------------------------------------------
    PostgreSQL 13.3 (Debian 13.3-1.pgdg100+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 8.3.0-6) 8.3.0, 64-bit
   (1 row)
   ```

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.
",https://api.github.com/repos/apache/spark/issues/33361/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33361,https://github.com/apache/spark/pull/33361,https://github.com/apache/spark/pull/33361.diff,https://github.com/apache/spark/pull/33361.patch,https://api.github.com/repos/apache/spark/issues/33361/reactions,0,0,0,0,0,0,0,0,0
140,https://api.github.com/repos/apache/spark/issues/33339,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33339/labels{/name},https://api.github.com/repos/apache/spark/issues/33339/comments,https://api.github.com/repos/apache/spark/issues/33339/events,https://github.com/apache/spark/pull/33339,944080936,MDExOlB1bGxSZXF1ZXN0Njg5NjE2NzY3,33339,[SPARK-36133][SQL] Add empty check for catalog name,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-07-14T06:13:02Z,2021-09-09T07:39:22Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Add the name check when initialize a catalog by name. Keep consistent with the namespace/table naming rule.

### Why are the changes needed?
When I try to add the command `SHOW CATALOGS`, I found catalog names are meaningless in some cases. eg:
1. if the catalog name is `""""`, `"" ""`, `""  ""` that contains some spaces and so on. I think it doesn't make sense
2. if the catalog name is `""a.b""` that contains `.`, I'm more confused. `b` is the name of a namespace or part of catalog name?

[#WISH#SPARK-36133](https://issues.apache.org/jira/browse/SPARK-36133)

### Does this PR introduce _any_ user-facing change?
Yes. If user want to regist external catalog plugin. user shoule keep catalog name in SparkConf is compliant.


### How was this patch tested?
Add ut test
",https://api.github.com/repos/apache/spark/issues/33339/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33339,https://github.com/apache/spark/pull/33339,https://github.com/apache/spark/pull/33339.diff,https://github.com/apache/spark/pull/33339.patch,https://api.github.com/repos/apache/spark/issues/33339/reactions,0,0,0,0,0,0,0,0,0
141,https://api.github.com/repos/apache/spark/issues/33323,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33323/labels{/name},https://api.github.com/repos/apache/spark/issues/33323/comments,https://api.github.com/repos/apache/spark/issues/33323/events,https://github.com/apache/spark/pull/33323,943418526,MDExOlB1bGxSZXF1ZXN0Njg5MDQ5ODA2,33323,[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-07-13T13:53:57Z,2021-08-17T04:37:02Z,,NONE,,"### What changes were proposed in this pull request?
Adds 3 new syntactic sugar overloads to Dataset's join method as proposed in [SPARK-35739](https://issues.apache.org/jira/browse/SPARK-35739).

### Why are the changes needed?
Improved development experience for developers using Spark SQL, specifically when coding in Java.  

Prior to changes the Seq overloads required developers to use less-known Java-to-Scala converter methods that made code less readable.  The overloads internalize those converter calls for two of the new methods and the third method adds a single-item overload that is useful for both Java and Scala.

### Does this PR introduce _any_ user-facing change?
Yes, the three new overloads technically constitute an API change to the Dataset class.  These overloads are net-new and have been commented appropriately in line with the existing methods.

### How was this patch tested?
Test cases were not added because it is unclear to me where/how syntactic sugar overloads fit into the testing suites (if at all).  Happy to add them if I can be pointed in the correct direction.

* Changes were tested in Scala via spark-shell.
* Changes were tested in Java by modifying an example:
  ```
  diff --git a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  index 86a9045d8a..342810c1e6 100644
  --- a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  +++ b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  @@ -124,6 +124,10 @@ public class JavaSparkSQLExample {
       // |-- age: long (nullable = true)
       // |-- name: string (nullable = true)

  +    df.join(df, new String[] {""age""}).show();
  +    df.join(df, ""age"", ""left"").show();
  +    df.join(df, new String[] {""age""}, ""left"").show();
  +
       // Select only the ""name"" column
       df.select(""name"").show();
       // +-------+
  ```",https://api.github.com/repos/apache/spark/issues/33323/timeline,,spark,apache,brandondahler,1155895,MDQ6VXNlcjExNTU4OTU=,https://avatars.githubusercontent.com/u/1155895?v=4,,https://api.github.com/users/brandondahler,https://github.com/brandondahler,https://api.github.com/users/brandondahler/followers,https://api.github.com/users/brandondahler/following{/other_user},https://api.github.com/users/brandondahler/gists{/gist_id},https://api.github.com/users/brandondahler/starred{/owner}{/repo},https://api.github.com/users/brandondahler/subscriptions,https://api.github.com/users/brandondahler/orgs,https://api.github.com/users/brandondahler/repos,https://api.github.com/users/brandondahler/events{/privacy},https://api.github.com/users/brandondahler/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33323,https://github.com/apache/spark/pull/33323,https://github.com/apache/spark/pull/33323.diff,https://github.com/apache/spark/pull/33323.patch,https://api.github.com/repos/apache/spark/issues/33323/reactions,0,0,0,0,0,0,0,0,0
142,https://api.github.com/repos/apache/spark/issues/33314,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33314/labels{/name},https://api.github.com/repos/apache/spark/issues/33314/comments,https://api.github.com/repos/apache/spark/issues/33314/events,https://github.com/apache/spark/pull/33314,942685825,MDExOlB1bGxSZXF1ZXN0Njg4Mzg1NTc1,33314,[SPARK-36118][SQL] Add bitmap functions for Spark SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-07-13T03:56:43Z,2021-08-03T03:42:27Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
add functions of bitmap building and computing cardinality for Spark SQL, If this is ok, I will update function.scala and FunctionRegistry.scala.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Bitmaps are used more and more widely, and many frameworks have native support, such as Clickhouse

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
CI, it performs well on billions of rows based on our real demand",https://api.github.com/repos/apache/spark/issues/33314/timeline,,spark,apache,ReachInfi,87301083,MDQ6VXNlcjg3MzAxMDgz,https://avatars.githubusercontent.com/u/87301083?v=4,,https://api.github.com/users/ReachInfi,https://github.com/ReachInfi,https://api.github.com/users/ReachInfi/followers,https://api.github.com/users/ReachInfi/following{/other_user},https://api.github.com/users/ReachInfi/gists{/gist_id},https://api.github.com/users/ReachInfi/starred{/owner}{/repo},https://api.github.com/users/ReachInfi/subscriptions,https://api.github.com/users/ReachInfi/orgs,https://api.github.com/users/ReachInfi/repos,https://api.github.com/users/ReachInfi/events{/privacy},https://api.github.com/users/ReachInfi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33314,https://github.com/apache/spark/pull/33314,https://github.com/apache/spark/pull/33314.diff,https://github.com/apache/spark/pull/33314.patch,https://api.github.com/repos/apache/spark/issues/33314/reactions,0,0,0,0,0,0,0,0,0
143,https://api.github.com/repos/apache/spark/issues/33298,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33298/labels{/name},https://api.github.com/repos/apache/spark/issues/33298/comments,https://api.github.com/repos/apache/spark/issues/33298/events,https://github.com/apache/spark/pull/33298,941858228,MDExOlB1bGxSZXF1ZXN0Njg3NjY5MDIw,33298,[SPARK-36087][SQL][WIP] An Impl of skew key detection and data inflation optimization,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-07-12T09:30:07Z,2021-07-12T14:23:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
1, introduce `ShuffleExecAccumulator` in `ShuffleExchangeExec` to support arbitrary statistics;

2, impl a key sampling `ShuffleExecAccumulator` to detect skew keys and show debug info on SparkUI;

3, in `OptimizeSkewedJoin`, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.


### Why are the changes needed?
1, make it easy to add a new statistics which can be used in AQE rules;
2, showing skew info on sparkUI is usefully;
3, spliting partitions based on joined size can resolve data inflation;


### Does this PR introduce _any_ user-facing change?
Yes, new features are added


### How was this patch tested?
added testsuites
",https://api.github.com/repos/apache/spark/issues/33298/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33298,https://github.com/apache/spark/pull/33298,https://github.com/apache/spark/pull/33298.diff,https://github.com/apache/spark/pull/33298.patch,https://api.github.com/repos/apache/spark/issues/33298/reactions,1,1,0,0,0,0,0,0,0
144,https://api.github.com/repos/apache/spark/issues/33289,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33289/labels{/name},https://api.github.com/repos/apache/spark/issues/33289/comments,https://api.github.com/repos/apache/spark/issues/33289/events,https://github.com/apache/spark/pull/33289,941389373,MDExOlB1bGxSZXF1ZXN0Njg3Mjg3MTk2,33289,[SPARK-36082][SQL] When the right side is small enough to use SingleColumn Null Aware Anti Join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-07-11T07:06:35Z,2021-07-12T09:16:26Z,,NONE,,"### What changes were proposed in this pull request?
NULL-aware ANTI join (https://issues.apache.org/jira/browse/SPARK-32290) will build right side into a HashMap.
code in SparkStrategy:
`case j @ ExtractSingleColumnNullAwareAntiJoin(leftKeys, rightKeys) =>
  Seq(joins.BroadcastHashJoinExec(leftKeys, rightKeys, LeftAnti, BuildRight,
    None, planLater(j.left), planLater(j.right), isNullAwareAntiJoin = true))`

we should add the conditions and use this optimization when the size of the right side is small enough.

### Why are the changes needed?
better stability


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
added Test
",https://api.github.com/repos/apache/spark/issues/33289/timeline,,spark,apache,mcdull-zhang,63445864,MDQ6VXNlcjYzNDQ1ODY0,https://avatars.githubusercontent.com/u/63445864?v=4,,https://api.github.com/users/mcdull-zhang,https://github.com/mcdull-zhang,https://api.github.com/users/mcdull-zhang/followers,https://api.github.com/users/mcdull-zhang/following{/other_user},https://api.github.com/users/mcdull-zhang/gists{/gist_id},https://api.github.com/users/mcdull-zhang/starred{/owner}{/repo},https://api.github.com/users/mcdull-zhang/subscriptions,https://api.github.com/users/mcdull-zhang/orgs,https://api.github.com/users/mcdull-zhang/repos,https://api.github.com/users/mcdull-zhang/events{/privacy},https://api.github.com/users/mcdull-zhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33289,https://github.com/apache/spark/pull/33289,https://github.com/apache/spark/pull/33289.diff,https://github.com/apache/spark/pull/33289.patch,https://api.github.com/repos/apache/spark/issues/33289/reactions,0,0,0,0,0,0,0,0,0
145,https://api.github.com/repos/apache/spark/issues/33288,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33288/labels{/name},https://api.github.com/repos/apache/spark/issues/33288/comments,https://api.github.com/repos/apache/spark/issues/33288/events,https://github.com/apache/spark/pull/33288,941245443,MDExOlB1bGxSZXF1ZXN0Njg3MTc5OTE5,33288,[SPARK-36080][SQL] Broadcast join the outer join stream side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-07-10T12:58:13Z,2021-08-19T16:50:19Z,,MEMBER,,"### What changes were proposed in this pull request?

Add a new optimize rule(`BroadcastJoinOuterJoinStreamSide`) to add a BHJ ahead of SMJ for left outer/semi/anti join if stream side can build broadcast and it is much smaller that other side. A real case from our cluster:
```sql
SELECT *
FROM t1 c
LEFT JOIN t2 b ON substring(b.extrnl_rfrnc_key,(instr(b.extrnl_rfrnc_key,'!')+1),char_length(b.extrnl_rfrnc_key))=c.exec_rsrc_ref_id
WHERE c.prcsr_trxn_id = 3415882487483039;
```

Before this PR | After this PR
-- | --
![image](https://user-images.githubusercontent.com/5399861/125163745-22941580-e1c1-11eb-8ea6-8c1b17102220.png) | ![image](https://user-images.githubusercontent.com/5399861/125164033-b3b7bc00-e1c2-11eb-8d84-c968dc98d501.png)


How to disable this rule:
```sql
set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.BroadcastJoinOuterJoinStreamSide;
```

### Why are the changes needed?

Improve query performance if left outer/semi/anti join if it's left side is very small and right side is very large.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test.



",https://api.github.com/repos/apache/spark/issues/33288/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33288,https://github.com/apache/spark/pull/33288,https://github.com/apache/spark/pull/33288.diff,https://github.com/apache/spark/pull/33288.patch,https://api.github.com/repos/apache/spark/issues/33288/reactions,2,2,0,0,0,0,0,0,0
146,https://api.github.com/repos/apache/spark/issues/33281,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33281/labels{/name},https://api.github.com/repos/apache/spark/issues/33281/comments,https://api.github.com/repos/apache/spark/issues/33281/events,https://github.com/apache/spark/pull/33281,940713697,MDExOlB1bGxSZXF1ZXN0Njg2NzQ1MjA3,33281,[SPARK-36073][SQL] EquivalentExpressions fixes and improvements,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-07-09T12:18:20Z,2021-07-28T08:40:13Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

This PR:
- Fixes the performance issue mentioned in https://github.com/apache/spark/pull/32559/files#r633488455 and partially fixed in https://github.com/apache/spark/pull/33142/files#r660897248 using a new option to remove expressions from equivalence maps.
- Fixes a bug with identifying common expressions in conditional expressions (a side effect of the above new approach). After this PR, `add` will be common subexpression in the following example:
  ```
  val ifExpr1 = If(Literal(true), add, Literal(3))
  val ifExpr3 = If(GreaterThan(add, Literal(4)), Add(ifExpr1, add), Multiply(ifExpr1, add))
  var equivalence = new EquivalentExpressions
  equivalence.addExprTree(ifExpr3)
  ```
- Fixes a bug of transparently canonicalized expressions (like `PromotePrecision`) are considered common subexpressions.
  After this PR, `transparent` will not be common subexpression in the following example:
  ```
  val add = Add(Literal(1), Literal(2))
  val transparent = PromotePrecision(add)
  var equivalence = new EquivalentExpressions
  equivalence.addExprTree(transparent)
  ```

### Why are the changes needed?
Bugfix + performance improvement.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Existing + new UTs.

",https://api.github.com/repos/apache/spark/issues/33281/timeline,,spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33281,https://github.com/apache/spark/pull/33281,https://github.com/apache/spark/pull/33281.diff,https://github.com/apache/spark/pull/33281.patch,https://api.github.com/repos/apache/spark/issues/33281/reactions,0,0,0,0,0,0,0,0,0
147,https://api.github.com/repos/apache/spark/issues/33257,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33257/labels{/name},https://api.github.com/repos/apache/spark/issues/33257/comments,https://api.github.com/repos/apache/spark/issues/33257/events,https://github.com/apache/spark/pull/33257,939520094,MDExOlB1bGxSZXF1ZXN0Njg1NzI3NzQ3,33257,[SPARK-36039][K8S] Fix executor pod hadoop conf mount,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-07-08T06:33:34Z,2021-09-14T10:53:06Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
Fix executor pod hadoop conf mount.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Arg --conf spark.kubernetes.hadoop.configMapName for executor pod not working.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No.
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
UT.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->",https://api.github.com/repos/apache/spark/issues/33257/timeline,,spark,apache,cutiechi,27606234,MDQ6VXNlcjI3NjA2MjM0,https://avatars.githubusercontent.com/u/27606234?v=4,,https://api.github.com/users/cutiechi,https://github.com/cutiechi,https://api.github.com/users/cutiechi/followers,https://api.github.com/users/cutiechi/following{/other_user},https://api.github.com/users/cutiechi/gists{/gist_id},https://api.github.com/users/cutiechi/starred{/owner}{/repo},https://api.github.com/users/cutiechi/subscriptions,https://api.github.com/users/cutiechi/orgs,https://api.github.com/users/cutiechi/repos,https://api.github.com/users/cutiechi/events{/privacy},https://api.github.com/users/cutiechi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33257,https://github.com/apache/spark/pull/33257,https://github.com/apache/spark/pull/33257.diff,https://github.com/apache/spark/pull/33257.patch,https://api.github.com/repos/apache/spark/issues/33257/reactions,1,1,0,0,0,0,0,0,0
148,https://api.github.com/repos/apache/spark/issues/33232,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33232/labels{/name},https://api.github.com/repos/apache/spark/issues/33232/comments,https://api.github.com/repos/apache/spark/issues/33232/events,https://github.com/apache/spark/pull/33232,937995089,MDExOlB1bGxSZXF1ZXN0Njg0NDU4NTQz,33232,[SPARK-36027][SQL] Add the code change to pushdown filter in case of typedFilter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-07-06T15:05:17Z,2021-07-08T05:12:30Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
In case of Filter having child as TypedFilter, Pushdown of Filters does not take place

scala> def testUdfFunction(r: String): Boolean = {
 | r.equals(""hello"")
 | }
testUdfFunction: (r: String)Boolean

val df= spark.read.parquet(""/testDir/testParquetSize/Parquetgzip/"")
df: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 1 more field]

Before Fix 
```
df.filter(x => testUdfFunction(x.getAs(""_1""))).filter(""_2<='id103855'"").queryExecution.executedPlan
 
Filter (isnotnull(_2#1) AND (_2#1 <= id103855))
+- *(1) Filter $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3184/1455948476@5ce4af92.apply
 +- *(1) ColumnarToRow
 +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string,_3:string>
 
df.filter(x => testUdfFunction(x.getAs(""_1""))).filter(""_2<='id103855'"").queryExecution.optimizedPlan

Filter (isnotnull(_2#1) AND (_2#1 <= id103855))
+- TypedFilter $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3191/320569017@37a2806c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))
 +- Relation[_1#0,_2#1,_3#2] parquet

```

After fix
```
df.filter(x => testUdfFunction(x.getAs(""_1""))).filter(""_2<='id103855'"").queryExecution.executedPlan

*(1) Filter $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2966/426572525@4155e85d.apply
+- *(1) Filter (isnotnull(_2#1) AND (_2#1 <= id103855))
   +- *(1) ColumnarToRow
      +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [isnotnull(_2#1), (_2#1 <= id103855)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [IsNotNull(_2), LessThanOrEqual(_2,id103855)], ReadSchema: struct<_1:string,_2:string,_3:string>

df.filter(x => testUdfFunction(x.getAs(""_1""))).filter(""_2<='id103855'"").queryExecution.optimizedPlan

TypedFilter $line18.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3445/1423649465@62065a8c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))
+- Filter (isnotnull(_2#1) AND (_2#1 <= id103855))
   +- Relation [_1#0,_2#1,_3#2] parquet
```

### Why are the changes needed?
Till now when Filter is having child as TypedFilter, the filter is not pushed down, There is a need to add this code change for pushing down the filter.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Added the unit test and tested on spark-shell",https://api.github.com/repos/apache/spark/issues/33232/timeline,,spark,apache,SaurabhChawla100,34540906,MDQ6VXNlcjM0NTQwOTA2,https://avatars.githubusercontent.com/u/34540906?v=4,,https://api.github.com/users/SaurabhChawla100,https://github.com/SaurabhChawla100,https://api.github.com/users/SaurabhChawla100/followers,https://api.github.com/users/SaurabhChawla100/following{/other_user},https://api.github.com/users/SaurabhChawla100/gists{/gist_id},https://api.github.com/users/SaurabhChawla100/starred{/owner}{/repo},https://api.github.com/users/SaurabhChawla100/subscriptions,https://api.github.com/users/SaurabhChawla100/orgs,https://api.github.com/users/SaurabhChawla100/repos,https://api.github.com/users/SaurabhChawla100/events{/privacy},https://api.github.com/users/SaurabhChawla100/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33232,https://github.com/apache/spark/pull/33232,https://github.com/apache/spark/pull/33232.diff,https://github.com/apache/spark/pull/33232.patch,https://api.github.com/repos/apache/spark/issues/33232/reactions,0,0,0,0,0,0,0,0,0
149,https://api.github.com/repos/apache/spark/issues/33201,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33201/labels{/name},https://api.github.com/repos/apache/spark/issues/33201/comments,https://api.github.com/repos/apache/spark/issues/33201/events,https://github.com/apache/spark/pull/33201,936198832,MDExOlB1bGxSZXF1ZXN0NjgyOTg1ODI1,33201,[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-07-03T07:49:24Z,2021-08-05T14:20:27Z,,NONE,,"
### What changes were proposed in this pull request?


The canCast method of type of char/varchar is modified to be consistent with StringType


the method cast will change the type char/varchar to StringType 

  def cast(to: DataType): Column = withExpr {
    val cast = Cast(expr, CharVarcharUtils.replaceCharVarcharWithStringForCast(to))
    cast.setTagValue(Cast.USER_SPECIFIED_CAST, true)
    cast
  }


The canCast method of type of char/varchar  must   be consistent with StringType




### Why are the changes needed?

Before I used stringType instead of char/varchar, my application code has the logic to judge using canCast. There was no problem before, but now its changed to char/varchar, and the judgment of canCast fails. If it doesnt pass, I Need to change a lot of application code



### Does this PR introduce _any_ user-facing change?

no


### How was this patch tested?

i add UT
",https://api.github.com/repos/apache/spark/issues/33201/timeline,,spark,apache,zheniantoushipashi,3105102,MDQ6VXNlcjMxMDUxMDI=,https://avatars.githubusercontent.com/u/3105102?v=4,,https://api.github.com/users/zheniantoushipashi,https://github.com/zheniantoushipashi,https://api.github.com/users/zheniantoushipashi/followers,https://api.github.com/users/zheniantoushipashi/following{/other_user},https://api.github.com/users/zheniantoushipashi/gists{/gist_id},https://api.github.com/users/zheniantoushipashi/starred{/owner}{/repo},https://api.github.com/users/zheniantoushipashi/subscriptions,https://api.github.com/users/zheniantoushipashi/orgs,https://api.github.com/users/zheniantoushipashi/repos,https://api.github.com/users/zheniantoushipashi/events{/privacy},https://api.github.com/users/zheniantoushipashi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33201,https://github.com/apache/spark/pull/33201,https://github.com/apache/spark/pull/33201.diff,https://github.com/apache/spark/pull/33201.patch,https://api.github.com/repos/apache/spark/issues/33201/reactions,0,0,0,0,0,0,0,0,0
150,https://api.github.com/repos/apache/spark/issues/33175,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33175/labels{/name},https://api.github.com/repos/apache/spark/issues/33175/comments,https://api.github.com/repos/apache/spark/issues/33175/events,https://github.com/apache/spark/pull/33175,934864854,MDExOlB1bGxSZXF1ZXN0NjgxODc1MTMx,33175,[SPARK-35973][SQL] DataSourceV2: Support SHOW CATALOGS,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2021-07-01T13:46:33Z,2021-08-25T13:49:08Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Datasource V2 can support multiple catalogs. Having ""SHOW CATALOGS"" to list the catalogs name will be useful.


### Why are the changes needed?
[SPARK-35973](https://issues.apache.org/jira/browse/SPARK-35973)


### Does this PR introduce _any_ user-facing change?
Yes, we can use `SHOW CATALOGS` command to list catalogs info. eg:
```
+-------------+
|      catalog|
+-------------+
|spark_catalog|
|      testcat|
+-------------+
```

### How was this patch tested?
Add ut test
",https://api.github.com/repos/apache/spark/issues/33175/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33175,https://github.com/apache/spark/pull/33175,https://github.com/apache/spark/pull/33175.diff,https://github.com/apache/spark/pull/33175.patch,https://api.github.com/repos/apache/spark/issues/33175/reactions,0,0,0,0,0,0,0,0,0
151,https://api.github.com/repos/apache/spark/issues/33174,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33174/labels{/name},https://api.github.com/repos/apache/spark/issues/33174/comments,https://api.github.com/repos/apache/spark/issues/33174/events,https://github.com/apache/spark/pull/33174,934604265,MDExOlB1bGxSZXF1ZXN0NjgxNjQ4Njcw,33174,[SPARK-35721][PYTHON] Path level discover for python unittests,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-07-01T09:06:21Z,2021-09-29T18:50:08Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Add path level discover for python unittests.
![image](https://user-images.githubusercontent.com/1736354/124094503-6bdeb980-da8b-11eb-9bbe-b086024f6902.png)

Change list:
- Introduce a **python_discover_paths** in modules.
- Add **_discover_python_unittests** function: it would be called in pthon/run-tests.py to load test module.
- Add **_append_discovred_goals function**: call _discover_python_unittests to refresh m.python_test_goals
- if modules have python_test_goals or **python_discover_paths** would also be considered as python tests.
- Fix: Move logging.basicConfig to head to make sure logging config before any possible logging print.
- Fix: Change python/pyspark/testing/utils.py SPARK_HOME use _find_spark_home to get value.
- Fix: export py4j PYTHONPATH before run test.

Note:
- **Why use walk_packages but not unittest.defaultTestLoader.discover?** we use `pkgutil.walk_packages` and `unittest.defaultTestLoader.loadTestsFromModule` to load test modules, consider we will add doctest discover in future, we can add something like blow as the impletations of doctest discover: 
```python
import doctest

def _contain_doctests_class(module):
    suite = doctest.DocTestSuite(module)
    if suite.countTestCases():
        return True
    else:
        return False
```
- **Why we doesn't add doctests in here**? Currently, not all modules doctests are added to `python_test_goals`, that means these doctests doesn't be excuted, so better add discover doctests in a separate PR.

- **What's the deps of discover?** the test discover will do real import for every modules, so we need install **all deps of PySpark test modules** before run-tests otherwise the ImportError would be raised.



### Why are the changes needed?
Now we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.

Such as:

pyspark-core pyspark.tests.test_pin_thread

Thus we need some auto-discover way to find all testcase rather than specified every case by manually.

related: https://github.com/apache/spark/pull/32867

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
1. Add doc tests for _discover_python_unittests.
2. Compare the CI results (this patch and before), see diff in:
Build modules: pyspark-sql, pyspark-mllib, pyspark-resource: https://www.diffchecker.com/4RAQydBB
Build modules: pyspark-core, pyspark-streaming, pyspark-ml: https://www.diffchecker.com/F1ccZDKG
Build modules: pyspark-pandashttps://www.diffchecker.com/eBDne4uA
Build modules: pyspark-pandas-slowhttps://www.diffchecker.com/lySQGrhA
3. local test for python modules:
./dev/run-tests --parallelism 2 --modules ""pyspark-sql""",https://api.github.com/repos/apache/spark/issues/33174/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33174,https://github.com/apache/spark/pull/33174,https://github.com/apache/spark/pull/33174.diff,https://github.com/apache/spark/pull/33174.patch,https://api.github.com/repos/apache/spark/issues/33174/reactions,0,0,0,0,0,0,0,0,0
152,https://api.github.com/repos/apache/spark/issues/33161,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33161/labels{/name},https://api.github.com/repos/apache/spark/issues/33161/comments,https://api.github.com/repos/apache/spark/issues/33161/events,https://github.com/apache/spark/pull/33161,934236621,MDExOlB1bGxSZXF1ZXN0NjgxMzI4ODcz,33161,[WIP][SPARK-31973][SQL] Skip partial aggregates in run-time if reduction ratio is low,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-07-01T00:34:28Z,2021-07-01T01:21:33Z,,NONE,,"### What changes were proposed in this pull request?

This PR builds on top of https://github.com/apache/spark/pull/28804. In addition to the other PR, one other change is that the partial aggregation hashmap is freed as soon as partial aggregation is disabled to prevent off-heap OOMs.


### Why are the changes needed?

This change can help improve query performance.


### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Existing and additional unit tests.

",https://api.github.com/repos/apache/spark/issues/33161/timeline,,spark,apache,shipra-a,85585890,MDQ6VXNlcjg1NTg1ODkw,https://avatars.githubusercontent.com/u/85585890?v=4,,https://api.github.com/users/shipra-a,https://github.com/shipra-a,https://api.github.com/users/shipra-a/followers,https://api.github.com/users/shipra-a/following{/other_user},https://api.github.com/users/shipra-a/gists{/gist_id},https://api.github.com/users/shipra-a/starred{/owner}{/repo},https://api.github.com/users/shipra-a/subscriptions,https://api.github.com/users/shipra-a/orgs,https://api.github.com/users/shipra-a/repos,https://api.github.com/users/shipra-a/events{/privacy},https://api.github.com/users/shipra-a/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33161,https://github.com/apache/spark/pull/33161,https://github.com/apache/spark/pull/33161.diff,https://github.com/apache/spark/pull/33161.patch,https://api.github.com/repos/apache/spark/issues/33161/reactions,0,0,0,0,0,0,0,0,0
153,https://api.github.com/repos/apache/spark/issues/33160,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33160/labels{/name},https://api.github.com/repos/apache/spark/issues/33160/comments,https://api.github.com/repos/apache/spark/issues/33160/events,https://github.com/apache/spark/pull/33160,934203946,MDExOlB1bGxSZXF1ZXN0NjgxMzAxMzE4,33160,"[SPARK-35959][BUILD][test-maven][test-hadoop3.2][test-java11] Add a new Maven profile ""no-shaded-hadoop-client"" for Hadoop versions older than 3.2.2/3.3.1","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,47,2021-06-30T23:33:10Z,2021-09-09T18:33:32Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Add a new Maven profile `no-shaded-hadoop-client` that, when activated, switches to non-shaded Hadoop client (e.g., `hadoop-client`, `hadoop-yarn-client`, etc). 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Currently Spark uses Hadoop shaded client by default. However, if Spark users want to build Spark with older version of Hadoop, such as 3.1.x, the shaded client cannot be used as it currently it only support Hadoop 3.2.2+ and 3.3.1+). Therefore, this proposes to offer a new Maven profile ""no-shaded-hadoop-client"" for this use case.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes, now users can choose to build Apache Spark with non-shaded Hadoop client, e.g.:
```shell
build/mvn package -DskipTests -Dhadoop.version=3.1.1 -Pno-shaded-hadoop-client
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/33160/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33160,https://github.com/apache/spark/pull/33160,https://github.com/apache/spark/pull/33160.diff,https://github.com/apache/spark/pull/33160.patch,https://api.github.com/repos/apache/spark/issues/33160/reactions,0,0,0,0,0,0,0,0,0
154,https://api.github.com/repos/apache/spark/issues/33154,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33154/labels{/name},https://api.github.com/repos/apache/spark/issues/33154/comments,https://api.github.com/repos/apache/spark/issues/33154/events,https://github.com/apache/spark/pull/33154,933481546,MDExOlB1bGxSZXF1ZXN0NjgwNjgwNTY2,33154,[SPARK-35949][CORE]Add `keep-spark-context-alive` arg for to prevent closing spark context after invoking main for some case,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-06-30T09:17:37Z,2021-07-16T02:42:21Z,,NONE,,"## SPARK-35949

## What changes were proposed in this pull request?
From v3.1, spark context will close after invoking main method. In some case, it is necessary to keep spark context alive. Such as start app as a server. I add `keep-saprk-context-alive` arg to set whether should keep spark context alive after main method.

## Why are the changes needed?
Due to pr c625eb4#diff-f8564df81d845c0cd2f621bc2ed22761cbf9731f28cb2828d9cbd0491f4e7584. In client mode, the spark context will be stopped on application start. it is necessary to keep spark context alive. Such as start app as a server

## Does this PR introduce any user-facing change?
Yes. Added a `keep-saprk-context-alive` args to set keeping the spark context alive until the app exit. Usage `spark-submit --keep-saprk-context-alive true`

## How was this patch tested?
Manually test.",https://api.github.com/repos/apache/spark/issues/33154/timeline,,spark,apache,sunpe,8380506,MDQ6VXNlcjgzODA1MDY=,https://avatars.githubusercontent.com/u/8380506?v=4,,https://api.github.com/users/sunpe,https://github.com/sunpe,https://api.github.com/users/sunpe/followers,https://api.github.com/users/sunpe/following{/other_user},https://api.github.com/users/sunpe/gists{/gist_id},https://api.github.com/users/sunpe/starred{/owner}{/repo},https://api.github.com/users/sunpe/subscriptions,https://api.github.com/users/sunpe/orgs,https://api.github.com/users/sunpe/repos,https://api.github.com/users/sunpe/events{/privacy},https://api.github.com/users/sunpe/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33154,https://github.com/apache/spark/pull/33154,https://github.com/apache/spark/pull/33154.diff,https://github.com/apache/spark/pull/33154.patch,https://api.github.com/repos/apache/spark/issues/33154/reactions,0,0,0,0,0,0,0,0,0
155,https://api.github.com/repos/apache/spark/issues/33135,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33135/labels{/name},https://api.github.com/repos/apache/spark/issues/33135/comments,https://api.github.com/repos/apache/spark/issues/33135/events,https://github.com/apache/spark/pull/33135,932792913,MDExOlB1bGxSZXF1ZXN0NjgwMDk2MDc0,33135,[SPARK-35931][CORE][YARN] Ability to override Yarn Cluster Submit Class with Configuration,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-06-29T14:58:07Z,2021-07-07T14:25:05Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

- This PR allows to add a custom implementation of YARN_CLUSTER_SUBMIT_CLASS as a configuration.
-  This is useful when there is a custom variant of Yarn cluster running which requires a modified Yarn client.
- Users can implement a class similar to YarnClusterApplication which can invoke a custom Yarn client. Users should add the relevant dependencies in the classpath.
- Default value of the configuration is set to the current class: org.apache.spark.deploy.yarn.YarnClusterApplication.

### Why are the changes needed?
This is required when there is a custom variant of Yarn cluster running which requires a modified Yarn client. For example, there may be different handling of credentials required in a custom Yarn cluster.


### Does this PR introduce _any_ user-facing change?

Yes, this PR introduces a new configuration key but the behaviour is unchanged without explicitly modifying the config. Documentation is added for the new configuration.

### How was this patch tested?
Tested following in a local Yarn cluster: 
Tested without the config added - Existing client is used to submit application
Tested with config added - New client is used to submit application
Tested with config added but class not present - Spark-submit throws error:  Could not load YARN classes.

Unit tests:
Added unit tests when the configuration is enabled.
Existing unit tests which validate the current YarnClusterApplication class name are passing.",https://api.github.com/repos/apache/spark/issues/33135/timeline,,spark,apache,akshatb1,31816865,MDQ6VXNlcjMxODE2ODY1,https://avatars.githubusercontent.com/u/31816865?v=4,,https://api.github.com/users/akshatb1,https://github.com/akshatb1,https://api.github.com/users/akshatb1/followers,https://api.github.com/users/akshatb1/following{/other_user},https://api.github.com/users/akshatb1/gists{/gist_id},https://api.github.com/users/akshatb1/starred{/owner}{/repo},https://api.github.com/users/akshatb1/subscriptions,https://api.github.com/users/akshatb1/orgs,https://api.github.com/users/akshatb1/repos,https://api.github.com/users/akshatb1/events{/privacy},https://api.github.com/users/akshatb1/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33135,https://github.com/apache/spark/pull/33135,https://github.com/apache/spark/pull/33135.diff,https://github.com/apache/spark/pull/33135.patch,https://api.github.com/repos/apache/spark/issues/33135/reactions,0,0,0,0,0,0,0,0,0
156,https://api.github.com/repos/apache/spark/issues/33132,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33132/labels{/name},https://api.github.com/repos/apache/spark/issues/33132/comments,https://api.github.com/repos/apache/spark/issues/33132/events,https://github.com/apache/spark/pull/33132,932535288,MDExOlB1bGxSZXF1ZXN0Njc5ODY1Nzc0,33132,[SPARK-35926][SQL] Add support YearMonthIntervalType for width_bucket,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-06-29T11:38:04Z,2021-07-06T08:04:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
1. The function `width_bucket` is introduced from [SPARK-21117](https://issues.apache.org/jira/browse/SPARK-21117)
2. The YearMonthIntervalType is just store the value with Int.
3. Modify the `inputType` to allow the width_bucket to support the YearMonthIntervalType

### Why are the changes needed?
[35926](https://issues.apache.org/jira/browse/SPARK-35926)

### Does this PR introduce _any_ user-facing change?
Yes. The user can use `width_bucket` with Period type.


### How was this patch tested?
Add ut test
",https://api.github.com/repos/apache/spark/issues/33132/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33132,https://github.com/apache/spark/pull/33132,https://github.com/apache/spark/pull/33132.diff,https://github.com/apache/spark/pull/33132.patch,https://api.github.com/repos/apache/spark/issues/33132/reactions,0,0,0,0,0,0,0,0,0
157,https://api.github.com/repos/apache/spark/issues/33114,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33114/labels{/name},https://api.github.com/repos/apache/spark/issues/33114/comments,https://api.github.com/repos/apache/spark/issues/33114/events,https://github.com/apache/spark/pull/33114,931170968,MDExOlB1bGxSZXF1ZXN0Njc4NzA1NjM4,33114,[SPARK-35913][SQL] Create hive permanent function with owner name,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-06-28T05:00:56Z,2021-08-03T09:05:47Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Create hive permanent function with owner name


### Why are the changes needed?
The hive permanent function created by spark does not have an owner name, while the permanent function created by hive has an owner name


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
manual test
",https://api.github.com/repos/apache/spark/issues/33114/timeline,,spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33114,https://github.com/apache/spark/pull/33114,https://github.com/apache/spark/pull/33114.diff,https://github.com/apache/spark/pull/33114.patch,https://api.github.com/repos/apache/spark/issues/33114/reactions,0,0,0,0,0,0,0,0,0
158,https://api.github.com/repos/apache/spark/issues/33110,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33110/labels{/name},https://api.github.com/repos/apache/spark/issues/33110/comments,https://api.github.com/repos/apache/spark/issues/33110/events,https://github.com/apache/spark/pull/33110,930995498,MDExOlB1bGxSZXF1ZXN0Njc4NTY0MDMy,33110,[SPARK-35911][SQL] Update exprId for IN subquery in DPP,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-06-27T18:52:00Z,2021-07-12T04:40:00Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
Update exprId for IN subquery for DPP in executed plan; to have same expr Id as DynamicPruning filter in optimized plan.


### Why are the changes needed?
This minor change shall make debugging easier in complex queries.


### Does this PR introduce _any_ user-facing change?
Yes, just exprId changes in the optimized and executed plans. Now both shall have same exprId for an DPP expression/subquery.


### How was this patch tested?
Added a check in existing UTs.
",https://api.github.com/repos/apache/spark/issues/33110/timeline,,spark,apache,Swinky,5418286,MDQ6VXNlcjU0MTgyODY=,https://avatars.githubusercontent.com/u/5418286?v=4,,https://api.github.com/users/Swinky,https://github.com/Swinky,https://api.github.com/users/Swinky/followers,https://api.github.com/users/Swinky/following{/other_user},https://api.github.com/users/Swinky/gists{/gist_id},https://api.github.com/users/Swinky/starred{/owner}{/repo},https://api.github.com/users/Swinky/subscriptions,https://api.github.com/users/Swinky/orgs,https://api.github.com/users/Swinky/repos,https://api.github.com/users/Swinky/events{/privacy},https://api.github.com/users/Swinky/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33110,https://github.com/apache/spark/pull/33110,https://github.com/apache/spark/pull/33110.diff,https://github.com/apache/spark/pull/33110.patch,https://api.github.com/repos/apache/spark/issues/33110/reactions,0,0,0,0,0,0,0,0,0
159,https://api.github.com/repos/apache/spark/issues/33104,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33104/labels{/name},https://api.github.com/repos/apache/spark/issues/33104/comments,https://api.github.com/repos/apache/spark/issues/33104/events,https://github.com/apache/spark/pull/33104,930816069,MDExOlB1bGxSZXF1ZXN0Njc4NDMwNjM0,33104,[SPARK-35902][Core] spark.driver.log.dfsDir with hdfs scheme failed,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-06-26T23:59:11Z,2021-06-30T01:56:18Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
when persist driver logs in client mode to dfs, log dir should support scheme path


### Why are the changes needed?
when spark.driver.log.dfsDir start with scheme like hdfsit failed  


### Does this PR introduce _any_ user-facing change?
no, user do not need change any code


### How was this patch tested?
tested by existing UT
",https://api.github.com/repos/apache/spark/issues/33104/timeline,,spark,apache,fhygh,25889738,MDQ6VXNlcjI1ODg5NzM4,https://avatars.githubusercontent.com/u/25889738?v=4,,https://api.github.com/users/fhygh,https://github.com/fhygh,https://api.github.com/users/fhygh/followers,https://api.github.com/users/fhygh/following{/other_user},https://api.github.com/users/fhygh/gists{/gist_id},https://api.github.com/users/fhygh/starred{/owner}{/repo},https://api.github.com/users/fhygh/subscriptions,https://api.github.com/users/fhygh/orgs,https://api.github.com/users/fhygh/repos,https://api.github.com/users/fhygh/events{/privacy},https://api.github.com/users/fhygh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33104,https://github.com/apache/spark/pull/33104,https://github.com/apache/spark/pull/33104.diff,https://github.com/apache/spark/pull/33104.patch,https://api.github.com/repos/apache/spark/issues/33104/reactions,0,0,0,0,0,0,0,0,0
160,https://api.github.com/repos/apache/spark/issues/33089,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33089/labels{/name},https://api.github.com/repos/apache/spark/issues/33089/comments,https://api.github.com/repos/apache/spark/issues/33089/events,https://github.com/apache/spark/pull/33089,930212392,MDExOlB1bGxSZXF1ZXN0Njc3OTQ1NTYy,33089,[SPARK-35892]Modify function saveTable in JdbcUtils to support config numPartitions bigger than RDD's partition number,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-06-25T14:10:22Z,2021-07-20T15:17:27Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
Modify function saveTable in JdbcUtils to support config numPartitions bigger than RDD's partition number.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
When config numPartitions bigger than RDD's partition numberthe RDD will not repartition, the configuration does not work.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Add Unit Test.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/33089/timeline,,spark,apache,zengruios,26316365,MDQ6VXNlcjI2MzE2MzY1,https://avatars.githubusercontent.com/u/26316365?v=4,,https://api.github.com/users/zengruios,https://github.com/zengruios,https://api.github.com/users/zengruios/followers,https://api.github.com/users/zengruios/following{/other_user},https://api.github.com/users/zengruios/gists{/gist_id},https://api.github.com/users/zengruios/starred{/owner}{/repo},https://api.github.com/users/zengruios/subscriptions,https://api.github.com/users/zengruios/orgs,https://api.github.com/users/zengruios/repos,https://api.github.com/users/zengruios/events{/privacy},https://api.github.com/users/zengruios/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33089,https://github.com/apache/spark/pull/33089,https://github.com/apache/spark/pull/33089.diff,https://github.com/apache/spark/pull/33089.patch,https://api.github.com/repos/apache/spark/issues/33089/reactions,0,0,0,0,0,0,0,0,0
161,https://api.github.com/repos/apache/spark/issues/33083,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33083/labels{/name},https://api.github.com/repos/apache/spark/issues/33083/comments,https://api.github.com/repos/apache/spark/issues/33083/events,https://github.com/apache/spark/pull/33083,930081899,MDExOlB1bGxSZXF1ZXN0Njc3ODM1NTc4,33083,Allow sequences (tuples and lists) as pivot values argument in PySpark.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-06-25T11:16:13Z,2021-08-13T09:36:27Z,,NONE,,"Both tuples and lists are accepted by PySpark on runtime.
",https://api.github.com/repos/apache/spark/issues/33083/timeline,,spark,apache,wrobell,105664,MDQ6VXNlcjEwNTY2NA==,https://avatars.githubusercontent.com/u/105664?v=4,,https://api.github.com/users/wrobell,https://github.com/wrobell,https://api.github.com/users/wrobell/followers,https://api.github.com/users/wrobell/following{/other_user},https://api.github.com/users/wrobell/gists{/gist_id},https://api.github.com/users/wrobell/starred{/owner}{/repo},https://api.github.com/users/wrobell/subscriptions,https://api.github.com/users/wrobell/orgs,https://api.github.com/users/wrobell/repos,https://api.github.com/users/wrobell/events{/privacy},https://api.github.com/users/wrobell/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33083,https://github.com/apache/spark/pull/33083,https://github.com/apache/spark/pull/33083.diff,https://github.com/apache/spark/pull/33083.patch,https://api.github.com/repos/apache/spark/issues/33083/reactions,0,0,0,0,0,0,0,0,0
162,https://api.github.com/repos/apache/spark/issues/33008,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33008/labels{/name},https://api.github.com/repos/apache/spark/issues/33008/comments,https://api.github.com/repos/apache/spark/issues/33008/events,https://github.com/apache/spark/pull/33008,926741465,MDExOlB1bGxSZXF1ZXN0Njc1MDA0NTAx,33008,[WIP][SPARK-35801][SQL] Support DELETE operations that require rewriting data,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-06-22T01:40:22Z,2021-06-30T19:41:18Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This WIP PR shows how we can use the proposed API in SPARK-35801 (per [design doc](https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60)) to support DELETE statements that require rewriting data.

**Note**: This PR must be split into a number of smaller PRs if we decide to adopt this approach. All changes are grouped here only to simplify the review process and support the design doc.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

These changes are required so that Spark can provide support for DELETE, UPDATE, MERGE statements.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes, this PR introduces a set of new APIs for Data Source V2.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

This PR comes with a trivial test. More tests to come.
",https://api.github.com/repos/apache/spark/issues/33008/timeline,,spark,apache,aokolnychyi,6235869,MDQ6VXNlcjYyMzU4Njk=,https://avatars.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33008,https://github.com/apache/spark/pull/33008,https://github.com/apache/spark/pull/33008.diff,https://github.com/apache/spark/pull/33008.patch,https://api.github.com/repos/apache/spark/issues/33008/reactions,10,0,0,0,10,0,0,0,0
163,https://api.github.com/repos/apache/spark/issues/32987,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32987/labels{/name},https://api.github.com/repos/apache/spark/issues/32987/comments,https://api.github.com/repos/apache/spark/issues/32987/events,https://github.com/apache/spark/pull/32987,925601145,MDExOlB1bGxSZXF1ZXN0Njc0MDM2MzE5,32987,[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,27,2021-06-20T13:35:30Z,2021-09-29T17:33:09Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
I am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:

```
transformed = F.regexp_replace(F.lower(F.trim('my_column')))
df.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))
```
or
```
df.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))
```

In these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.

In practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.

The only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.

I also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To increase the performance of conditional expressions.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, just performance improvements.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New and updated UT.
",https://api.github.com/repos/apache/spark/issues/32987/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32987,https://github.com/apache/spark/pull/32987,https://github.com/apache/spark/pull/32987.diff,https://github.com/apache/spark/pull/32987.patch,https://api.github.com/repos/apache/spark/issues/32987/reactions,0,0,0,0,0,0,0,0,0
164,https://api.github.com/repos/apache/spark/issues/32979,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32979/labels{/name},https://api.github.com/repos/apache/spark/issues/32979/comments,https://api.github.com/repos/apache/spark/issues/32979/events,https://github.com/apache/spark/pull/32979,925449356,MDExOlB1bGxSZXF1ZXN0NjczOTIwMjg4,32979,[SPARK-35828][K8S] Skip retrieving the non-exist driver pod for client mode,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-06-19T18:57:22Z,2021-08-04T03:52:08Z,,CONTRIBUTOR,,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

For a case like,

```scala
bin/spark-submit  \
--conf spark.kubernetes.file.upload.path=./ \
--deploy-mode client \
--master k8s://https://kubernetes.docker.internal:6443 \
--conf spark.kubernetes.container.image=yaooqinn/spark:v20210619 \
-c spark.kubernetes.context=docker-for-desktop_1 \
--conf spark.kubernetes.executor.podNamePrefix=sparksql \
--conf spark.dynamicAllocation.shuffleTracking.enabled=true \
--conf spark.dynamicAllocation.enabled=true \
--conf spark.kubernetes.driver.pod.name=abc \
--class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.12-3.2.0-SNAPSHOT.jar
```

When `spark.kubernetes.driver.pod.name` is specific, we now get the driver pod for whatever the deploy mode is, while the driver pod only exists in cluster mode. So we should skip retrieving it instead of getting the following error:

```logtalk
21/06/19 16:18:49 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: No pod was found named abc in the cluster in the namespace default (this was supposed to be the driver pod.).
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$2(ExecutorPodsAllocator.scala:81)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$1(ExecutorPodsAllocator.scala:79)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:76)
	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:118)
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2969)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:559)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2686)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:948)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:942)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

an unused config should stay inoperative instead of failing an application at runtime.

when we switch deploy modes, we do need to justify irrelevant configs.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, spark.kubernetes.driver.pod.name will cause the client mode app to fail

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

new test added
",https://api.github.com/repos/apache/spark/issues/32979/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32979,https://github.com/apache/spark/pull/32979,https://github.com/apache/spark/pull/32979.diff,https://github.com/apache/spark/pull/32979.patch,https://api.github.com/repos/apache/spark/issues/32979/reactions,0,0,0,0,0,0,0,0,0
165,https://api.github.com/repos/apache/spark/issues/32902,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32902/labels{/name},https://api.github.com/repos/apache/spark/issues/32902/comments,https://api.github.com/repos/apache/spark/issues/32902/events,https://github.com/apache/spark/pull/32902,920265479,MDExOlB1bGxSZXF1ZXN0NjY5NDQwNjI5,32902,[SPARK-35754][CORE] Add config to put migrating blocks on disk only,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-06-14T10:20:48Z,2021-07-01T19:17:17Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR adds a config which makes block manager decommissioner to migrate block data on disk only. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
While migrating block data, if enough memory is not available on peer block managers existing blocks are dropped. After this PR migrating blocks won't drop any existing blocks. 

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
UT in BlockManagerSuite",https://api.github.com/repos/apache/spark/issues/32902/timeline,,spark,apache,q2w,12875634,MDQ6VXNlcjEyODc1NjM0,https://avatars.githubusercontent.com/u/12875634?v=4,,https://api.github.com/users/q2w,https://github.com/q2w,https://api.github.com/users/q2w/followers,https://api.github.com/users/q2w/following{/other_user},https://api.github.com/users/q2w/gists{/gist_id},https://api.github.com/users/q2w/starred{/owner}{/repo},https://api.github.com/users/q2w/subscriptions,https://api.github.com/users/q2w/orgs,https://api.github.com/users/q2w/repos,https://api.github.com/users/q2w/events{/privacy},https://api.github.com/users/q2w/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32902,https://github.com/apache/spark/pull/32902,https://github.com/apache/spark/pull/32902.diff,https://github.com/apache/spark/pull/32902.patch,https://api.github.com/repos/apache/spark/issues/32902/reactions,0,0,0,0,0,0,0,0,0
166,https://api.github.com/repos/apache/spark/issues/32875,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32875/labels{/name},https://api.github.com/repos/apache/spark/issues/32875/comments,https://api.github.com/repos/apache/spark/issues/32875/events,https://github.com/apache/spark/pull/32875,918179228,MDExOlB1bGxSZXF1ZXN0NjY3NjM2MjE3,32875,[SPARK-35703] Relax constraint for bucket join and remove HashClusteredDistribution,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-06-11T03:42:49Z,2021-10-01T22:40:09Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR proposes the following:
1. Introducing a new trait `ShuffleSpec` which is used in `EnsureRequirements` when the node has more than one children and serves two purposes: 1) compare all children and check if they are compatible w.r.t partitioning & distribution, 2) create a new partitioning to re-shuffle the other side in case they are not compatible.
2. Remove `HashClusteredDistribution` and replace its usages with `ClusteredDistribution`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Spark currently only allow bucket join when the set of cluster keys from output partitioning _exactly match_ the set of join keys from the required distribution. For instance, in the following:
```sql
SELECT * FROM A JOIN B ON A.c1 = B.c1 AND A.c2 = B.c2
```
bucket join will only be triggered if both `A` and `B` are bucketed on columns `c1` and `c2`, in which case Spark will avoid shuffling both sides of the join.

The above requirement, however, is too strict, as shuffle can also be avoided if both `A` and `B` are bucketed on either column `c1` or `c2`. That is, if all rows that have the same value in column `c1` are clustered into the same partition, then all rows have the same values in column `c1` and `c2` are also clustered into the same partition. 

In order to allow this, we'll need to change the logic of deciding whether two sides of a join operator are ""co-partitioned"". Currently, this is done by checking each side's output partitioning against its required distribution separately, using `Partitioning.satisfies` method. Since `HashClusteredDistribution` requires a `HashPartitioning` to have the exact match on the cluster keys, this can be done in isolation without looking at the other side's output partitioning and required distribution.

However, the approach is no longer valid if we are going to relax the above constraint, as we need to compare the output partitioning and required distribution **on both sides**. For instance, in the above example, if `A` is bucketed on `c1` while `B` is bucketed on `c2`, we may need to do the following check:
1. identify where `A.c1` and `B.c2` is used in the join keys (e.g., position 0 and 1 respectively)
2. check if the positions derived from both sides exactly match each other (this becomes more complicated if a key appears in multiple positions within the join keys.)

This means we'll need to design some additional API to achieve the above. This propose the following:
```scala
trait ShuffleSpec {
  // Used as a cost indicator to shuffle children
  def numPartitions: Int

  // Used to check whether this spec is compatible with `other`
  def isCompatibleWith(other: ShuffleSpec): Boolean

  // Used to create a new partitioning for the other `distribution` in case `isCompatibleWith` failed.
  def createPartitioning(distribution: Distribution): Partitioning
}
```

A similar API is also required if we are going to support DSv2 `DataSourcePartitioning` as output partitioning in bucket join scenario, or support custom hash functions such as `HiveHash` for bucketing. With the former, even if both `A` and `B` are partitioned on columns `c1` and `c2` in the above example, they could be partitioned via different transform expressions, e.g., `A` is on `(bucket(32, c1), day(c2)` while `B` is on `(bucket(32, c1), hour(c2)`. This means we'll need to compare the partitioning from both sides of the join which makes the current approach with `Partitioning.satisfies` insufficient. The same API `isCompatibleWith` can potentially be reused for the purpose.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes, now bucket join will be enabled for more cases as mentioned above.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

1. Added a new test suite `ShuffleSpecSuite`
2. Added additional tests in `EnsureRequirementsSuite`.",https://api.github.com/repos/apache/spark/issues/32875/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32875,https://github.com/apache/spark/pull/32875,https://github.com/apache/spark/pull/32875.diff,https://github.com/apache/spark/pull/32875.patch,https://api.github.com/repos/apache/spark/issues/32875/reactions,0,0,0,0,0,0,0,0,0
167,https://api.github.com/repos/apache/spark/issues/32819,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32819/labels{/name},https://api.github.com/repos/apache/spark/issues/32819/comments,https://api.github.com/repos/apache/spark/issues/32819/events,https://github.com/apache/spark/pull/32819,914767827,MDExOlB1bGxSZXF1ZXN0NjY0NjU0NTEx,32819,[WIP][SPARK-35677][Core][SQL] Support dynamic range of executor numbers for dynamic allocation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-06-08T10:21:54Z,2021-09-08T11:55:05Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?

Currently, Spark allows users to set scalability within a Spark application using dynamic allocation. `spark.dynamicAllocation.minExecutors` & `spark.dynamicAllocation.maxExecutors` are used for scaling up and down. Within an applicationSpark tactfully use them to request executors from cluster manager according to the real-time workload. Once set, the range is fixed through the whole application lifecycle. This is not very convenient for long-running application when the range should be changeable for some cases, such as:
1. the cluster manager itself or the queue will scale up and down, which looks very likely to happen in modern cloud platforms
2. the application is long-running, but the timeliness, priority, e.t.c are not only determined by the workload of the application, but also by the traffic across the cluster manager or just different moments
3. e.t.c.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

make the dynamic allocation for long term Spark applications

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Configs below are changeable:

spark.dynamicAllocation.maxExecutors 
spark.dynamicAllocation.minExecutors 

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
new tests",https://api.github.com/repos/apache/spark/issues/32819/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32819,https://github.com/apache/spark/pull/32819,https://github.com/apache/spark/pull/32819.diff,https://github.com/apache/spark/pull/32819.patch,https://api.github.com/repos/apache/spark/issues/32819/reactions,0,0,0,0,0,0,0,0,0
168,https://api.github.com/repos/apache/spark/issues/32813,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32813/labels{/name},https://api.github.com/repos/apache/spark/issues/32813/comments,https://api.github.com/repos/apache/spark/issues/32813/events,https://github.com/apache/spark/pull/32813,914313249,MDExOlB1bGxSZXF1ZXN0NjY0MjQwNDkz,32813,[SPARK-34591][MLLIB][WIP] Add decision tree pruning as a parameter,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-06-08T04:30:49Z,2021-09-01T06:03:04Z,,NONE,,"### What changes were proposed in this pull request?
This PR adds a parameter to enable/disable a featuer where LearningNodes are merged after a RF model is trained.

### Why are the changes needed?
2 Reasons:

1. In addition to basic classification, another use case for decision trees are the probabilities associated with predictions. 
 Once pruned, these predictions are lost and it makes the trees/predictions challenging to work with if not unusable.

2. It is not in line with the default behavior in sklearn.  In sklearn, the trees are left unpruned by default.

Please see Jira ticket for more explanation.

### Does this PR introduce _any_ user-facing change?

Yes, it adds a parameter that is exposed to the Tree based classifiers.  Will add tests here to ensure parameter is exposed correctly.

### How was this patch tested?
I modified the two tests introduced with this change to verify postive/negative use of feature.  I also added assertions for default behavior

Will add tests that ensure user exposed API is validated.

Locally ran `./build/mvn -pl mllib package` and verified tests passed
Additionally, running through git workflow as described here:
    	https://spark.apache.org/developer-tools.html#github-workflow-tests
",https://api.github.com/repos/apache/spark/issues/32813/timeline,,spark,apache,CBribiescas,6864707,MDQ6VXNlcjY4NjQ3MDc=,https://avatars.githubusercontent.com/u/6864707?v=4,,https://api.github.com/users/CBribiescas,https://github.com/CBribiescas,https://api.github.com/users/CBribiescas/followers,https://api.github.com/users/CBribiescas/following{/other_user},https://api.github.com/users/CBribiescas/gists{/gist_id},https://api.github.com/users/CBribiescas/starred{/owner}{/repo},https://api.github.com/users/CBribiescas/subscriptions,https://api.github.com/users/CBribiescas/orgs,https://api.github.com/users/CBribiescas/repos,https://api.github.com/users/CBribiescas/events{/privacy},https://api.github.com/users/CBribiescas/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32813,https://github.com/apache/spark/pull/32813,https://github.com/apache/spark/pull/32813.diff,https://github.com/apache/spark/pull/32813.patch,https://api.github.com/repos/apache/spark/issues/32813/reactions,0,0,0,0,0,0,0,0,0
169,https://api.github.com/repos/apache/spark/issues/32801,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32801/labels{/name},https://api.github.com/repos/apache/spark/issues/32801/comments,https://api.github.com/repos/apache/spark/issues/32801/events,https://github.com/apache/spark/pull/32801,913150539,MDExOlB1bGxSZXF1ZXN0NjYzMjIwNzM1,32801,[SPARK-12567][SQL] Add aes_encrypt and aes_decrypt builtin functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-06-07T06:23:11Z,2021-06-28T01:46:56Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR adds AES encryption/decryption builtin functions for SQL called `aes_encrypt` and `aes_decrypt`.
MySQL, Oracle Database and Hive implement this feature.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Like hash functions, encryption/decryption is common for data processing.
Hive also implements these functions.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. New functions will be available in SQL.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New tests.",https://api.github.com/repos/apache/spark/issues/32801/timeline,,spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32801,https://github.com/apache/spark/pull/32801,https://github.com/apache/spark/pull/32801.diff,https://github.com/apache/spark/pull/32801.patch,https://api.github.com/repos/apache/spark/issues/32801/reactions,0,0,0,0,0,0,0,0,0
170,https://api.github.com/repos/apache/spark/issues/32769,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32769/labels{/name},https://api.github.com/repos/apache/spark/issues/32769/comments,https://api.github.com/repos/apache/spark/issues/32769/events,https://github.com/apache/spark/pull/32769,910450034,MDExOlB1bGxSZXF1ZXN0NjYwODY4NjAz,32769,[SPARK-35630][SQL] ExpandExec should not introduce unnecessary exchanges,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-06-03T12:36:03Z,2021-07-21T12:15:28Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Improved `ExpandExec` so it would retain its child's outputPartitioning, when possible. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently `ExpandExecs` `outputPartitioning` is always `UnknownPartitioning(0)`. In some cases we do actually know the correct output partitioning. In those cases we could reduce the number of exchanges by using the correct partitioning.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UT
",https://api.github.com/repos/apache/spark/issues/32769/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32769,https://github.com/apache/spark/pull/32769,https://github.com/apache/spark/pull/32769.diff,https://github.com/apache/spark/pull/32769.patch,https://api.github.com/repos/apache/spark/issues/32769/reactions,0,0,0,0,0,0,0,0,0
171,https://api.github.com/repos/apache/spark/issues/32766,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32766/labels{/name},https://api.github.com/repos/apache/spark/issues/32766/comments,https://api.github.com/repos/apache/spark/issues/32766/events,https://github.com/apache/spark/pull/32766,910342711,MDExOlB1bGxSZXF1ZXN0NjYwNzc5MDcw,32766,[SPARK-35627][CORE] Decommission executors in batches to not overload network bandwidth,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-06-03T10:14:59Z,2021-09-13T22:42:08Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR is adding a thread which will run at scheduled interval to ask a batch of executors to start decommissioning themselves.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currenlty, each executor is asked to starts offloading rdd and shuffle blocks as soon it is decommissioned. This can overload the network bandwidth of the application.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
UT in progress",https://api.github.com/repos/apache/spark/issues/32766/timeline,,spark,apache,q2w,12875634,MDQ6VXNlcjEyODc1NjM0,https://avatars.githubusercontent.com/u/12875634?v=4,,https://api.github.com/users/q2w,https://github.com/q2w,https://api.github.com/users/q2w/followers,https://api.github.com/users/q2w/following{/other_user},https://api.github.com/users/q2w/gists{/gist_id},https://api.github.com/users/q2w/starred{/owner}{/repo},https://api.github.com/users/q2w/subscriptions,https://api.github.com/users/q2w/orgs,https://api.github.com/users/q2w/repos,https://api.github.com/users/q2w/events{/privacy},https://api.github.com/users/q2w/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32766,https://github.com/apache/spark/pull/32766,https://github.com/apache/spark/pull/32766.diff,https://github.com/apache/spark/pull/32766.patch,https://api.github.com/repos/apache/spark/issues/32766/reactions,0,0,0,0,0,0,0,0,0
172,https://api.github.com/repos/apache/spark/issues/32708,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32708/labels{/name},https://api.github.com/repos/apache/spark/issues/32708/comments,https://api.github.com/repos/apache/spark/issues/32708/events,https://github.com/apache/spark/pull/32708,907099592,MDExOlB1bGxSZXF1ZXN0NjU3OTkzNjM3,32708,[WIP] [SPARK-35572] [K8S] add hostNetwork feature to executor,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,17,2021-05-31T06:07:47Z,2021-10-04T00:09:48Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
add hostNetwork feature to executor
modify BasicExecutorFeatureStep add hostNetwork in executorpod 

### Why are the changes needed?
In the process of the company's business promotion, the function of k8s hostNetwork is used. But it is found that spark does not support enable hostNetwork in the configuration.
Found that this function will still be commonly used, so I decided to add a perceptible parameter to enable hostNetwork in spark way.

### Does this PR introduce _any_ user-facing change?
yes, add new config spark.kubernetes.executor.hostNetwork.enable

### How was this patch tested?
add unit test",https://api.github.com/repos/apache/spark/issues/32708/timeline,,spark,apache,zwangsheng,52876270,MDQ6VXNlcjUyODc2Mjcw,https://avatars.githubusercontent.com/u/52876270?v=4,,https://api.github.com/users/zwangsheng,https://github.com/zwangsheng,https://api.github.com/users/zwangsheng/followers,https://api.github.com/users/zwangsheng/following{/other_user},https://api.github.com/users/zwangsheng/gists{/gist_id},https://api.github.com/users/zwangsheng/starred{/owner}{/repo},https://api.github.com/users/zwangsheng/subscriptions,https://api.github.com/users/zwangsheng/orgs,https://api.github.com/users/zwangsheng/repos,https://api.github.com/users/zwangsheng/events{/privacy},https://api.github.com/users/zwangsheng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32708,https://github.com/apache/spark/pull/32708,https://github.com/apache/spark/pull/32708.diff,https://github.com/apache/spark/pull/32708.patch,https://api.github.com/repos/apache/spark/issues/32708/reactions,0,0,0,0,0,0,0,0,0
173,https://api.github.com/repos/apache/spark/issues/32679,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32679/labels{/name},https://api.github.com/repos/apache/spark/issues/32679/comments,https://api.github.com/repos/apache/spark/issues/32679/events,https://github.com/apache/spark/pull/32679,902507698,MDExOlB1bGxSZXF1ZXN0NjUzODY5NTYw,32679,[SPARK-28098][SQL]Support read partitioned Hive tables with subdirect,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-05-26T14:48:59Z,2021-09-06T10:06:48Z,,NONE,,"### What changes were proposed in this pull request?
This support could read source files of partitioned hive table with subdirectories.

### Why are the changes needed?
 While use spark engine to read a partititioned hive table with subdirectories, The source files in subdirectories couldn't
get.

### Does this PR introduce _any_ user-facing change?
no

### How was this patch tested?
new test
",https://api.github.com/repos/apache/spark/issues/32679/timeline,,spark,apache,chong0929,26707386,MDQ6VXNlcjI2NzA3Mzg2,https://avatars.githubusercontent.com/u/26707386?v=4,,https://api.github.com/users/chong0929,https://github.com/chong0929,https://api.github.com/users/chong0929/followers,https://api.github.com/users/chong0929/following{/other_user},https://api.github.com/users/chong0929/gists{/gist_id},https://api.github.com/users/chong0929/starred{/owner}{/repo},https://api.github.com/users/chong0929/subscriptions,https://api.github.com/users/chong0929/orgs,https://api.github.com/users/chong0929/repos,https://api.github.com/users/chong0929/events{/privacy},https://api.github.com/users/chong0929/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32679,https://github.com/apache/spark/pull/32679,https://github.com/apache/spark/pull/32679.diff,https://github.com/apache/spark/pull/32679.patch,https://api.github.com/repos/apache/spark/issues/32679/reactions,0,0,0,0,0,0,0,0,0
174,https://api.github.com/repos/apache/spark/issues/32666,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32666/labels{/name},https://api.github.com/repos/apache/spark/issues/32666/comments,https://api.github.com/repos/apache/spark/issues/32666/events,https://github.com/apache/spark/pull/32666,901672285,MDExOlB1bGxSZXF1ZXN0NjUzMTEyMDg1,32666,[SPARK-30696][SQL] fromUTCtime and toUTCtime produced wrong result on Daylight Saving Time changes days,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-05-26T02:58:39Z,2021-07-14T10:15:41Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Before this patch, fromUTCtime and toUTCtime produced wrong result on Daylight Saving Time changes days
For example, in LA in 1960, timezone switch from UTC-7h to UTC-8h at 2AM in 1960-09-25 but previous version have the cutoff at 8AM

### Why are the changes needed?
correctness


### Does this PR introduce _any_ user-facing change?
yes, as correct the result produced by those two functions


### How was this patch tested?
Add test
",https://api.github.com/repos/apache/spark/issues/32666/timeline,,spark,apache,dgd-contributor,84778052,MDQ6VXNlcjg0Nzc4MDUy,https://avatars.githubusercontent.com/u/84778052?v=4,,https://api.github.com/users/dgd-contributor,https://github.com/dgd-contributor,https://api.github.com/users/dgd-contributor/followers,https://api.github.com/users/dgd-contributor/following{/other_user},https://api.github.com/users/dgd-contributor/gists{/gist_id},https://api.github.com/users/dgd-contributor/starred{/owner}{/repo},https://api.github.com/users/dgd-contributor/subscriptions,https://api.github.com/users/dgd-contributor/orgs,https://api.github.com/users/dgd-contributor/repos,https://api.github.com/users/dgd-contributor/events{/privacy},https://api.github.com/users/dgd-contributor/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32666,https://github.com/apache/spark/pull/32666,https://github.com/apache/spark/pull/32666.diff,https://github.com/apache/spark/pull/32666.patch,https://api.github.com/repos/apache/spark/issues/32666/reactions,0,0,0,0,0,0,0,0,0
175,https://api.github.com/repos/apache/spark/issues/32655,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32655/labels{/name},https://api.github.com/repos/apache/spark/issues/32655/comments,https://api.github.com/repos/apache/spark/issues/32655/events,https://github.com/apache/spark/pull/32655,900075089,MDExOlB1bGxSZXF1ZXN0NjUxNjg1MjE0,32655,[SPARK-33743]change TimestampType match to datetime2 instead of datetime for MsSQLServerDialect,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-05-24T22:46:36Z,2021-09-13T12:46:47Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

SPARK-33743 is to change datetime datatype mapping in JDBC mssqldialect.
### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
case TimestampType => Some(JdbcType(""DATETIME2"", java.sql.Types.TIMESTAMP))

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Spark datetime type is timestamp type. This supports a microsecond resolution.
Sql supports 2 date time types:

datetime can support only milli seconds resolution (0 to 999).
datetime2 is extension of datetime , is compatible with datetime and supports 0 to 9999999 sub second resolution.
datetime2 (Transact-SQL) - SQL Server | Microsoft Docs
datetime (Transact-SQL) - SQL Server | Microsoft Docs

Currently MsSQLServerDialect maps timestamp type to datetime. Datetime only allows 3 digits of microseconds. This implies results in errors when writing timestamp with more than 3 digits of microseconds to sql server table. We want to map timestamp to datetime2, which is compatible with datetime but allows 7 digits of microseconds.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit tests were updated and passed in JDBCSuit.scala.
E2E test done with SQL Server.",https://api.github.com/repos/apache/spark/issues/32655/timeline,,spark,apache,luxu1-ms,68044595,MDQ6VXNlcjY4MDQ0NTk1,https://avatars.githubusercontent.com/u/68044595?v=4,,https://api.github.com/users/luxu1-ms,https://github.com/luxu1-ms,https://api.github.com/users/luxu1-ms/followers,https://api.github.com/users/luxu1-ms/following{/other_user},https://api.github.com/users/luxu1-ms/gists{/gist_id},https://api.github.com/users/luxu1-ms/starred{/owner}{/repo},https://api.github.com/users/luxu1-ms/subscriptions,https://api.github.com/users/luxu1-ms/orgs,https://api.github.com/users/luxu1-ms/repos,https://api.github.com/users/luxu1-ms/events{/privacy},https://api.github.com/users/luxu1-ms/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32655,https://github.com/apache/spark/pull/32655,https://github.com/apache/spark/pull/32655.diff,https://github.com/apache/spark/pull/32655.patch,https://api.github.com/repos/apache/spark/issues/32655/reactions,0,0,0,0,0,0,0,0,0
176,https://api.github.com/repos/apache/spark/issues/32583,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32583/labels{/name},https://api.github.com/repos/apache/spark/issues/32583/comments,https://api.github.com/repos/apache/spark/issues/32583/events,https://github.com/apache/spark/pull/32583,894519805,MDExOlB1bGxSZXF1ZXN0NjQ2ODcxOTU0,32583,[SPARK-35437][SQL] Use expressions to filter Hive partitions at client side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-05-18T15:42:11Z,2021-09-29T11:00:50Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Improve partition filtering speed and reduce metastore pressure.
We can first pull all the partition names, filter by expressions, and then obtain detailed information about the corresponding partitions from the MetaStore Server.

### Why are the changes needed?
When `convertFilters` cannot take effect, cannot filter the queried partitions in advance on the hive MetaStore Server. At this time, `getAllPartitionsOf` will get all partition details.

When the Hive client cannot use the server filter, it will first obtain the values of all partitions, and then filter.

When we have a table with a lot of partitions and there is no way to filter it on the MetaStore Server, we will get all the partition details and filter it on the client side. This is slow and puts a lot of pressure on the MetaStore Server.




### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add UT
",https://api.github.com/repos/apache/spark/issues/32583/timeline,,spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32583,https://github.com/apache/spark/pull/32583,https://github.com/apache/spark/pull/32583.diff,https://github.com/apache/spark/pull/32583.patch,https://api.github.com/repos/apache/spark/issues/32583/reactions,0,0,0,0,0,0,0,0,0
177,https://api.github.com/repos/apache/spark/issues/32562,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32562/labels{/name},https://api.github.com/repos/apache/spark/issues/32562/comments,https://api.github.com/repos/apache/spark/issues/32562/events,https://github.com/apache/spark/pull/32562,892694415,MDExOlB1bGxSZXF1ZXN0NjQ1MzMyNTY1,32562,[WIP][SPARK-35414][SQL] Submit broadcast collect job first to avoid broadcast timeout in AQE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-05-16T15:26:35Z,2021-07-30T02:34:20Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
1. replace executeCollectIterator() by executeCollectIteratorFuture() in SparkPlan.scala to run collect query in async way and return the future of collect result
2. in BroadcastExchangeExec->relationFuture, call executeCollectIteratorFuture() in current thread and get the collectFuture, wait collectFuture in ""broadcast-exchange"" thread 


### Why are the changes needed?
#31269 gives a partial fix to SPARK-33933, which is not a perfect solution. This changes can make sure the broadcast collect job is submitted before shuffle map jobs. #31269 ensure the calling of materialize() of BroadcastQueryStage is before ShuffleQueryStage. In BroadcastQueryStage's materialize(), doPrepare() will call relationFuture, which will submit collect job before return the future.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add UT
",https://api.github.com/repos/apache/spark/issues/32562/timeline,,spark,apache,zhongyu09,3882710,MDQ6VXNlcjM4ODI3MTA=,https://avatars.githubusercontent.com/u/3882710?v=4,,https://api.github.com/users/zhongyu09,https://github.com/zhongyu09,https://api.github.com/users/zhongyu09/followers,https://api.github.com/users/zhongyu09/following{/other_user},https://api.github.com/users/zhongyu09/gists{/gist_id},https://api.github.com/users/zhongyu09/starred{/owner}{/repo},https://api.github.com/users/zhongyu09/subscriptions,https://api.github.com/users/zhongyu09/orgs,https://api.github.com/users/zhongyu09/repos,https://api.github.com/users/zhongyu09/events{/privacy},https://api.github.com/users/zhongyu09/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32562,https://github.com/apache/spark/pull/32562,https://github.com/apache/spark/pull/32562.diff,https://github.com/apache/spark/pull/32562.patch,https://api.github.com/repos/apache/spark/issues/32562/reactions,0,0,0,0,0,0,0,0,0
178,https://api.github.com/repos/apache/spark/issues/32558,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32558/labels{/name},https://api.github.com/repos/apache/spark/issues/32558/comments,https://api.github.com/repos/apache/spark/issues/32558/events,https://github.com/apache/spark/pull/32558,892501853,MDExOlB1bGxSZXF1ZXN0NjQ1MTkxMjUx,32558,[SPARK-34953][CORE][SQL] Add the code change for adding the DateType in the infer schema while reading in CSV and JSON,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,45,2021-05-15T17:41:21Z,2021-07-22T04:19:22Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Till now there is no support in infer schema for the DateType format while reading the CSV/json. In this PR, code change is done to support the DateType when inferschema set true in the options

### Why are the changes needed?
Many times there are multiple columns which are DateType but after inferred from schema they are added as StringType in the schema and than there is need to convert into to_date before we start the running any query. After this change DateType will be added in the schema instead of the StringType

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Unit test added , also testing is done while running read command on spark shell.
",https://api.github.com/repos/apache/spark/issues/32558/timeline,,spark,apache,SaurabhChawla100,34540906,MDQ6VXNlcjM0NTQwOTA2,https://avatars.githubusercontent.com/u/34540906?v=4,,https://api.github.com/users/SaurabhChawla100,https://github.com/SaurabhChawla100,https://api.github.com/users/SaurabhChawla100/followers,https://api.github.com/users/SaurabhChawla100/following{/other_user},https://api.github.com/users/SaurabhChawla100/gists{/gist_id},https://api.github.com/users/SaurabhChawla100/starred{/owner}{/repo},https://api.github.com/users/SaurabhChawla100/subscriptions,https://api.github.com/users/SaurabhChawla100/orgs,https://api.github.com/users/SaurabhChawla100/repos,https://api.github.com/users/SaurabhChawla100/events{/privacy},https://api.github.com/users/SaurabhChawla100/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32558,https://github.com/apache/spark/pull/32558,https://github.com/apache/spark/pull/32558.diff,https://github.com/apache/spark/pull/32558.patch,https://api.github.com/repos/apache/spark/issues/32558/reactions,0,0,0,0,0,0,0,0,0
179,https://api.github.com/repos/apache/spark/issues/32552,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32552/labels{/name},https://api.github.com/repos/apache/spark/issues/32552/comments,https://api.github.com/repos/apache/spark/issues/32552/events,https://github.com/apache/spark/pull/32552,892024323,MDExOlB1bGxSZXF1ZXN0NjQ0Nzk4MzYy,32552,[SPARK-34819][SQL] MapType supports comparable semantics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,58,2021-05-14T15:37:19Z,2021-08-26T11:07:08Z,,MEMBER,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR proposes to support comparable semantics for map types.

NOTE: This PR is the rework of #31967(@WangGuangxin)/#15970(@hvanhovell).

The approach of the PR is similar to `NormalizeFloatingNumbers` and it has the same restriction; in the plan optimizing phase, a new rule named `NormalizeMaps` inserts an expression `SortMapKeys` to make sure two maps having the same key value pairs but with different key ordering are equal (e.g., Map('a' -> 1, 'b' -> 2) should equal to Map('b' -> 2, 'a' -> 1). As for aggregates, this rule is applied in the physical planning phase because all the grouping exprs are not extracted during the logical phase (This is the same restriction with `NormalizeFloatingNumbers`).

The major differences from `NormalizeFloatingNumbers` are as follows;
 - The rule covers all the binary comparisons (`EqualTo`, `GreaterThan`, ...) and `In`/`InSet` in a plan (`NormalizeFloatingNumbers` is applied only into the `EqualTo` comparison in a join plan, an equi-join).
 - This rule does not apply `normalize` recursively and just adds a `SortMapKeys` expr just on each top-level expr (e.g., top-level grouping expr and left/right side expr of binary comparisons).
 - This rule additionally handles `SortOrder`s in sort-related plans.

For sorting map entries, I reused the array ordering logic (See: `MapType.compare` and `CodegenContext.genComp`) because keys and values in map entries follow the array format; it checks if key arrays in two maps are the same first, an then check if value arrays are the same. 

NOTE: Adding duplicate `SortMapKeys` exprs in a binary comparison tree is a known issue; for example, in a query below, `MapType`'s column, `a`, is sorted twice;
```
scala> Seq((Map(1->1), Map(1->2), Map(1->1))).toDF(""a"", ""b"", ""c"").write.saveAsTable(""t"")
scala> sql(""select * from t where a = b and a = c"").explain()
== Physical Plan ==
*(1) Filter ((sortmapkeys(a#35) = sortmapkeys(b#36)) AND (sortmapkeys(a#35) = sortmapkeys(c#37)))
+- FileScan parquet default.t[a#35,b#36,c#37] Batched: false, DataFilters: [(sortmapkeys(a#35) = sortmapkeys(b#36)), (sortmapkeys(a#35) = sortmapkeys(c#37))], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/maropu/Repositories/spark/spark-master/spark-warehouse/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:map<int,int>,b:map<int,int>,c:map<int,int>>
```
But, I don't have a smart idea to avoid it in this PR for now. Probably, I think common subexpression elimination in filter plans can solve it, but Spark does not have the optimization now. (Fro more details, see the previous @viirya PR: https://github.com/apache/spark/pull/30565).

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To improve map usability.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, a user can use map-typed data in GROUP BY, ORDER BY, and PARTITION BY in WINDOW clauses.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add unit tests.",https://api.github.com/repos/apache/spark/issues/32552/timeline,,spark,apache,maropu,692303,MDQ6VXNlcjY5MjMwMw==,https://avatars.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32552,https://github.com/apache/spark/pull/32552,https://github.com/apache/spark/pull/32552.diff,https://github.com/apache/spark/pull/32552.patch,https://api.github.com/repos/apache/spark/issues/32552/reactions,2,2,0,0,0,0,0,0,0
180,https://api.github.com/repos/apache/spark/issues/32477,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32477/labels{/name},https://api.github.com/repos/apache/spark/issues/32477/comments,https://api.github.com/repos/apache/spark/issues/32477/events,https://github.com/apache/spark/pull/32477,880493723,MDExOlB1bGxSZXF1ZXN0NjM0MTQ4NjEz,32477,[SPARK-35348][SQL] Support the utils for escapse the regex for ANSI SQL: SIMILAR TO  ESCAPE syntax,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2021-05-08T08:21:29Z,2021-09-01T09:13:39Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
`ANSI SQL: SIMILAR TO ... ESCAPE` is very useful.
There are some mainstream database support the syntax.
**PostgreSQL**:
https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-SIMILARTO-REGEXP

**Redshift**:
https://docs.aws.amazon.com/redshift/latest/dg/pattern-matching-conditions-similar-to.html

**Sybase**:
http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/like-regexp-similarto.html

**Firebird**:
http://firebirdsql.org/file/documentation/html/en/refdocs/fblangref25/firebird-25-language-reference.html#fblangref25-commons-predsiimilarto

This util supports the following pattern-matching metacharacters:

Operator | Description
-- | --
% | Matches any sequence of zero or more characters.
_ | Matches any single character.
\| | Denotes alternation (either of two alternatives).
\* | Repeat the previous item zero or more times.
\+ | Repeat the previous item one or more times.
? | Repeat the previous item zero or one time.
{m} | Repeat the previous item exactlymtimes.
{m,} | Repeat the previous itemmor more times.
{m,n} | Repeat the previous item at leastmand not more thanntimes.
() | Parentheses group items into a single logical item.
[...] | A bracket expression specifies a character class, just as in POSIX regular expressions.

**Note**
`SIMILAR TO` is similar to `RLIKE`, but with the following differences:
       1. The `SIMILAR TO` operator returns true only if its pattern matches the entire string,
          unlike `RLIKE` behavior, where the pattern can match any portion of the string.
       2. The regex string allow use _ and % as wildcard characters denoting any single character
          and any string, respectively (these are comparable to . and .* in POSIX regular
          expressions).
       3. The regex string allow use escape character like `LIKE` behavior.
       4. '.', '^' and '$' is not a meta character for `SIMILAR TO`.

### Why are the changes needed?
`ANSI SQL: SIMILAR TO ... ESCAPE` is very useful.


### Does this PR introduce _any_ user-facing change?
Yes, a new feature.


### How was this patch tested?
New tests
",https://api.github.com/repos/apache/spark/issues/32477/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32477,https://github.com/apache/spark/pull/32477,https://github.com/apache/spark/pull/32477.diff,https://github.com/apache/spark/pull/32477.patch,https://api.github.com/repos/apache/spark/issues/32477/reactions,0,0,0,0,0,0,0,0,0
181,https://api.github.com/repos/apache/spark/issues/32475,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32475/labels{/name},https://api.github.com/repos/apache/spark/issues/32475/comments,https://api.github.com/repos/apache/spark/issues/32475/events,https://github.com/apache/spark/pull/32475,880430889,MDExOlB1bGxSZXF1ZXN0NjM0MDkwMTIw,32475,[SPARK-34775][SQL] Push down limit through window when partitionSpec is not empty,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-05-08T07:23:04Z,2021-08-16T08:21:13Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This is a followup from #31691. Push down limit through Window when the partitionSpec of all window functions is not empty and the same order is used.
Push down limit through Window when the partitionSpec of all window functions is not empty

And the origin author is @leoluan2009, since he didn't reply for long and did this follow up after invitation 

### Why are the changes needed?
Improve query performance.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/32475/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32475,https://github.com/apache/spark/pull/32475,https://github.com/apache/spark/pull/32475.diff,https://github.com/apache/spark/pull/32475.patch,https://api.github.com/repos/apache/spark/issues/32475/reactions,0,0,0,0,0,0,0,0,0
182,https://api.github.com/repos/apache/spark/issues/32473,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32473/labels{/name},https://api.github.com/repos/apache/spark/issues/32473/comments,https://api.github.com/repos/apache/spark/issues/32473/events,https://github.com/apache/spark/pull/32473,880141699,MDExOlB1bGxSZXF1ZXN0NjMzODE2NDkz,32473,[SPARK-35345][SQL] Add Parquet tests to BloomFilterBenchmark,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,56,2021-05-08T02:46:42Z,2021-08-16T10:21:10Z,,CONTRIBUTOR,,"

### What changes were proposed in this pull request?
Add BloomFilter Benchmark test for Parquet


### Why are the changes needed?
Currently, we only have BloomFilter Benchmark test for ORC. Will add one for Parquet too.

### Does this PR introduce _any_ user-facing change?
no


### How was this patch tested?
tested the newly added benchmark test
",https://api.github.com/repos/apache/spark/issues/32473/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32473,https://github.com/apache/spark/pull/32473,https://github.com/apache/spark/pull/32473.diff,https://github.com/apache/spark/pull/32473.patch,https://api.github.com/repos/apache/spark/issues/32473/reactions,0,0,0,0,0,0,0,0,0
183,https://api.github.com/repos/apache/spark/issues/32468,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32468/labels{/name},https://api.github.com/repos/apache/spark/issues/32468/comments,https://api.github.com/repos/apache/spark/issues/32468/events,https://github.com/apache/spark/pull/32468,878735727,MDExOlB1bGxSZXF1ZXN0NjMyNTE5NDM1,32468,[SPARK-35335][SQL] Coalesce shuffle partition as much as possible for REPARTITION_BY_NONE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-05-07T10:25:08Z,2021-08-16T08:22:09Z,,MEMBER,,"### What changes were proposed in this pull request?

Set minPartitionNum to 1 for `CoalesceShufflePartitions` to coalesce shuffle partition as much as possible for `REPARTITION_BY_NONE`.

### Why are the changes needed?

Before this PR. We use `repartition` to avoid generating small files. But it still generates `spark.sql.adaptive.coalescePartitions.minPartitionNum` number of files, which may leads to exceeding the directory item limitation:
```
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /path/to/table is exceeded: limit=1048576 items=1048576
```

After this PR. We can avoid generating small files:
```sql
INSERT INTO TABLE target_tbl SELECT /*+ REPARTITION */ * FROM t
```
```scala
df.repartition().write.saveAsTable(""target_tbl"")
```


### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test.
",https://api.github.com/repos/apache/spark/issues/32468/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32468,https://github.com/apache/spark/pull/32468,https://github.com/apache/spark/pull/32468.diff,https://github.com/apache/spark/pull/32468.patch,https://api.github.com/repos/apache/spark/issues/32468/reactions,0,0,0,0,0,0,0,0,0
184,https://api.github.com/repos/apache/spark/issues/32397,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32397/labels{/name},https://api.github.com/repos/apache/spark/issues/32397/comments,https://api.github.com/repos/apache/spark/issues/32397/events,https://github.com/apache/spark/pull/32397,870986343,MDExOlB1bGxSZXF1ZXN0NjI2MDgyMDI3,32397,"[SPARK-35084][CORE] Spark 3: supporting ""--packages"" in  k8s cluster mode","[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,52,2021-04-29T12:56:44Z,2021-08-20T02:34:10Z,,NONE,,"### What changes were proposed in this pull request?
Supporting '--packages' in the k8s cluster mode

### Why are the changes needed?
In spark 3, '--packages' in the k8s cluster mode is not supported. I expected that managing dependencies by using packages like spark 2.

Spark 2.4.5

https://github.com/apache/spark/blob/v2.4.5/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala

```scala
     if (!isMesosCluster && !isStandAloneCluster) {
      // Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files
      // too for packages that include Python code
      val resolvedMavenCoordinates = DependencyUtils.resolveMavenDependencies(
        args.packagesExclusions, args.packages, args.repositories, args.ivyRepoPath,
        args.ivySettingsPath)
      
      if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
        if (args.isPython || isInternal(args.primaryResource)) {
          args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
        }
      } 
      
      // install any R packages that may have been passed through --jars or --packages.
      // Spark Packages may contain R source code inside the jar.
      if (args.isR && !StringUtils.isBlank(args.jars)) {
        RPackageUtils.checkAndBuildRPackage(args.jars, printStream, args.verbose)
      }
    } 
 ```

Spark 3.0.2

https://github.com/apache/spark/blob/v3.0.2/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala

```scala
       if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        // In K8s client mode, when in the driver, add resolved jars early as we might need
        // them at the submit time for artifact downloading.
        // For example we might use the dependencies for downloading
        // files from a Hadoop Compatible fs eg. S3. In this case the user might pass:
        // --packages com.amazonaws:aws-java-sdk:1.7.4:org.apache.hadoop:hadoop-aws:2.7.6
        if (isKubernetesClusterModeDriver) {
          val loader = getSubmitClassLoader(sparkConf)
          for (jar <- resolvedMavenCoordinates.split("","")) {
            addJarToClasspath(jar, loader)
          }
        } else if (isKubernetesCluster) {
          // We need this in K8s cluster mode so that we can upload local deps
          // via the k8s application, like in cluster mode driver
          childClasspath ++= resolvedMavenCoordinates.split("","")
        } else {
          args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
          if (args.isPython || isInternal(args.primaryResource)) {
            args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
          }
        }
      }
```

unlike spark2, in spark 3, jars are not added in any place.

### Does this PR introduce _any_ user-facing change?
Unlike spark 2, resolved jars are added not in cluster mode spark submit but in driver.

It's because in spark 3, the feature is added that is uploading jars with prefix ""file://"" to s3.
So, if resolved jars are added in spark submit, every jars from packages are uploading to s3! When I tested it, it is very bad experience to me.

### How was this patch tested?
In my k8s environment, i tested the code.
",https://api.github.com/repos/apache/spark/issues/32397/timeline,,spark,apache,ocworld,13185662,MDQ6VXNlcjEzMTg1NjYy,https://avatars.githubusercontent.com/u/13185662?v=4,,https://api.github.com/users/ocworld,https://github.com/ocworld,https://api.github.com/users/ocworld/followers,https://api.github.com/users/ocworld/following{/other_user},https://api.github.com/users/ocworld/gists{/gist_id},https://api.github.com/users/ocworld/starred{/owner}{/repo},https://api.github.com/users/ocworld/subscriptions,https://api.github.com/users/ocworld/orgs,https://api.github.com/users/ocworld/repos,https://api.github.com/users/ocworld/events{/privacy},https://api.github.com/users/ocworld/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32397,https://github.com/apache/spark/pull/32397,https://github.com/apache/spark/pull/32397.diff,https://github.com/apache/spark/pull/32397.patch,https://api.github.com/repos/apache/spark/issues/32397/reactions,8,8,0,0,0,0,0,0,0
185,https://api.github.com/repos/apache/spark/issues/32395,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32395/labels{/name},https://api.github.com/repos/apache/spark/issues/32395/comments,https://api.github.com/repos/apache/spark/issues/32395/events,https://github.com/apache/spark/pull/32395,870772402,MDExOlB1bGxSZXF1ZXN0NjI1OTA5ODIy,32395,[SPARK-35270][SQL][CORE] Remove the use of guava in order to upgrade guava version to 27,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-04-29T08:47:48Z,2021-07-26T00:50:59Z,,MEMBER,,"### What changes were proposed in this pull request?

Remove the use of guava in order to upgrade guava version to 27.


### Why are the changes needed?

Hadoop 3.2.2 uses Guava 27, the change is for the guava version upgrade.


### Does this PR introduce _any_ user-facing change?

no


### How was this patch tested?

Modify the guava version to 27.0-jre, and then compile.",https://api.github.com/repos/apache/spark/issues/32395/timeline,,spark,apache,wForget,17894939,MDQ6VXNlcjE3ODk0OTM5,https://avatars.githubusercontent.com/u/17894939?v=4,,https://api.github.com/users/wForget,https://github.com/wForget,https://api.github.com/users/wForget/followers,https://api.github.com/users/wForget/following{/other_user},https://api.github.com/users/wForget/gists{/gist_id},https://api.github.com/users/wForget/starred{/owner}{/repo},https://api.github.com/users/wForget/subscriptions,https://api.github.com/users/wForget/orgs,https://api.github.com/users/wForget/repos,https://api.github.com/users/wForget/events{/privacy},https://api.github.com/users/wForget/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32395,https://github.com/apache/spark/pull/32395,https://github.com/apache/spark/pull/32395.diff,https://github.com/apache/spark/pull/32395.patch,https://api.github.com/repos/apache/spark/issues/32395/reactions,0,0,0,0,0,0,0,0,0
186,https://api.github.com/repos/apache/spark/issues/32365,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32365/labels{/name},https://api.github.com/repos/apache/spark/issues/32365/comments,https://api.github.com/repos/apache/spark/issues/32365/events,https://github.com/apache/spark/pull/32365,868688914,MDExOlB1bGxSZXF1ZXN0NjI0MTcyNjM2,32365,[SPARK-35228][SQL] Add expression ToHiveString for keep consistent between hive/spark format in df.show,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,78,2021-04-27T10:07:37Z,2021-09-28T10:29:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Since `spark-sql` and `df.show` show result in different style of string, 
`spark-sql` show data follow hive and  `df.show` show result in spark's result.
Here we add expression `ToHiveString` for keep consistent between `df.show` and `spark-sql` shell

It means we can use this expression to `df.show()` show result in hive format.

The main different between cast to string  and toPrettyString
|  DateType   | Cast(value, StringType) |  ToPrettyString(value)|
|  ----  | ----  | --- |
| DayTimeIntervalType  | INTERVAL [-]'[-]d HH:mm:ss.nnnnnnnnn' DAY TO SECOND |  [-]d HH:mm:ss.nnnnnnnnn   |
| YearMonthIntervalType  | INTERVAL [-]'[-]YYYY-MM' YEAR TO MONTH |  [-]YYYY-MM   |
| ArrayType  | [elem, elem, elem] |  [elem,elem,elem]   |
| MapType  | {key1 -> value1, key2 -> value2} |  {key1:value1,key2:value2} |
| StructType  | {1, 2.0, 3.0} |  {""c1"":1,""c2"":2.0,""c3"":3.0}  |
| DecimalType |   decimal.toString |  decimal.toPlainString |

### Why are the changes needed?
Add expression ToHiveString for keep consistent between hive/spark format in df.show

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/32365/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32365,https://github.com/apache/spark/pull/32365,https://github.com/apache/spark/pull/32365.diff,https://github.com/apache/spark/pull/32365.patch,https://api.github.com/repos/apache/spark/issues/32365/reactions,0,0,0,0,0,0,0,0,0
187,https://api.github.com/repos/apache/spark/issues/32332,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32332/labels{/name},https://api.github.com/repos/apache/spark/issues/32332/comments,https://api.github.com/repos/apache/spark/issues/32332/events,https://github.com/apache/spark/pull/32332,866947011,MDExOlB1bGxSZXF1ZXN0NjIyNzIwNDQ1,32332,[SPARK-35211][PYTHON] verify inferred schema for _create_dataframe,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,34,2021-04-25T08:14:25Z,2021-09-13T04:54:47Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Do extra schema verification after it is inferred

This PR do not introduce any semantic changes except for the extra schema verification.

This pr fixes SPARK-35211 when schema verification is turned on. If schema verification is turned off, the bug described in SPARK-35211 still exists. I will create another PR to solve the issue.


### Why are the changes needed?
``` python
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""false"")
from pyspark.testing.sqlutils  import ExamplePoint
import pandas as pd
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
df = spark.createDataFrame(pdf)
df.show()
```
The result is not correct because of incorrect type conversion. Here is the incorrect result:
```
+----------+
|     point|
+----------+
|(0.0, 0.0)|
|(0.0, 0.0)|
+----------+
```

With this PR, type check will be performed:
```
(spark)   spark git:(sadhen/SPARK-35211)  bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/04/24 17:42:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1619257343692).
SparkSession available as 'spark'.
>>> spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""false"")
>>> from pyspark.testing.sqlutils  import ExamplePoint
>>> import pandas as pd
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
>>> df = spark.createDataFrame(pdf)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 653, in createDataFrame
    return super(SparkSession, self).createDataFrame(
  File ""/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py"", line 340, in createDataFrame
    return self._create_dataframe(data, schema, samplingRatio, verifySchema)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 699, in _create_dataframe
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 499, in _createFromLocal
    data = list(data)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 688, in prepare
    verify_func(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1390, in verify_struct
    verifier(v)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1304, in verify_udf
    verifier(dataType.toInternal(obj))
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1354, in verify_array
    element_verifier(i)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1403, in verify_default
    verify_acceptable_types(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1291, in verify_acceptable_types
    raise TypeError(new_msg(""%s can not accept object %r in type %s""
TypeError: element in array field point: DoubleType can not accept object 1 in type <class 'int'>
```


### Does this PR introduce _any_ user-facing change?
No



### How was this patch tested?
unit test

```
python/run-tests --testnames pyspark.sql.tests.test_dataframe
```
",https://api.github.com/repos/apache/spark/issues/32332/timeline,,spark,apache,darcy-shen,1267865,MDQ6VXNlcjEyNjc4NjU=,https://avatars.githubusercontent.com/u/1267865?v=4,,https://api.github.com/users/darcy-shen,https://github.com/darcy-shen,https://api.github.com/users/darcy-shen/followers,https://api.github.com/users/darcy-shen/following{/other_user},https://api.github.com/users/darcy-shen/gists{/gist_id},https://api.github.com/users/darcy-shen/starred{/owner}{/repo},https://api.github.com/users/darcy-shen/subscriptions,https://api.github.com/users/darcy-shen/orgs,https://api.github.com/users/darcy-shen/repos,https://api.github.com/users/darcy-shen/events{/privacy},https://api.github.com/users/darcy-shen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32332,https://github.com/apache/spark/pull/32332,https://github.com/apache/spark/pull/32332.diff,https://github.com/apache/spark/pull/32332.patch,https://api.github.com/repos/apache/spark/issues/32332/reactions,0,0,0,0,0,0,0,0,0
188,https://api.github.com/repos/apache/spark/issues/32321,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32321/labels{/name},https://api.github.com/repos/apache/spark/issues/32321/comments,https://api.github.com/repos/apache/spark/issues/32321/events,https://github.com/apache/spark/pull/32321,866714166,MDExOlB1bGxSZXF1ZXN0NjIyNTYxMzAz,32321,[SPARK-34771][PYTHON] Support UDT for Pandas/Spark conversion with Arrow support Enabled,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-04-24T10:27:23Z,2021-07-30T07:33:07Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Fix a bug of Pandas/Spark conversion with Arrow support enabled.

Support DataType with UDT:
+ [x] UDT
+ [x] ArrayType(UDT)
+ [x] <del>StructType(..., UDT, ...)</del>, postponed


``` python
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", True)
from pyspark.testing.sqlutils  import ExamplePoint, ExamplePointUDT
from pyspark.sql.types import StructType, StructField
import pandas as pd
schema = StructType([StructField('point', ExamplePointUDT(), False)])
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
df = spark.createDataFrame(pdf, schema)
print(df.toPandas())
```

before (because it fallbacks to arrow disabled):
```
       point
0  (0.0,0.0)
1  (0.0,0.0)
```

after:
```
       point
0  (1.0,1.0)
1  (2.0,2.0)
```


### Why are the changes needed?
Fix a bug.

For UDT without Arrow support, I will create another PR.


### Does this PR introduce _any_ user-facing change?
Support Python UDT in PySpark now. No user-facing changes.


### How was this patch tested?
``` bash
python/run-tests --testnames pyspark.sql.tests.test_arrow
```
",https://api.github.com/repos/apache/spark/issues/32321/timeline,,spark,apache,darcy-shen,1267865,MDQ6VXNlcjEyNjc4NjU=,https://avatars.githubusercontent.com/u/1267865?v=4,,https://api.github.com/users/darcy-shen,https://github.com/darcy-shen,https://api.github.com/users/darcy-shen/followers,https://api.github.com/users/darcy-shen/following{/other_user},https://api.github.com/users/darcy-shen/gists{/gist_id},https://api.github.com/users/darcy-shen/starred{/owner}{/repo},https://api.github.com/users/darcy-shen/subscriptions,https://api.github.com/users/darcy-shen/orgs,https://api.github.com/users/darcy-shen/repos,https://api.github.com/users/darcy-shen/events{/privacy},https://api.github.com/users/darcy-shen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32321,https://github.com/apache/spark/pull/32321,https://github.com/apache/spark/pull/32321.diff,https://github.com/apache/spark/pull/32321.patch,https://api.github.com/repos/apache/spark/issues/32321/reactions,0,0,0,0,0,0,0,0,0
189,https://api.github.com/repos/apache/spark/issues/32298,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32298/labels{/name},https://api.github.com/repos/apache/spark/issues/32298/comments,https://api.github.com/repos/apache/spark/issues/32298/events,https://github.com/apache/spark/pull/32298,864996333,MDExOlB1bGxSZXF1ZXN0NjIxMTMxODQ3,32298,[SPARK-34079][SQL] Merge non-correlated scalar subqueries for better reuse,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,72,2021-04-22T14:08:16Z,2021-09-20T16:05:10Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
This PR adds a new optimizer rule `MergeScalarSubqueries` to merge multiple non-correlated `ScalarSubquery`s to compute multiple scalar values once.

E.g. the following query:
```
SELECT
  (SELECT avg(a) FROM t GROUP BY b),
  (SELECT sum(b) FROM t GROUP BY b)
```
is optimized from:
```
Project [scalar-subquery#231 [] AS scalarsubquery()#241, scalar-subquery#232 [] AS scalarsubquery()#242L]
:  :- Aggregate [b#234], [avg(a#233) AS avg(a)#236]
:  :  +- Relation default.t[a#233,b#234] parquet
:  +- Aggregate [b#240], [sum(b#240) AS sum(b)#238L]
:     +- Project [b#240]
:        +- Relation default.t[a#239,b#240] parquet
+- OneRowRelation
```
to:
```
CommonScalarSubqueries [scalar-subquery#250 []]
:  +- Project [named_struct(avg(a), avg(a)#236, sum(b), sum(b)#238L) AS mergedValue#249]
:     +- Aggregate [b#234], [avg(a#233) AS avg(a)#236, sum(b#234) AS sum(b)#238L]
:        +- Project [a#233, b#234]
:           +- Relation default.t[a#233,b#234] parquet
+- Project [scalarsubqueryreference(0, 0, DoubleType, 231) AS scalarsubquery()#241,
            scalarsubqueryreference(0, 1, LongType, 232) AS scalarsubquery()#242L]
   +- OneRowRelation
```
and in the physical plan subquery references are replaced to the same planned subquery instance and are reused:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *(1) Project [Subquery subquery#250, [id=#104].avg(a) AS scalarsubquery()#241, ReusedSubquery Subquery subquery#250, [id=#104].sum(b) AS scalarsubquery()#242L]
   :  :- Subquery subquery#250, [id=#104]
   :  :  +- AdaptiveSparkPlan isFinalPlan=true
         +- == Final Plan ==
            *(2) HashAggregate(keys=[b#234], functions=[avg(a#233), sum(b#234)], output=[mergedValue#249])
            +- CustomShuffleReader coalesced
               +- ShuffleQueryStage 0
                  +- Exchange hashpartitioning(b#234, 5), ENSURE_REQUIREMENTS, [id=#125]
                     +- *(1) HashAggregate(keys=[b#234], functions=[partial_avg(a#233), partial_sum(b#234)], output=[b#234, sum#252, count#253L, sum#254L])
                        +- *(1) ColumnarToRow
                           +- FileScan parquet default.t[a#233,b#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
         +- == Initial Plan ==
            HashAggregate(keys=[b#234], functions=[avg(a#233), sum(b#234)], output=[mergedValue#249])
            +- Exchange hashpartitioning(b#234, 5), ENSURE_REQUIREMENTS, [id=#102]
               +- HashAggregate(keys=[b#234], functions=[partial_avg(a#233), partial_sum(b#234)], output=[b#234, sum#252, count#253L, sum#254L])
                  +- FileScan parquet default.t[a#233,b#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
   :  +- ReusedSubquery Subquery subquery#250, [id=#104]
   +- *(1) Scan OneRowRelation[]
+- == Initial Plan ==
   ...
```

Please note that the above simple example could be easily optimized into a common select expression without reuse node, but this PR can handle more complex queries as well. 

### Why are the changes needed?
Performance improvement.
```
TPCDS Snappy:                Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-----------------------------------------------------------------------------------------------------------
q9 - master                          45892          47172        1220          0.0      Infinity       1.0X
q9 - scalar subquery merge           16769          16863         124          0.0      Infinity       2.7X
```

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Existing and new UTs.
",https://api.github.com/repos/apache/spark/issues/32298/timeline,,spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32298,https://github.com/apache/spark/pull/32298,https://github.com/apache/spark/pull/32298.diff,https://github.com/apache/spark/pull/32298.patch,https://api.github.com/repos/apache/spark/issues/32298/reactions,0,0,0,0,0,0,0,0,0
190,https://api.github.com/repos/apache/spark/issues/32289,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32289/labels{/name},https://api.github.com/repos/apache/spark/issues/32289/comments,https://api.github.com/repos/apache/spark/issues/32289/events,https://github.com/apache/spark/pull/32289,864624262,MDExOlB1bGxSZXF1ZXN0NjIwODI3NjQ5,32289,[SPARK-33357][K8S] Support Spark application managing with SparkAppHandle on Kubernetes,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-04-22T07:16:00Z,2021-09-28T06:24:50Z,,NONE,,"Co-authored-by: hongdd <hongdongdong@cmss.chinamobile.com>

### What changes were proposed in this pull request?
Supporting SparkAppHandle object to be able to manage a running Spark application on Kubernetes. It can be used to monitor the application changes and to stop the application by pod deletion.

This Pull Request has been raised due to a inactivity of a previous one - #30520


### Why are the changes needed?
There is an inconsistency in the Spark application managing with SparkAppHandle object between Kubernetes and other resource managers such as Yarn/Mesos.

Currently, this feature is not properly implemented on Kubernetes which may cause some issues.


### Does this PR introduce _any_ user-facing change?
Yes, it changes the behavior of `SparkAppHandle` object which the user may use to communicate with the launched Spark application. Its interface is remained as it is. Some missing functionalities have been implemented.


### How was this patch tested?
Few unit tests has been added. May be found in org.apache.spark.deploy.k8s.submit package:

- `PodStatusWatcherSuite` - new ones
- `ClientSuite` - added some
",https://api.github.com/repos/apache/spark/issues/32289/timeline,,spark,apache,grarkydev,54981921,MDQ6VXNlcjU0OTgxOTIx,https://avatars.githubusercontent.com/u/54981921?v=4,,https://api.github.com/users/grarkydev,https://github.com/grarkydev,https://api.github.com/users/grarkydev/followers,https://api.github.com/users/grarkydev/following{/other_user},https://api.github.com/users/grarkydev/gists{/gist_id},https://api.github.com/users/grarkydev/starred{/owner}{/repo},https://api.github.com/users/grarkydev/subscriptions,https://api.github.com/users/grarkydev/orgs,https://api.github.com/users/grarkydev/repos,https://api.github.com/users/grarkydev/events{/privacy},https://api.github.com/users/grarkydev/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32289,https://github.com/apache/spark/pull/32289,https://github.com/apache/spark/pull/32289.diff,https://github.com/apache/spark/pull/32289.patch,https://api.github.com/repos/apache/spark/issues/32289/reactions,0,0,0,0,0,0,0,0,0
191,https://api.github.com/repos/apache/spark/issues/32286,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32286/labels{/name},https://api.github.com/repos/apache/spark/issues/32286/comments,https://api.github.com/repos/apache/spark/issues/32286/events,https://github.com/apache/spark/pull/32286,864553535,MDExOlB1bGxSZXF1ZXN0NjIwNzY5MjQz,32286,[SPARK-35181][CORE] Use zstd for spark.io.compression.codec by default,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,30,2021-04-22T05:26:14Z,2021-07-02T16:49:05Z,,MEMBER,,"### What changes were proposed in this pull request?

This PR aims to use `zstd` as `spark.io.compression.codec` instead of `lz4` in order to reduce the disk IOs and traffic during shuffle processing and  worker decommission storage migration (between executors and to external storage).

- Since SPARK-29434 and SPARK-29576, Apache Spark 3.0+ uses ZSTD `spark.shuffle.mapStatus.compression.codec` by default instead of `GZIP`.
- Since SPARK-34503, Apache Spark 3.2 uses ZSTD for `spark.eventLog.compression.codec` by default instead of `LZ4`.

### Why are the changes needed?

To reduce the disk footprint. For TPCDS 3TB case, `zstd` has 44% less shuffle write size and 43% less shuffle read size
For some cases, the query execution with `zstd` io is 20% faster than `lz4` io.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs.",https://api.github.com/repos/apache/spark/issues/32286/timeline,,spark,apache,dongjoon-hyun,9700541,MDQ6VXNlcjk3MDA1NDE=,https://avatars.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32286,https://github.com/apache/spark/pull/32286,https://github.com/apache/spark/pull/32286.diff,https://github.com/apache/spark/pull/32286.patch,https://api.github.com/repos/apache/spark/issues/32286/reactions,5,5,0,0,0,0,0,0,0
192,https://api.github.com/repos/apache/spark/issues/32237,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32237/labels{/name},https://api.github.com/repos/apache/spark/issues/32237/comments,https://api.github.com/repos/apache/spark/issues/32237/events,https://github.com/apache/spark/pull/32237,861381960,MDExOlB1bGxSZXF1ZXN0NjE4MDUzNjgx,32237,"Quote arguments with special characters (e.g. ""&"") on Windows","[{'id': 1753478521, 'node_id': 'MDU6TGFiZWwxNzUzNDc4NTIx', 'url': 'https://api.github.com/repos/apache/spark/labels/Stale', 'name': 'Stale', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-04-19T14:40:17Z,2021-10-04T00:09:49Z,,NONE,,"This fixes issue #35124. Note that it is quite unlikely that this issue would be mitigated or fixed in the Python standard library itself code due to the history of ""subprocess.list2cmdline"" (see https://bugs.python.org/issue8972 which was created in 2010).

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?

This adds support for a number of special characters (such as ""&"") appearing in Spark configuration values (common in cloud resource URIs).

<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?

If one of the relevant characters appears in a configuration value, the Windows submit scripts do not work correctly, interpreting the character literally. For example, an ampersand marks the beginning of a new command (leading Windows to actually start multiple processes which would typically fail with a rather strange and unexpected error message).

<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?

No.

<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?

This was tested manually, providing a configuration value with a ""&"" character in it.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/32237/timeline,,spark,apache,malthe,26405,MDQ6VXNlcjI2NDA1,https://avatars.githubusercontent.com/u/26405?v=4,,https://api.github.com/users/malthe,https://github.com/malthe,https://api.github.com/users/malthe/followers,https://api.github.com/users/malthe/following{/other_user},https://api.github.com/users/malthe/gists{/gist_id},https://api.github.com/users/malthe/starred{/owner}{/repo},https://api.github.com/users/malthe/subscriptions,https://api.github.com/users/malthe/orgs,https://api.github.com/users/malthe/repos,https://api.github.com/users/malthe/events{/privacy},https://api.github.com/users/malthe/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32237,https://github.com/apache/spark/pull/32237,https://github.com/apache/spark/pull/32237.diff,https://github.com/apache/spark/pull/32237.patch,https://api.github.com/repos/apache/spark/issues/32237/reactions,0,0,0,0,0,0,0,0,0
193,https://api.github.com/repos/apache/spark/issues/32210,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32210/labels{/name},https://api.github.com/repos/apache/spark/issues/32210/comments,https://api.github.com/repos/apache/spark/issues/32210/events,https://github.com/apache/spark/pull/32210,860332908,MDExOlB1bGxSZXF1ZXN0NjE3MjU0MzUw,32210,[SPARK-32634][SQL] Introduce sort-based fallback for shuffled hash join (non-code-gen path),"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,42,2021-04-17T05:48:32Z,2021-06-28T07:04:23Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
A major pain point for spark users to stay away from using shuffled hash join is out of memory issue. Shuffled hash join tends to have OOM issue because it allocates in-memory hashed relation (`UnsafeHashedRelation` or `LongHashedRelation`) for build side, and there's no recovery (e.g. fallback/spill) once the size of hashed relation grows and cannot fit in memory. On the other hand, shuffled hash join is more CPU and IO efficient than sort merge join when joining one large table and a small table (but small table is too large to be broadcasted), as SHJ does not sort the large table, but SMJ needs to do that. See historical discussion in https://github.com/apache/spark/pull/11788 for evidence.

To improve the reliability of shuffled hash join, a fallback mechanism can be introduced to avoid shuffled hash join OOM issue automatically. Similarly we already have a fallback to sort-based aggregation for hash aggregate. The idea is:

* Build hashed relation as current, but stop adding rows to hashed relation if there's no enough memory. Do not throw exception, and do not fail the task/query.
* Sort stream side and build side on join keys if necessary. Note here we need to read all build rows in hashed relation back and destruct the hashed relation on the fly to free memory.
* Execute sort merge join on sorted stream & build side.

Note:

(1).the fallback is automatic and happened per task, which means task 0 can incur the fallback e.g. if it has a big build side, but task 1,2 don't need to incur the fallback depending on the size of hashed relation.

(2).there's no major code change for SHJ and SMJ. Major change is around `HashedRelation` to introduce some new methods, e.g. `HashedRelation.destructiveValues()` to return an Iterator of build side rows in hashed relation and destruct hashed relation on the fly.

(3).Per this PR, a new config `spark.sql.join.enableShuffledHashJoinFallback` is introduced to enable/disable this feature (disable by default). This PR only supports fallback for non-code-gen execution path. Fallback for code-gen will be added in followup PRs, as it will depend on sort merge join code-gen work (SPARK-34705).

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Shuffled hash join OOM is a huge pain point for users and developers. This is the major reason why people stay away from shuffled hash join. This can improve reliability for shuffled hash join quite a bit.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test in `JoinSuite.scala`.",https://api.github.com/repos/apache/spark/issues/32210/timeline,,spark,apache,c21,4629931,MDQ6VXNlcjQ2Mjk5MzE=,https://avatars.githubusercontent.com/u/4629931?v=4,,https://api.github.com/users/c21,https://github.com/c21,https://api.github.com/users/c21/followers,https://api.github.com/users/c21/following{/other_user},https://api.github.com/users/c21/gists{/gist_id},https://api.github.com/users/c21/starred{/owner}{/repo},https://api.github.com/users/c21/subscriptions,https://api.github.com/users/c21/orgs,https://api.github.com/users/c21/repos,https://api.github.com/users/c21/events{/privacy},https://api.github.com/users/c21/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32210,https://github.com/apache/spark/pull/32210,https://github.com/apache/spark/pull/32210.diff,https://github.com/apache/spark/pull/32210.patch,https://api.github.com/repos/apache/spark/issues/32210/reactions,1,1,0,0,0,0,0,0,0
194,https://api.github.com/repos/apache/spark/issues/32031,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32031/labels{/name},https://api.github.com/repos/apache/spark/issues/32031/comments,https://api.github.com/repos/apache/spark/issues/32031/events,https://github.com/apache/spark/pull/32031,848854905,MDExOlB1bGxSZXF1ZXN0NjA3NzI2MTY3,32031,[WIP] Initial work of Remote Shuffle Service on Kubernetes,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,63,2021-04-01T23:37:24Z,2021-09-07T21:27:05Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?

This PR contains Remote Shuffle Service to support dynamic allocation on Kubernetes. The code is mostly copied from [Uber Remote Shuffle Service](https://github.com/uber/RemoteShuffleService) and modified with some renaming. Also added Kubernetes related support which does not exist in original Uber Remote Shuffle Service.

See [here](https://github.com/hiboyang/spark/tree/remote-shuffle-service/remote-shuffle-service) for how to build and run remote shuffle service in Kubernetes. This is initial work and comments/suggestions are welcome.

### Why are the changes needed?

It is still difficult to use dynamic allocation with Spark on Kubernetes. There are several disaggregated/remote shuffle solutions in different companies. Hopefully we could get a remote shuffle implementation into Spark and enhanced in the future by the Spark community.

### Does this PR introduce _any_ user-facing change?

Yes, user could set Spark config (spark.shuffle.manager=org.apache.spark.shuffle.RssShuffleManager) to run Spark applications with remote shuffle service. It will make Spark use the new RssShuffleManager to write/read shuffle data to/from remote shuffle service.

### How was this patch tested?

Manually tested with Spark application in Kubernetes.
",https://api.github.com/repos/apache/spark/issues/32031/timeline,,spark,apache,hiboyang,14280154,MDQ6VXNlcjE0MjgwMTU0,https://avatars.githubusercontent.com/u/14280154?v=4,,https://api.github.com/users/hiboyang,https://github.com/hiboyang,https://api.github.com/users/hiboyang/followers,https://api.github.com/users/hiboyang/following{/other_user},https://api.github.com/users/hiboyang/gists{/gist_id},https://api.github.com/users/hiboyang/starred{/owner}{/repo},https://api.github.com/users/hiboyang/subscriptions,https://api.github.com/users/hiboyang/orgs,https://api.github.com/users/hiboyang/repos,https://api.github.com/users/hiboyang/events{/privacy},https://api.github.com/users/hiboyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32031,https://github.com/apache/spark/pull/32031,https://github.com/apache/spark/pull/32031.diff,https://github.com/apache/spark/pull/32031.patch,https://api.github.com/repos/apache/spark/issues/32031/reactions,0,0,0,0,0,0,0,0,0
195,https://api.github.com/repos/apache/spark/issues/31997,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31997/labels{/name},https://api.github.com/repos/apache/spark/issues/31997/comments,https://api.github.com/repos/apache/spark/issues/31997/events,https://github.com/apache/spark/pull/31997,843617958,MDExOlB1bGxSZXF1ZXN0NjAyOTc1MzY1,31997,Search PYSPARK_PYTHON in configurations,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-03-29T17:40:57Z,2021-06-25T20:55:04Z,,NONE,,"Check if the PYSPARK_PYTHON was defined in the configurations passed to the context.

### What changes were proposed in this pull request?

Searches for the PYSPARK_PYTHON environment variable in the configurations passed to the Context in Python interface.

### Why are the changes needed?

When the variable is not defined in the local OS the corresponding block will use the `python3`, [line 230](https://github.com/ggarciabas/spark/blob/master/python/pyspark/context.py#L230):
```python
self.pythonExec = os.environ.get(""PYSPARK_PYTHON"", 'python3')
```
However, if some specific virtual environment is sent to the executors and/or the python path is changed in the executors, the configuration `spark.executorEnv.PYSPARK_PYTHON` will not be considered.",https://api.github.com/repos/apache/spark/issues/31997/timeline,,spark,apache,ggarciabas,3122199,MDQ6VXNlcjMxMjIxOTk=,https://avatars.githubusercontent.com/u/3122199?v=4,,https://api.github.com/users/ggarciabas,https://github.com/ggarciabas,https://api.github.com/users/ggarciabas/followers,https://api.github.com/users/ggarciabas/following{/other_user},https://api.github.com/users/ggarciabas/gists{/gist_id},https://api.github.com/users/ggarciabas/starred{/owner}{/repo},https://api.github.com/users/ggarciabas/subscriptions,https://api.github.com/users/ggarciabas/orgs,https://api.github.com/users/ggarciabas/repos,https://api.github.com/users/ggarciabas/events{/privacy},https://api.github.com/users/ggarciabas/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31997,https://github.com/apache/spark/pull/31997,https://github.com/apache/spark/pull/31997.diff,https://github.com/apache/spark/pull/31997.patch,https://api.github.com/repos/apache/spark/issues/31997/reactions,0,0,0,0,0,0,0,0,0
196,https://api.github.com/repos/apache/spark/issues/31896,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31896/labels{/name},https://api.github.com/repos/apache/spark/issues/31896/comments,https://api.github.com/repos/apache/spark/issues/31896/events,https://github.com/apache/spark/pull/31896,835653456,MDExOlB1bGxSZXF1ZXN0NTk2Mjc5NTc3,31896,[WIP][SPARK-32899][CORE] Support submit application with user-defined cluster manager,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-03-19T07:34:19Z,2021-07-08T00:44:03Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add the support to submit applications to the user-defined cluster manager.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
We have supported users to define the customed cluster manager with ExternalClusterManager trait. However, we can not submit the application with SparkSubmit. The reason is that we check the master whether is the natively support one in SparkSubmit. However, the customed cluster manager is checked in SparkContext. So we could not submit applications with 
`SparkSubmit` or pyspark to those user-defined cluster managers.

In this patch, we extend the existed plugin: `SparkSubmitOperations` to support customize of the submit operation. This makes the `SparkSubmitOperations` a developer API. And decouple the current spark submit preparing code into subclass which makes it easier for maintaining.  

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UTs.",https://api.github.com/repos/apache/spark/issues/31896/timeline,,spark,apache,ConeyLiu,12733256,MDQ6VXNlcjEyNzMzMjU2,https://avatars.githubusercontent.com/u/12733256?v=4,,https://api.github.com/users/ConeyLiu,https://github.com/ConeyLiu,https://api.github.com/users/ConeyLiu/followers,https://api.github.com/users/ConeyLiu/following{/other_user},https://api.github.com/users/ConeyLiu/gists{/gist_id},https://api.github.com/users/ConeyLiu/starred{/owner}{/repo},https://api.github.com/users/ConeyLiu/subscriptions,https://api.github.com/users/ConeyLiu/orgs,https://api.github.com/users/ConeyLiu/repos,https://api.github.com/users/ConeyLiu/events{/privacy},https://api.github.com/users/ConeyLiu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31896,https://github.com/apache/spark/pull/31896,https://github.com/apache/spark/pull/31896.diff,https://github.com/apache/spark/pull/31896.patch,https://api.github.com/repos/apache/spark/issues/31896/reactions,0,0,0,0,0,0,0,0,0
197,https://api.github.com/repos/apache/spark/issues/31847,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31847/labels{/name},https://api.github.com/repos/apache/spark/issues/31847/comments,https://api.github.com/repos/apache/spark/issues/31847/events,https://github.com/apache/spark/pull/31847,832376174,MDExOlB1bGxSZXF1ZXN0NTkzNTQzNjc1,31847,[SPARK-34755][SQL] Support the utils for transform number format,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-03-16T03:32:04Z,2021-07-05T14:55:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
Data Type Formatting Functions: `to_number` and `to_char` is very useful. There are some mainstream database support the syntax.
**PostgreSQL:**
**Oracle:**
**Vertica**
**Redshift**
**DB2**
**Teradata**
**Snowflake:**
**Exasol**
**Phoenix**
**Singlestore**
**Intersystems**
The implement has many different between `Postgresql` ,`Oracle` and `Phoenix`.
So, this PR follows the implement of `to_number` in `Oracle` that give a strict parameter verification.
So, this PR follows the implement of `to_number` in `Phoenix` that uses BigDecimal.



This PR support the patterns for numeric formatting as follows:

Pattern | Description
-- | --
9 | Value with the specified number of digits
0 | Value with leading zeros
.(period) | Decimal point
,(comma) | Group (thousand) separator
S | Sign anchored to number (uses locale)
$ | a value with a leading dollar sign
D | Decimal point (uses locale)
G | Group separator (uses locale)


### Why are the changes needed?
to_number and to_char are very useful for formatted currency to number conversion.


### Does this PR introduce _any_ user-facing change?
No.


### How was this patch tested?
Jenkins test
",https://api.github.com/repos/apache/spark/issues/31847/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31847,https://github.com/apache/spark/pull/31847,https://github.com/apache/spark/pull/31847.diff,https://github.com/apache/spark/pull/31847.patch,https://api.github.com/repos/apache/spark/issues/31847/reactions,0,0,0,0,0,0,0,0,0
198,https://api.github.com/repos/apache/spark/issues/31763,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31763/labels{/name},https://api.github.com/repos/apache/spark/issues/31763/comments,https://api.github.com/repos/apache/spark/issues/31763/events,https://github.com/apache/spark/pull/31763,823587159,MDExOlB1bGxSZXF1ZXN0NTg2MDE4MDI1,31763,[SPARK-33114][CORE] Add metadata in MapStatus to support custom shuffle manager ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-03-06T07:31:45Z,2021-07-05T10:13:11Z,,CONTRIBUTOR,,"This PR is copied from https://github.com/apache/spark/pull/30004, with extra change addressing the comments. My git environment was messed up and could not update previous PR 30004. Thus create this new PR to replace the previous one.

### What changes were proposed in this pull request?
Add generic metadata in MapStatus class to support custom shuffle manager. Also add a new method to retrieve all map output statuses and their metadata. See Jira: https://issues.apache.org/jira/projects/SPARK/issues/SPARK-33114

### Why are the changes needed?
Current MapStatus class is tightly bound with local (sort merge) shuffle which uses BlockManagerId to store the shuffle data location. It could not support other custom shuffle manager implementation. 

For example, when we implement Remote Shuffle Service, we want to put remote shuffle server information into MapStatus so reducer could fetch that information and figure out where to fetch data. The added MapStatus.metadata field could store such information.

If people implement other shuffle manager, they could also store their related information into this metadata field.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Added unit test
",https://api.github.com/repos/apache/spark/issues/31763/timeline,,spark,apache,hiboyang,14280154,MDQ6VXNlcjE0MjgwMTU0,https://avatars.githubusercontent.com/u/14280154?v=4,,https://api.github.com/users/hiboyang,https://github.com/hiboyang,https://api.github.com/users/hiboyang/followers,https://api.github.com/users/hiboyang/following{/other_user},https://api.github.com/users/hiboyang/gists{/gist_id},https://api.github.com/users/hiboyang/starred{/owner}{/repo},https://api.github.com/users/hiboyang/subscriptions,https://api.github.com/users/hiboyang/orgs,https://api.github.com/users/hiboyang/repos,https://api.github.com/users/hiboyang/events{/privacy},https://api.github.com/users/hiboyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31763,https://github.com/apache/spark/pull/31763,https://github.com/apache/spark/pull/31763.diff,https://github.com/apache/spark/pull/31763.patch,https://api.github.com/repos/apache/spark/issues/31763/reactions,0,0,0,0,0,0,0,0,0
199,https://api.github.com/repos/apache/spark/issues/31744,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31744/labels{/name},https://api.github.com/repos/apache/spark/issues/31744/comments,https://api.github.com/repos/apache/spark/issues/31744/events,https://github.com/apache/spark/pull/31744,822530796,MDExOlB1bGxSZXF1ZXN0NTg1MTQxMTI2,31744,[WIP][SPARK-34625][R] Enable Arrow optimization for float types with SparkR,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-03-04T21:40:23Z,2021-08-28T03:26:11Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
I deleted several error-handlers like the following from the SparkR package `types.R` file. 

```r
  if (any(field_strings == ""FloatType"")) {
    stop(""Arrow optimization in R does not support float type yet."")
  }
```

Before the proposed changes, error handlers in `types.R` like the snippet below prevented arrow optimization from being to applied to float types.

### Why are the changes needed?

The R `arrow` package now supports `FloatType`, `BinaryType`, `ArrayType`,`StructType`. This was brought to my attention by Neal Richardson, maintainer of the R Arrow package, in the comments on this issue: https://issues.apache.org/jira/browse/ARROW-3783

This change allows SparkR users to leverage arrow optimization for the additional types supported. Documentation for currently supported types can be viewed at https://arrow.apache.org/docs/r/articles/arrow.html#arrow-to-r 

```r
str(collect(SparkR::sql(""SELECT float('1') AS x;""))$x[[1]])
## num 1

## Warning message:In value[[3L]](cond) :  The conversion from Spark DataFrame to R DataFrame was attempted with Arrow optimization because 'spark.sql.execution.arrow.sparkr.enabled' is set to true; however, failed, attempting non-optimization. Reason: Error in checkSchemaInArrow(schema(x)): Arrow optimization in R does not support float type yet.
```

### Does this PR introduce _any_ user-facing change?

`FloatType`, `BinaryType`, `ArrayType`, and `StructType`, types will now be returned with arrow optimization (when `spark.sql.execution.arrow.sparkr.enabled = ""true""` and the `R` `arrow` package is available in the executing environment.


### How was this patch tested?

I built a copy of the SparkR package locally under R 3.6.0 using this branch, connected to a Databricks cluster running Databricks runtime version 7.3 LTS (Spark 3.0.1, Scala 2.12), and executed the following to test whether `FloatType` could be returned without error.

```r
str(collect(SparkR::sql(""SELECT float('-9999999999999999.9999999999999') AS x1,
                        float('-1.0') AS x2,
                        float('-0.00001') AS x3,
                        float('0') AS x4,
                        float('0.00001') AS x5,
                        float('1.0') AS x6,
                        float('9999999999999999.9999999999999') AS x7;"")))

# 'data.frame':	1 obs. of  7 variables:
#  $ x1: num -1e+16
#  $ x2: num -1
#  $ x3: num -1e-05
#  $ x4: num 0
#  $ x5: num 1e-05
#  $ x6: num 1
#  $ x7: num 1e+16
```

In addition, I executed a handful of quick tests verifying that `BinaryType`, `ArrayType`, and `StructType` could be returned as well.  However, I have not worked with these data types in Spark, so I think some additional testing is probably merited.

```r
str(collect(SparkR::sql(""SELECT binary('0') x;""))$x[[1]])
# raw 30
str(collect(SparkR::sql('SELECT array(\'{""x"": [{""a"": 1, ""b"": 2, ""c"": 3}]}\') x;'))$x[[1]])
# chr ""{\""x\"": [{\""a\"": 1, \""b\"": 2, \""c\"": 3}]}""
str(collect(SparkR::sql(""SELECT struct('1') x;""))$x[[1]])
# chr ""1""
```
",https://api.github.com/repos/apache/spark/issues/31744/timeline,,spark,apache,msummersgill,19575728,MDQ6VXNlcjE5NTc1NzI4,https://avatars.githubusercontent.com/u/19575728?v=4,,https://api.github.com/users/msummersgill,https://github.com/msummersgill,https://api.github.com/users/msummersgill/followers,https://api.github.com/users/msummersgill/following{/other_user},https://api.github.com/users/msummersgill/gists{/gist_id},https://api.github.com/users/msummersgill/starred{/owner}{/repo},https://api.github.com/users/msummersgill/subscriptions,https://api.github.com/users/msummersgill/orgs,https://api.github.com/users/msummersgill/repos,https://api.github.com/users/msummersgill/events{/privacy},https://api.github.com/users/msummersgill/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31744,https://github.com/apache/spark/pull/31744,https://github.com/apache/spark/pull/31744.diff,https://github.com/apache/spark/pull/31744.patch,https://api.github.com/repos/apache/spark/issues/31744/reactions,0,0,0,0,0,0,0,0,0
200,https://api.github.com/repos/apache/spark/issues/31569,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31569/labels{/name},https://api.github.com/repos/apache/spark/issues/31569/comments,https://api.github.com/repos/apache/spark/issues/31569/events,https://github.com/apache/spark/pull/31569,808942800,MDExOlB1bGxSZXF1ZXN0NTczODkxMDU1,31569,[SPARK-34443][CORE] Replace symbol literals with Symbol constructor invocations to comply with Scala 2.13,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1988040187, 'node_id': 'MDU6TGFiZWwxOTg4MDQwMTg3', 'url': 'https://api.github.com/repos/apache/spark/labels/AVRO', 'name': 'AVRO', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,65,2021-02-16T02:54:56Z,2021-07-01T04:37:09Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR replaces all the occurrence of symbol literals with `Symbol()` constructors.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
As of Scala 2.13, symbol literals are deprecated so when we build with Scala 2.13 and sbt, the compiler loudly inform us.
```
[warn] /home/kou/work/oss/spark-scala-2.13/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala:34:38: [deprecation @  | origin= | version=2.13.0] symbol literal is deprecated; use Symbol(""id"") instead
[warn]     val ds = spark.range(20).select(('id % 3).as(""key""), 'id).as[(Long, Long)]
[warn]                                      ^
[warn] /home/kou/work/oss/spark-scala-2.13/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala:34:58: [deprecation @  | origin= | version=2.13.0] symbol literal is deprecated; use Symbol(""id"") instead
[warn]     val ds = spark.range(20).select(('id % 3).as(""key""), 'id).as[(Long, Long)]
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

I confirmed that `compile` and `test:compile` successfully finish with both Scala 2.12 and 2.13.",https://api.github.com/repos/apache/spark/issues/31569/timeline,,spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31569,https://github.com/apache/spark/pull/31569,https://github.com/apache/spark/pull/31569.diff,https://github.com/apache/spark/pull/31569.patch,https://api.github.com/repos/apache/spark/issues/31569/reactions,0,0,0,0,0,0,0,0,0
201,https://api.github.com/repos/apache/spark/issues/31267,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31267/labels{/name},https://api.github.com/repos/apache/spark/issues/31267/comments,https://api.github.com/repos/apache/spark/issues/31267/events,https://github.com/apache/spark/pull/31267,790587745,MDExOlB1bGxSZXF1ZXN0NTU4NzczNDE3,31267,[SPARK-21195][CORE] MetricSystem should pick up dynamically registered metrics in sources,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-01-21T01:23:55Z,2021-09-28T03:29:08Z,,NONE,,"### What changes were proposed in this pull request?
MetricSystem picks up new metrics from sources that are added throughout execution. If you do measurements via dynamic proxies you might not want to redeclare all metrics that the proxies will create and you'd prefer them to get populated as they're being produced. Right now all sources are processed only onceat startup and metrics are picked up only if they have been registered statically at compile time. Behaviour I am proposing lets you not have to declare metrics in two places.

This had been previously suggested in https://github.com/apache/spark/pull/18406 and https://github.com/apache/spark/pull/29980. I have reduced the scope of the change to just dynamic metric registration.

### Why are the changes needed?
Currently there's no way to access MetricRegistry that MetricsSystem uses to hold its state and as such it's not possible to reprocess a source. MetricsSystem throws if any metric had already been registered previously.

n.b. the MetricRegistry is added as a constructor argument to make testing easier but could as well be accessed via reflection as a private variable.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added tests",https://api.github.com/repos/apache/spark/issues/31267/timeline,,spark,apache,robert3005,512084,MDQ6VXNlcjUxMjA4NA==,https://avatars.githubusercontent.com/u/512084?v=4,,https://api.github.com/users/robert3005,https://github.com/robert3005,https://api.github.com/users/robert3005/followers,https://api.github.com/users/robert3005/following{/other_user},https://api.github.com/users/robert3005/gists{/gist_id},https://api.github.com/users/robert3005/starred{/owner}{/repo},https://api.github.com/users/robert3005/subscriptions,https://api.github.com/users/robert3005/orgs,https://api.github.com/users/robert3005/repos,https://api.github.com/users/robert3005/events{/privacy},https://api.github.com/users/robert3005/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31267,https://github.com/apache/spark/pull/31267,https://github.com/apache/spark/pull/31267.diff,https://github.com/apache/spark/pull/31267.patch,https://api.github.com/repos/apache/spark/issues/31267/reactions,0,0,0,0,0,0,0,0,0
202,https://api.github.com/repos/apache/spark/issues/31024,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/31024/labels{/name},https://api.github.com/repos/apache/spark/issues/31024/comments,https://api.github.com/repos/apache/spark/issues/31024/events,https://github.com/apache/spark/pull/31024,778546917,MDExOlB1bGxSZXF1ZXN0NTQ4NjEzMzYz,31024,[SPARK-33979][SQL] Reorder predicate,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,60,2021-01-05T03:31:11Z,2021-09-12T02:02:09Z,,MEMBER,,"### What changes were proposed in this pull request?

This pr add support reorder predicates based on selectivity and compute cost.

1. Conjunctive predicates
   Formula: `(selectivity - 1.0) / expression cost`
2. Disjunctive predicates
   Formula: `(-selectivity) / expression cost`

 
### Why are the changes needed?

Improve filter performance. Many mainstream databases support this optimization:

https://issues.apache.org/jira/browse/HIVE-21857
https://issues.apache.org/jira/browse/IMPALA-2805

https://www.ibm.com/support/knowledgecenter/SSSHTQ_8.1.0/com.ibm.netcool_OMNIbus.doc_8.1.0/omnibus/wip/admin/reference/omn_adm_per_optimizationrules.html#omn_adm_per_optimizationrules__reorder
https://docs.oracle.com/en/database/oracle/oracle-database/21/addci/extensible-optimizer-interface.html#GUID-28A4EDA6-19DD-4773-B3B8-1802C3B01E21
 https://docs.oracle.com/cd/B10501_01/server.920/a96533/hintsref.htm#13676


### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and Benchmark test.

Benchmark by `Benchmark` framework.
```scala
val numRows = 1024 * 1024 * 15
val mid = numRows / 2
spark.range(numRows).selectExpr(""id as a"", ""id as b"", ""id as c"").write.saveAsTable(""t1"")
spark.sql(""ANALYZE TABLE t1 COMPUTE STATISTICS FOR ALL COLUMNS"")
val benchmark1 = new org.apache.spark.benchmark.Benchmark(""Benchmark predicate reorder(Reduce MultiLikeBase priority)"", numRows, minNumIters = 5)
Seq(false, true).foreach { pushDownEnabled =>
  Seq(false, true).foreach { reorderEnabled =>
    val name = s""Parquet Vectorized ${if (pushDownEnabled) s""(pushDownEnabled)"" else """"} ${if (reorderEnabled) s""(reorderEnabled)"" else """"}""
    benchmark1.addCase(name) { _ =>
      withSQLConf(
        SQLConf.CBO_ENABLED.key -> ""true"",
        SQLConf.PARQUET_FILTER_PUSHDOWN_ENABLED.key -> s""$pushDownEnabled"",
        SQLConf.PREDICATE_REORDER_ENABLED.key -> s""$reorderEnabled"") {
        spark.sql(s""SELECT * FROM t1 WHERE cast(a as string) like all (${Range(1, 500).map(s => s""'%${s}%test%'"").mkString("", "")}) and c > $mid and b < ${mid + 10}"").write.format(""noop"").mode(""Overwrite"").save()
      }
    }
  }
}
benchmark1.run()

val benchmark2 = new org.apache.spark.benchmark.Benchmark(""Benchmark predicate reorder(Reorder predicates by selectivity)"", numRows, minNumIters = 5)
Seq(false, true).foreach { pushDownEnabled =>
  Seq(false, true).foreach { reorderEnabled =>
    val name = s""Parquet Vectorized ${if (pushDownEnabled) s""(pushDownEnabled)"" else """"} ${if (reorderEnabled) s""(reorderEnabled)"" else """"}""
    benchmark2.addCase(name) { _ =>
      withSQLConf(
        SQLConf.CBO_ENABLED.key -> ""true"",
        SQLConf.PARQUET_FILTER_PUSHDOWN_ENABLED.key -> s""$pushDownEnabled"",
        SQLConf.PREDICATE_REORDER_ENABLED.key -> s""$reorderEnabled"") {
        spark.sql(s""SELECT * FROM t1 WHERE a > 10 and b > 20000 and c > $mid and b < ${mid + 10}"").write.format(""noop"").mode(""Overwrite"").save()
      }
    }
  }
}
benchmark2.run()
```
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark predicate reorder(Reduce MultiLikeBase priority):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------------------------
Parquet Vectorized                                                   3872           4100         249          4.1         246.2       1.0X
Parquet Vectorized  (reorderEnabled)                                  453            479          23         34.7          28.8       8.6X
Parquet Vectorized (pushDownEnabled)                                 4045           4707        1010          3.9         257.2       1.0X
Parquet Vectorized (pushDownEnabled) (reorderEnabled)                 426            489          42         37.0          27.1       9.1X

Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark predicate reorder(Reorder predicates by selectivity):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
----------------------------------------------------------------------------------------------------------------------------------------------
Parquet Vectorized                                                        541            664         148         29.1          34.4       1.0X
Parquet Vectorized  (reorderEnabled)                                      484            537          52         32.5          30.8       1.1X
Parquet Vectorized (pushDownEnabled)                                      409            479          71         38.5          26.0       1.3X
Parquet Vectorized (pushDownEnabled) (reorderEnabled)                     368            396          20         42.7          23.4       1.5X
```
",https://api.github.com/repos/apache/spark/issues/31024/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/31024,https://github.com/apache/spark/pull/31024,https://github.com/apache/spark/pull/31024.diff,https://github.com/apache/spark/pull/31024.patch,https://api.github.com/repos/apache/spark/issues/31024/reactions,0,0,0,0,0,0,0,0,0
203,https://api.github.com/repos/apache/spark/issues/30565,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/30565/labels{/name},https://api.github.com/repos/apache/spark/issues/30565/comments,https://api.github.com/repos/apache/spark/issues/30565/events,https://github.com/apache/spark/pull/30565,754993855,MDExOlB1bGxSZXF1ZXN0NTMwNzkwMjQz,30565,[WIP][SPARK-33625][SQL] Subexpression elimination for whole-stage codegen in Filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2020-12-02T06:41:47Z,2021-07-20T04:34:50Z,,MEMBER,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This patch proposes to enable whole-stage subexpression elimination for Filter.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

We made subexpression elimination available for whole-stage codegen in ProjectExec. Another one operator that frequently runs into subexpressions, is Filter. We should also make whole-stage codegen subexpression elimination in FilterExec too.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit test",https://api.github.com/repos/apache/spark/issues/30565/timeline,,spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/30565,https://github.com/apache/spark/pull/30565,https://github.com/apache/spark/pull/30565.diff,https://github.com/apache/spark/pull/30565.patch,https://api.github.com/repos/apache/spark/issues/30565/reactions,2,2,0,0,0,0,0,0,0
204,https://api.github.com/repos/apache/spark/issues/30483,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/30483/labels{/name},https://api.github.com/repos/apache/spark/issues/30483/comments,https://api.github.com/repos/apache/spark/issues/30483/events,https://github.com/apache/spark/pull/30483,749636624,MDExOlB1bGxSZXF1ZXN0NTI2NDIxNTE4,30483,[SPARK-33449][SQL] Support File Metadata Cache for Parquet,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,76,2020-11-24T11:47:18Z,2021-08-19T06:04:14Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
The main purpose of this pr is to introduce the File Meta Cache mechanism for Spark SQL and the basic File Meta Cache implementation for Parquet is provided at the same time.

The main change of this pr as follows:

- Defined a `FileMetaCacheManager` to cache the mapping `FileMetaKey` to `FileMeta`. The `FileMetaKey` is the cache key, `equals` is determined by the file path by default. The `FileMeta` used to represent the cache value and It is generated by the  `FileMetaKey#getFileMeta `  method. 

- Currently, the `FileMetaCacheManager` supports a simple cache expiration elimination mechanism, and the expiration time is determined by the new config  `FILE_META_CACHE_TTL_SINCE_LAST_ACCESS `

- For Parquet file format, this pr added `ParquetFileMetaKey ` and `ParquetFileMeta` to cache Parquet file Footer and the Footer cache can be used by Vectorized read scene both in DS API V1 and V2, the feature will be enabled when `FILE_META_CACHE_PARQUET_ENABLED ` is true

Currently, the file meta cache mechanism cannot be used by `RowBasedReader`, and it needs the completion of [PARQUET-1965](https://issues.apache.org/jira/browse/PARQUET-1965) for further support.

### Why are the changes needed?
Support Parquet datasource use File Meta Cache mechanism to reduce the times of metadata reads multiple queries are performed on the same dataset. 

### Does this PR introduce _any_ user-facing change?
Add 3 new config:

- `FILE_META_CACHE_PARQUET_ENABLED(spark.sql.fileMetaCache.parquet.enabled)` to indicate if enable parquet file meta cache mechanism
- `FILE_META_CACHE_TTL_SINCE_LAST_ACCESS(spark.sql.fileMetaCache.ttlSinceLastAccess)` to represent Time-to-live for file metadata cache entry after last access, the unit is seconds.

### How was this patch tested?

- Pass the Jenkins or GitHub Action
- Add new test suites to `ParquetQuerySuite`
",https://api.github.com/repos/apache/spark/issues/30483/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/30483,https://github.com/apache/spark/pull/30483,https://github.com/apache/spark/pull/30483.diff,https://github.com/apache/spark/pull/30483.patch,https://api.github.com/repos/apache/spark/issues/30483/reactions,0,0,0,0,0,0,0,0,0
205,https://api.github.com/repos/apache/spark/issues/29810,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/29810/labels{/name},https://api.github.com/repos/apache/spark/issues/29810/comments,https://api.github.com/repos/apache/spark/issues/29810/events,https://github.com/apache/spark/pull/29810,704975672,MDExOlB1bGxSZXF1ZXN0NDg5ODA2OTk2,29810,"[SPARK-32940][SQL] Collect, first and last should be deterministic aggregate functions","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2020-09-19T18:22:57Z,2021-09-30T09:35:09Z,,CONTRIBUTOR,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Collect, first and last have mistakenly been marked as non-deterministic. They are actually deterministic iff their child expression is deterministic.

For example collect was marked as non-deterministic in #14749. The reasoning was that its output depends on the actual order of input rows. Although it is correct that these aggregators depend on the order of input rows, it does not make them non-deterministic.

In `EliminateSorts` optimizer rule, there is a method `isOrderIrrelevantAggs`, that lists all aggregators that do not depend on their input row order. Collect, first and last are correctly not listed there. 
An aggregator would be non-deterministic if its output for a group would depend on previous groups it has aggregated - I can't think of any practical examples of this kind of aggregator in Spark.

An analogous aggregator to these would be sum on float and double datatype  - its result does depend on the order of its inputs, but is deterministic. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

The optimizer rule `PushPredicateThroughNonJoin` can work in more cases. 

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
UT",https://api.github.com/repos/apache/spark/issues/29810/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/29810,https://github.com/apache/spark/pull/29810,https://github.com/apache/spark/pull/29810.diff,https://github.com/apache/spark/pull/29810.patch,https://api.github.com/repos/apache/spark/issues/29810/reactions,0,0,0,0,0,0,0,0,0
206,https://api.github.com/repos/apache/spark/issues/29719,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/29719/labels{/name},https://api.github.com/repos/apache/spark/issues/29719/comments,https://api.github.com/repos/apache/spark/issues/29719/events,https://github.com/apache/spark/pull/29719,698516471,MDExOlB1bGxSZXF1ZXN0NDg0MzUxMTEz,29719,[SPARK-32846][SQL][PYTHON] Support createDataFrame from an RDD of pd.DataFrames,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2020-09-10T21:32:27Z,2021-08-17T12:16:17Z,,NONE,,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
Added support to `createDataFrame` to receive an RDD of `pd.DataFrame` objects, and convert them using arrow into an RDD of record batches which is then directly converted to a spark DF.

Added a `pandasRDD` flag to `createDataFrame` to distinguish between `RDD[pd.DataFrame]` and other RDDs without peeking into their content.

```python
from pyspark.sql import SparkSession
import pyspark
import pyarrow as pa
import numpy as np
import pandas as pd
import re

spark = SparkSession \
    .builder \
    .master(""local"") \
    .appName(""Python RDD[pd.DataFrame] to spark DF example"") \
    .getOrCreate()

# Enable Arrow-based columnar data transfers
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""true"")
sc = spark.sparkContext

# Create a spark DF from an RDD of pandas DFs
prdd = sc.range(0, 4).map(lambda x: pd.DataFrame([[x,]*4], columns=list('ABCD')))

prdd_large = sc.range(0, 32, numSlices=32). \
    map(lambda x: pd.DataFrame(np.random.randint(0, 100, size=(40 << 15, 4)), columns=list('ABCD')))

df = spark.createDataFrame(prdd, schema=None, pandasRDD=True)
df.toPandas()
```
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Added a new test using for creating a spark DF from an RDD of pandas dataframes. 
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
",https://api.github.com/repos/apache/spark/issues/29719/timeline,,spark,apache,linar-jether,11908736,MDQ6VXNlcjExOTA4NzM2,https://avatars.githubusercontent.com/u/11908736?v=4,,https://api.github.com/users/linar-jether,https://github.com/linar-jether,https://api.github.com/users/linar-jether/followers,https://api.github.com/users/linar-jether/following{/other_user},https://api.github.com/users/linar-jether/gists{/gist_id},https://api.github.com/users/linar-jether/starred{/owner}{/repo},https://api.github.com/users/linar-jether/subscriptions,https://api.github.com/users/linar-jether/orgs,https://api.github.com/users/linar-jether/repos,https://api.github.com/users/linar-jether/events{/privacy},https://api.github.com/users/linar-jether/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/29719,https://github.com/apache/spark/pull/29719,https://github.com/apache/spark/pull/29719.diff,https://github.com/apache/spark/pull/29719.patch,https://api.github.com/repos/apache/spark/issues/29719/reactions,0,0,0,0,0,0,0,0,0
207,https://api.github.com/repos/apache/spark/issues/29330,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/29330/labels{/name},https://api.github.com/repos/apache/spark/issues/29330/comments,https://api.github.com/repos/apache/spark/issues/29330/events,https://github.com/apache/spark/pull/29330,671807202,MDExOlB1bGxSZXF1ZXN0NDYxOTUzMTc2,29330,[SPARK-32432][SQL] Added support for reading ORC/Parquet files with SymlinkTextInputFormat,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2020-08-03T05:36:20Z,2021-07-20T12:28:49Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pull-request is to add support for reading ORC/Parquet files with SymlinkTextInputFormat in Apache Spark.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Hive style symlink (SymlinkTextInputFormat) is commonly used in different analytic engines including prestodb and prestosql.
Currently SymlinkTextInputFormat works with JSON/CSV files but does not work with ORC/Parquet files in Apache Spark (and Apache Hive).
On the other hand, prestodb and prestosql support SymlinkTextInputFormat with ORC/Parquet files.
This pull-request is to add support for reading ORC/Parquet files with SymlinkTextInputFormat in Apache Spark.

See details in the JIRA.  SPARK-32432

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes.
Currently Spark returns exceptions if users try to use SymlinkTextInputFormat with ORC/Parquet files.
With this patch, Spark can handle symlink which indicates locations of ORC/Parquet files.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
I added a new test suite `SymlinkSuite` and confirmed it passed.

```
$ ./build/sbt ""project hive"" ""test-only org.apache.spark.sql.hive.SymlinkSuite""
```",https://api.github.com/repos/apache/spark/issues/29330/timeline,,spark,apache,moomindani,1304020,MDQ6VXNlcjEzMDQwMjA=,https://avatars.githubusercontent.com/u/1304020?v=4,,https://api.github.com/users/moomindani,https://github.com/moomindani,https://api.github.com/users/moomindani/followers,https://api.github.com/users/moomindani/following{/other_user},https://api.github.com/users/moomindani/gists{/gist_id},https://api.github.com/users/moomindani/starred{/owner}{/repo},https://api.github.com/users/moomindani/subscriptions,https://api.github.com/users/moomindani/orgs,https://api.github.com/users/moomindani/repos,https://api.github.com/users/moomindani/events{/privacy},https://api.github.com/users/moomindani/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/29330,https://github.com/apache/spark/pull/29330,https://github.com/apache/spark/pull/29330.diff,https://github.com/apache/spark/pull/29330.patch,https://api.github.com/repos/apache/spark/issues/29330/reactions,0,0,0,0,0,0,0,0,0
208,https://api.github.com/repos/apache/spark/issues/28642,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/28642/labels{/name},https://api.github.com/repos/apache/spark/issues/28642/comments,https://api.github.com/repos/apache/spark/issues/28642/events,https://github.com/apache/spark/pull/28642,624755391,MDExOlB1bGxSZXF1ZXN0NDIzMTAyMzI2,28642,[SPARK-31809][SQL] Infer IsNotNull from join condition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2020-05-26T09:52:56Z,2021-08-16T07:21:11Z,,MEMBER,,"### What changes were proposed in this pull request?

We should infer `IsNotNull` from all the equality join keys. In our production environment, we often find these type of join keys(`coalesce(a, b)`, `cast(stringcol as double)`), which may contain many null values. For example:
```sql
CREATE TABLE t1(a string, b string, c string);
CREATE TABLE t2(a string, b string, c string);
EXPLAIN SELECT t1.* FROM t1 JOIN t2 ON coalesce(t1.a, t1.b)=t2.a;
```
After this pr:
```
== Physical Plan ==
*(5) Project [a#5, b#6, c#7]
+- *(5) SortMergeJoin [coalesce(a#5, b#6)], [a#8], Inner
   :- *(2) Sort [coalesce(a#5, b#6) ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(coalesce(a#5, b#6), 200), true, [id=#44]
   :     +- *(1) Filter isnotnull(coalesce(a#5, b#6))
   :        +- Scan hive default.t1 [a#5, b#6, c#7], HiveTableRelation `default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#5, b#6, c#7], Statistics(sizeInBytes=8.0 EiB)
   +- *(4) Sort [a#8 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#8, 200), true, [id=#52]
         +- *(3) Filter isnotnull(a#8)
            +- Scan hive default.t2 [a#8], HiveTableRelation `default`.`t2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#8, b#9, c#10], Statistics(sizeInBytes=8.0 EiB)
```

### Why are the changes needed?

1. Avoid skew join in some cases.
2. Hive support this optimization.


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:
Case1:
Before this PR | After this PR
-- | --
![image](https://issues.apache.org/jira/secure/attachment/13003914/13003914_default.png) | ![image](https://issues.apache.org/jira/secure/attachment/13003913/13003913_infer.png)

Case2:
Before this PR | After this PR
-- | --
![image](https://user-images.githubusercontent.com/5399861/128879249-f08c0577-caf7-422f-b25c-f47113cc5793.png) | ![image](https://user-images.githubusercontent.com/5399861/128879432-eff937d2-999b-4ac8-a216-25b40e093b67.png)
",https://api.github.com/repos/apache/spark/issues/28642/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/28642,https://github.com/apache/spark/pull/28642,https://github.com/apache/spark/pull/28642.diff,https://github.com/apache/spark/pull/28642.patch,https://api.github.com/repos/apache/spark/issues/28642/reactions,0,0,0,0,0,0,0,0,0
209,https://api.github.com/repos/apache/spark/issues/28032,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/28032/labels{/name},https://api.github.com/repos/apache/spark/issues/28032/comments,https://api.github.com/repos/apache/spark/issues/28032/events,https://github.com/apache/spark/pull/28032,588181296,MDExOlB1bGxSZXF1ZXN0MzkzOTc3MjM5,28032,[SPARK-31264][SQL] Repartition before writing data source tables/directories,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2020-03-26T06:12:14Z,2021-08-16T07:21:11Z,,MEMBER,,"### What changes were proposed in this pull request?

This PR adds a new rule `RepartitionWritingDataSource` to support repartitioning before writing data.  It supports three patterns:
- Repartition by none when writing normal tables/directories to reduce small files.
- Repartition by dynamic partition column when writing dynamic partition tables/directories to reduce small files because a single map task may contains many dynamic partition values.
- Repartition by bucket column with bucket number and sort by sort column when writing bucket tables/directories.

We only support data source tables/directories because [it cannot fully support Hive tables](https://github.com/apache/spark/blob/9f7cdb89f7614ef38ba2d8877d4c0f87ad9b6f5f/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala#L182-L188).

Hive has a similar rule: [SortedDynPartitionOptimizer](https://github.com/apache/hive/blob/917221e8378ec48ea05ef6b6c7d9515609b8ec01/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java).


### Why are the changes needed?
1. To reduce generating small files.
2. To ease pressure on the NameNode and improve insert dynamic partition table/directory performance.
   Spark job failed  because too many data blocks were created | HDFS data block monitor
   -- | --
   ![image](https://user-images.githubusercontent.com/5399861/77612149-62020880-6f62-11ea-8b2f-dfd46d0fc5a6.png) | ![image](https://user-images.githubusercontent.com/5399861/77612239-9bd30f00-6f62-11ea-9178-3bcd65aa4034.png)



### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Unit test and benchmark test:
Query | Before this PR | After this PR
-- | -- | --
CREATE TABLE t1 USING parquet   PARTITIONED BY (p1, p2) AS (SELECT id, id % 1000 AS p1, id % 10000 AS p2 FROM   range(5000000)) | 15 min | 1.1 min

",https://api.github.com/repos/apache/spark/issues/28032/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/28032,https://github.com/apache/spark/pull/28032,https://github.com/apache/spark/pull/28032.diff,https://github.com/apache/spark/pull/28032.patch,https://api.github.com/repos/apache/spark/issues/28032/reactions,0,0,0,0,0,0,0,0,0
210,https://api.github.com/repos/apache/spark/issues/27432,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/27432/labels{/name},https://api.github.com/repos/apache/spark/issues/27432/comments,https://api.github.com/repos/apache/spark/issues/27432/events,https://github.com/apache/spark/pull/27432,558645188,MDExOlB1bGxSZXF1ZXN0MzY5OTczNTA3,27432,[SPARK-28325][SQL] Support ANSI SQL: SIMILAR TO ... ESCAPE syntax,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2020-02-02T05:20:00Z,2021-08-04T10:53:07Z,,CONTRIBUTOR,,"### What changes were proposed in this pull request?
`[ NOT ] SIMILAR TO <similar pattern> [ ESCAPE <escape character> ]`
This is a ANSI SQL.
For example:
```
SELECT 'abc' SIMILAR TO 'abc' AS `true`;
SELECT 'abc' SIMILAR TO 'a' AS `false`;
SELECT 'abc' SIMILAR TO '%(b|d)%' AS `true`;
SELECT 'abc' SIMILAR TO '(b|c)%' AS `false`;
```
There are some mainstream database support the syntax.
**PostgreSQL:**
https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-SIMILARTO-REGEXP

**Redshift:**
https://docs.aws.amazon.com/redshift/latest/dg/pattern-matching-conditions-similar-to.html

**Sybase**
http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/like-regexp-similarto.html

**Firebird**
http://firebirdsql.org/file/documentation/html/en/refdocs/fblangref25/firebird-25-language-reference.html#fblangref25-commons-predsiimilarto

`SIMILAR TO` is similar to `RLIKE`, but with the following differences:
       1. The `SIMILAR TO` operator returns true only if its pattern matches the entire string,
          unlike `RLIKE` behavior, where the pattern can match any portion of the string.
       2. The regex string allow use _ and % as wildcard characters denoting any single character
          and any string, respectively (these are comparable to . and .* in POSIX regular
          expressions).
       3. The regex string allow use escape character like `LIKE` behavior.
       4. '.', '^' and '$' is not a meta character for `SIMILAR TO`.


### Why are the changes needed?
Support ANSI SQL: `SIMILAR TO ... ESCAPE` syntax
`SIMILAR TO ... ESCAPE ` has a lot of convenience compared to `RLIKE`.


### Does this PR introduce any user-facing change?
No


### How was this patch tested?
Exists and new UT",https://api.github.com/repos/apache/spark/issues/27432/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/27432,https://github.com/apache/spark/pull/27432,https://github.com/apache/spark/pull/27432.diff,https://github.com/apache/spark/pull/27432.patch,https://api.github.com/repos/apache/spark/issues/27432/reactions,0,0,0,0,0,0,0,0,0
